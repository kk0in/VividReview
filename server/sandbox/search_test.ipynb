{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf\n",
    "import json\n",
    "\n",
    "PDF = \"../pdfs\"\n",
    "SPM = \"../spms\"\n",
    "CROP = \"../crops\"\n",
    "ANNOTATIONS = \"../annotations\"  \n",
    "\n",
    "project_id = 10\n",
    "pdf_path = os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")\n",
    "spm_path = os.path.join(SPM, f\"{project_id}_page_info.json\")\n",
    "annnotation_path = os.path.join(ANNOTATIONS, f\"{project_id}_annotation.json\")\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "with open(annnotation_path, \"r\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = page_info\n",
    "\n",
    "doc = pymupdf.open(pdf_path)\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text = page.get_text(\"text\")  \n",
    "    images_info = page.get_image_info(xrefs=True)  \n",
    "    image_path = os.path.join(CROP, str(project_id), str(page_num + 1)) \n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    crop_images = []\n",
    "    for image_index, img_info in enumerate(images_info):\n",
    "        xref = img_info['xref']  # 이미지의 xref 값\n",
    "        base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "        image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "        crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")  \n",
    "        crop_images.append(crop_path)\n",
    "        # 이미지 저장\n",
    "        with open(crop_path, \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "\n",
    "    output[\"pages\"][str(page_num+1)][\"pdf_text\"] = text    \n",
    "    output[\"pages\"][str(page_num+1)][\"pdf_images\"] = crop_images\n",
    "    output[\"pages\"][str(page_num+1)][\"annotation\"] = annotations[str(page_num+1)]\n",
    "\n",
    "with open(spm_path, 'w') as file:\n",
    "    json.dump(output, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS = \"../annotations\"  \n",
    "\n",
    "annnotation_path = os.path.join(ANNOTATIONS, f\"{project_id}_annotation.json\")\n",
    "\n",
    "with open(annnotation_path, \"r\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'start': 0.0, 'end': 40.0, 'text': \"okay, so welcome to lecture two of cs231n. on tuesday we, just recall, we, sort of, gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. and today, we're really going to dive in, for the first time, into the details. and we'll start to see, in much more depth, exactly how some of these learning algorithms actually work in practice. so, the first lecture of the class is probably, sort of, the largest big picture vision. and the majority of the lectures in this class will be much more detail orientated, much more focused on the specific mechanics, of these different algorithms. so, today we'll see our first learning algorithm and that'll be really exciting, i think. but, before we get to that, i wanted to talk about a couple of administrative issues.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nFei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n1\\n', 'pdf_images': []}, '2': {'start': 40.47999954223633, 'end': 100.77999877929688, 'text': \"one, is piazza. so, i saw it when i checked yesterday, it seemed like we had maybe 500 students signed up on piazza. which means that there are several hundred of you who are not yet there. so, we really want piazza to be the main source of communication between the students and the core staff. so, we've gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance. and, any, sort of, questions like that should really go to piazza. you'll probably get answers to your questions faster on piazza, because all the tas are knowing to check that. and it's, sort of, easy for emails to get lost in the shuffle if you just send to the course list. it's also come to my attention that some scpd students are having a bit of a hard time signing up for piazza. scpd students are supposed to receive a @stanford.edu email address. so, once you get that email address, then you can use the stanford email to sign into piazza. probably that doesn't affect those of you who are sitting in the room right now, but, for those students listening on scpd.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAdministrative: Piazza\\nFor questions about midterm, poster session, projects, \\nuse Piazza instead of staff list!\\nSCPD students: Use your @stanford.edu address to register for Piazza; contact \\nscpd-customerservice@stanford.edu for help.\\n2\\n', 'pdf_images': []}, '3': {'start': 100.77999877929688, 'end': 152.3000030517578, 'text': \"the next administrative issue is about assignment one. assignment one will be up later today, probably sometime this afternoon, but i promise, before i go to sleep tonight, it'll be up. but, if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. it'll be pretty much the same content. we're just reshuffling it a little bit to make it, like, for example, upgrading to work with python 3, rather than python 2.7. and some of these minor cosmetic changes, but the content of the assignment will still be the same as last year. so, in this assignment you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. you'll also implement several different linear classifiers, including the svm and softmax, as well as a simple two-layer neural network. and we'll cover all this content over the next couple of lectures. so,\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAdministrative: Assignment 1\\nOut tonight, due 4/18 11:59pm\\n- K-Nearest Neighbor\\n- Linear classifiers: SVM, Softmax\\n- Two-layer neural network\\n- Image features\\n3\\n', 'pdf_images': []}, '4': {'start': 152.3000030517578, 'end': 200.66000366210938, 'text': \"all of our assignments are using python and numpy. if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed. but, this is, actually, pretty important. numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. and you'll get a lot of practice with this on the first assignment. so, for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAdministrative: Python + Numpy\\n4\\nhttp://cs231n.github.io/python-numpy-tutorial/ \\n', 'pdf_images': ['../crops/10/4/1.png']}, '5': {'start': 200.66000366210938, 'end': 282.0400085449219, 'text': \"the other thing i wanted to talk about is that we're happy to announce that we're officially supported through google cloud for this class. so, google cloud is somewhat similar to amazon aws. you can go and start virtual machines up in the cloud. these virtual machines can have gpus. we're working on the tutorial for exactly how to use google cloud and get it to work for the assignments. but our intention is that you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. and because google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use google cloud credits for free for the class. so you can feel free to use these for the assignments and also for the course projects when you want to start using gpus and larger machines and whatnot. so, we'll post more details about that, probably, on piazza later today. but, i just wanted to mention, because i know there had been a couple of questions about, can i use my laptop? do i have to run on corn? do i have to, whatever? and the answer is that, you'll be able to run on google cloud and we'll provide you some coupons for that. yeah, so, those are, kind of, the major administrative issues i wanted to talk about today. and then, let's dive into the content.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAdministrative: Google Cloud\\n5\\nhttp://cs231n.github.io/gce-tutorial/ \\n', 'pdf_images': ['../crops/10/5/1.png']}, '6': {'start': 282.0400085449219, 'end': 337.94000244140625, 'text': \"so, the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision. and this is something that we'll really focus on throughout the course of the class. is, exactly, how do we work on this image classification task? so, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware of some predetermined set of categories or labels. so, these might be, like, a dog or a cat or a truck or a plane, and there's some fixed set of category labels, and the job of the computer is to look at the picture and assign it one of these fixed category labels. this seems like a really easy problem, because so much of your own visual system in your brain is hardwired to doing these, sort of, visual recognition tasks. but this is actually a really, really hard problem for a machine.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nImage Classification: A core task in Computer Vision\\n6\\ncat\\n(assume given set of discrete labels)\\n{dog, cat, truck, plane, ...}\\nThis image by Nikita is \\nlicensed under CC-BY 2.0\\n', 'pdf_images': ['../crops/10/6/1.png']}, '7': {'start': 337.94000244140625, 'end': 399.2200012207031, 'text': \"so, if you dig in and think about, actually, what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it. and the computer really is representing the image as this gigantic grid of numbers. so, the image might be something like 800 by 600 pixels. and each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. so, to the computer, this is just a gigantic grid of numbers. and it's very difficult to distill the cat-ness out of this, like, giant array of thousands, or whatever, very many different numbers. so, we refer to this problem as the semantic gap. this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image, and there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. and this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nThis image by Nikita is \\nlicensed under CC-BY 2.0\\nThe Problem: Semantic Gap\\n7\\nWhat the computer sees\\nAn image is just a big grid of \\nnumbers between [0, 255]:\\ne.g. 800 x 600 x 3\\n(3 channels RGB)\\n', 'pdf_images': ['../crops/10/7/1.png', '../crops/10/7/2.png']}, '8': {'start': 399.2200012207031, 'end': 423.5799865722656, 'text': \"so, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid, every single pixel, in this giant grid of numbers would be completely different. but, somehow, it's still representing the same cat. and our algorithms need to be robust to this. but, not only viewpoint is one problem, another is illumination.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nChallenges: Viewpoint variation\\n8\\nAll pixels change when \\nthe camera moves!\\nThis image by Nikita is \\nlicensed under CC-BY 2.0\\n', 'pdf_images': ['../crops/10/8/1.png', '../crops/10/8/2.png']}, '9': {'start': 423.5799865722656, 'end': 437.3999938964844, 'text': \"there can be different lighting conditions going on in the scene. whether the cat is appearing in this very dark, moody scene, or like is this very bright, sunlit scene, it's still a cat, and our algorithms need to be robust to that.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nChallenges: Illumination\\n9\\nThis image is CC0 1.0 public domain\\nThis image is CC0 1.0 public domain\\nThis image is CC0 1.0 public domain\\nThis image is CC0 1.0 public domain\\n', 'pdf_images': ['../crops/10/9/1.png', '../crops/10/9/2.png', '../crops/10/9/3.png', '../crops/10/9/4.png']}, '10': {'start': 437.3999938964844, 'end': 450.79998779296875, 'text': 'objects can also deform. i think cats are, maybe, among the more deformable of animals that you might see out there. and cats can really assume a lot of different, varied poses and positions. and our algorithms should be robust to these different kinds of transforms.', 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nChallenges: Deformation\\n10\\nThis image by Umberto Salvagnin \\nis licensed under CC-BY 2.0\\nThis image by Tom Thai is \\nlicensed under CC-BY 2.0 \\nThis image by sare bear is \\nlicensed under CC-BY 2.0\\nThis image by Umberto Salvagnin \\nis licensed under CC-BY 2.0\\n', 'pdf_images': ['../crops/10/10/1.png', '../crops/10/10/2.png', '../crops/10/10/3.png', '../crops/10/10/4.png']}, '11': {'start': 450.79998779296875, 'end': 475.6600036621094, 'text': \"there can also be problems of occlusion, where you might only see part of a cat, like, just the face, or in this extreme example, just a tail peeking out from under the couch cushion. but, in these cases, it's pretty easy for you, as a person, to realize that this is probably a cat, and you still recognize these images as cats. and this is something that our algorithms also must be robust to, which is quite difficult, i think.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nChallenges: Occlusion\\n11\\nThis image is CC0 1.0 public domain\\nThis image by jonsson is licensed \\nunder CC-BY 2.0\\nThis image is CC0 1.0 public domain\\n', 'pdf_images': ['../crops/10/11/1.png', '../crops/10/11/2.png', '../crops/10/11/3.png']}, '12': {'start': 475.6600036621094, 'end': 488.82000732421875, 'text': 'there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background. and this is another thing that we need to handle.', 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n12\\nThis image is CC0 1.0 public domain\\nChallenges: Background Clutter\\nThis image is CC0 1.0 public domain\\n', 'pdf_images': ['../crops/10/12/1.png', '../crops/10/12/2.png']}, '13': {'start': 488.82000732421875, 'end': 569.8599853515625, 'text': \"there's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances. and cats can come in different shapes and sizes and colors and ages. and our algorithm, again, needs to work and handle all these different variations. so, this is actually a really, really challenging problem. and it's sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things. but now if we want our computer programs to deal with all of these problems, all simultaneously, and not just for cats, by the way, but for just about any object category you can imagine, this is a fantastically challenging problem. and it's, actually, somewhat miraculous that this works at all, in my opinion. but, actually, not only does it work, but these things work very close to human accuracy in some limited situations. and take only hundreds of milliseconds to do so. so, this is some pretty amazing, incredible technology, in my opinion, and over the course of the rest of the class we will really see what kinds of advancements have made this possible. so now, if you, kind of, think about what is the api for writing an image classifier, you might sit down and try to write a method in python like this. where you want to take in an image and then do some crazy magic and then, eventually, spit out this class label to say cat or dog or whatnot. and there's really no obvious way to do this, right?\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nChallenges: Intraclass variation\\n13\\nThis image is CC0 1.0 public domain\\n', 'pdf_images': ['../crops/10/13/1.png']}, '14': {'start': 569.8599853515625, 'end': 612.739990234375, 'text': \"if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something like rsa encryption, you, sort of, can write down an algorithm and enumerate all the steps that need to happen in order for this things to work. but, when we're trying to recognize objects, or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense, for how you might go about recognizing these objects. so, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down and write this function, i think most people would be in trouble. that being said, people have definitely made explicit attempts to try to write, sort of, high-end coded rules for recognizing different animals. so,\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAn image classifier\\n14\\nUnlike e.g. sorting a list of numbers,\\n \\nno obvious way to hard-code the algorithm for \\nrecognizing a cat, or other classes.\\n', 'pdf_images': ['../crops/10/14/1.png']}, '15': {'start': 612.9600219726562, 'end': 739.6599731445312, 'text': \"we touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses. and we know that edges, from hubel and wiesel, we know that edges are pretty important when it comes to visual recognition. so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats. but this turns out not to work very well. one, it's super brittle. and, two, say, if you want to start over for another object category, and maybe not worry about cats, but talk about trucks or dogs or fishes or something else, then you need to start all over again. so, this is really not a very scalable approach. we want to come up with some algorithm, or some method, for these recognition tasks which scales much more naturally to all the variety of objects in the world. so, the insight that, sort of, makes this all work is this idea of the data-driven approach. rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this. and we can actually use tools like google image search, or something like that, to go out and collect a very large number of examples of these different categories. by the way, this actually takes quite a lot of effort to go out and actually collect these datasets but, luckily, there's a lot of really good, high quality datasets out there already for you to use. then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. then finally, we'll use this training model and apply it on new images that will then be able to recognize cats and dogs and whatnot.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nAttempts have been made\\n15\\nJohn Canny, “A Computational Approach to Edge Detection”, IEEE TPAMI 1986\\nFind edges\\nFind corners\\n?\\n', 'pdf_images': ['../crops/10/15/1.png', '../crops/10/15/2.png']}, '16': {'start': 739.6599731445312, 'end': 792.4600219726562, 'text': \"so here our api has changed a little bit. rather than a single function that just inputs an image and recognizes a cat, we have these two functions. one, called, train, that's going to input images and labels and then output a model, and then, separately, another function called, predict, which will input the model and than make predictions for images. and this is, kind of, the key insight that allowed all these things to start working really well over the last 10, 20 years or so. so, this class is primarily about neural networks and convolutional neural networks and deep learning and all that, but this idea of a data-driven approach is much more general than just deep learning. and i think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. so, probably, the simplest classifier you can imagine is something we call nearest neighbor.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nData-Driven Approach\\n16\\n1. Collect a dataset of images and labels\\n2. Use Machine Learning to train a classifier\\n3. Evaluate the classifier on new images\\nExample training set\\n', 'pdf_images': ['../crops/10/16/1.png', '../crops/10/16/2.png']}, '17': {'start': 792.4600219726562, 'end': 826.6199951171875, 'text': \"the algorithm is pretty dumb, honestly. so, during the training step we won't do anything, we'll just memorize all of the training data. so this is very simple. and now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image. a very simple algorithm. but it, sort of, has a lot of these nice properties with respect to data-drivenness and whatnot. so, to be a little bit more concrete,\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nFirst classifier: Nearest Neighbor\\n17\\nMemorize all \\ndata and labels\\nPredict the label \\nof the most similar \\ntraining image\\n', 'pdf_images': ['../crops/10/17/1.png']}, '18': {'start': 826.6199951171875, 'end': 855.1400146484375, 'text': \"you might imagine working on this dataset called cifar-10, which is very commonly used in machine learning, as kind of a small test case. and you'll be working with this dataset on your homework. so, the cifar-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. and for each of those 10 categories it provides 50,000 training images, roughly evenly distributed across these 10 categories. and then 10,000 additional testing images that you're supposed to test your algorithm on.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nExample Dataset: CIFAR10\\n18\\n Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images”, Technical Report, 2009.\\n10 classes\\n50,000 training images\\n10,000 testing images\\n', 'pdf_images': ['../crops/10/18/1.png']}, '19': {'start': 855.4000244140625, 'end': 957.3200073242188, 'text': \"so here's an example of applying this simple nearest neighbor classifier to some of these test images on cifar-10. so, on this grid on the right, for the left most column, gives a test image in the cifar-10 dataset. and now on the right, we've sorted the training images and show the most similar training images to each of these test examples. and you can see that they look kind of visually similar to the training images, although they are not always correct, right? so, maybe on the second row, we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. but, this image is a dog and it's nearest neighbor is also a dog, but this next one, i think is actually a deer or a horse or something else. but, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. so, if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set. and now, the closest example, we know it's label, because it comes from the training set. and now, we'll simply say that this testing image is also a dog. you can see from these examples that is probably not going to work very well, but it's still kind of a nice example to work through. but then, one detail that we need to know is, given a pair of images, how can we actually compare them? because, if we're going to take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nExample Dataset: CIFAR10\\n19\\n Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images”, Technical Report, 2009.\\n10 classes\\n50,000 training images\\n10,000 testing images\\nTest images and nearest neighbors\\n', 'pdf_images': ['../crops/10/19/1.png', '../crops/10/19/2.png']}, '20': {'start': 957.3200073242188, 'end': 1005.1799926757812, 'text': \"so, in the example in the previous slide, we've used what's called the l1 distance, also sometimes called the manhattan distance. so, this is a really sort of simple, easy idea for comparing images. and that's that we're going to just compare individual pixels in these images. so, supposing that our test image is maybe just a tiny four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. and then, sum all these up across all the pixels in the image. so, this is kind of a stupid way to compare images, but it does some reasonable things sometimes. but, this gives us a very concrete way to measure the difference between two images. and in this case, we have this difference of 456 between these two images.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nDistance Metric to compare images\\n20\\nL1 distance:\\nadd\\n', 'pdf_images': ['../crops/10/20/1.png', '../crops/10/20/2.png']}, '21': {'start': 1005.1799926757812, 'end': 1044.0, 'text': \"so, here's some full python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by numpy. so, here we can see that this training function, that we talked about earlier, is, again, very simple, in the case of nearest neighbor, you just memorize the training data, there's not really much to do here. and now, at test time, we're going to take in our image and then go in and compare using this l1 distance function, our test image to each of these training examples and find the most similar example in the training set.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n21\\nNearest Neighbor classifier\\n', 'pdf_images': ['../crops/10/21/1.png']}, '22': {'start': 1044.0, 'end': 1055.8800048828125, 'text': \"and you can see that, we're actually able to do this in just one or two lines of python code by utilizing these vectorized operations in numpy. so, this is something that you'll get practice with on the first assignment.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n22\\nNearest Neighbor classifier\\nMemorize training data\\n', 'pdf_images': ['../crops/10/22/1.png']}, '23': {'start': 1055.8800048828125, 'end': 1064.6800537109375, 'text': 'so now, a couple questions about this simple classifier. first, if we have n examples in our training set, then how fast can we expect training and testing to be?', 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n23\\nNearest Neighbor classifier\\nFor each test image:\\n  Find closest train image\\n  Predict label of nearest image\\n', 'pdf_images': ['../crops/10/23/1.png']}, '24': {'start': 1064.6800537109375, 'end': 1094.3800048828125, 'text': \"well, training is probably constant because we don't really need to do anything, we just need to memorize the data. and if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. but now, at test time we need to do this comparison stop and compare our test image to each of the n training examples in the dataset. and this is actually quite slow. so, this is actually somewhat backwards, if you think about it.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n24\\nNearest Neighbor classifier\\nQ: With N examples, \\nhow fast are training \\nand prediction?\\n', 'pdf_images': ['../crops/10/24/1.png']}, '25': {'start': 1094.3800048828125, 'end': 1126.3800048828125, 'text': 'because, in practice, we want our classifiers to be slow at training time and then fast at testing time. because, you might imagine, that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good. but then, when you go and deploy the classifier at test time, you want it to run on your mobile phone or in a browser or some other low power device, and you really want the testing time performance of your classifier to be quite fast.', 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n25\\nNearest Neighbor classifier\\nQ: With N examples, \\nhow fast are training \\nand prediction?\\nA: Train O(1),\\n     predict O(N)\\n', 'pdf_images': ['../crops/10/25/1.png']}, '26': {'start': 1126.3800048828125, 'end': 1152.1800537109375, 'text': \"so, from this perspective, this nearest neighbor algorithm, is, actually, a little bit backwards. and we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. so then, the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? so, here we've drawn, what we call the decision regions of a nearest neighbor classifier.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n26\\nNearest Neighbor classifier\\nQ: With N examples, \\nhow fast are training \\nand prediction?\\nA: Train O(1),\\n     predict O(N)\\nThis is bad: we want \\nclassifiers that are fast \\nat prediction; slow for \\ntraining is ok\\n', 'pdf_images': ['../crops/10/26/1.png']}, '27': {'start': 1152.1800537109375, 'end': 1232.9200439453125, 'text': \"so, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point. so, here we see we have five classes and some blue ones up in the corner here, some purple ones in the upper-right hand corner. and now for each pixel in this entire plane, we've gone and computed what is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. so, you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points. but this classifier is maybe not so great. and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier. for one, this central region actually contains mostly green points, but one little yellow point in the middle. but because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of this green cluster. and that's, maybe, not so great. maybe those points actually should have been green. and then, similarly we also see these, sort of, fingers, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. so, this kind of motivates a slight generalization of this algorithm called k-nearest neighbors.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nWhat does this look like?\\n27\\n', 'pdf_images': ['../crops/10/27/1.png']}, '28': {'start': 1234.219970703125, 'end': 1373.1600341796875, 'text': \"so rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find k of our nearest neighbors, according to our distance metric, and then take a vote among each of our neighbors. and then predict the majority vote among our neighbors. you can imagine slightly more complex ways of doing this. maybe you'd vote weighted on the distance, or something like that, but the simplest thing that tends to work pretty well is just taking a majority vote. so here we've shown the exact same set of points using this k=1 nearest neighbor classifier, as well as k=3 and k=5 in the middle and on the right. and once we move to k=3, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. now this entire green portion in the middle is all being classified as green. you can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. and then, once we move to the k=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. so, generally when you're using nearest neighbors classifiers, you almost always want to use some value of k, which is larger than one because this tends to smooth out your decision boundaries and lead to better results. question? [student asking a question] yes, so the question is, what is the deal with these white regions? the white regions are where there was no majority among the k-nearest neighbors. you could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners, but for this simple example we're just coloring it white to indicate there was no nearest neighbor in those points. whenever we're thinking about computer vision i think it's really useful to kind of flip back and forth between several different viewpoints. one, is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images. because the pixels of the image actually allow us to think of these images as high dimensional vectors. and it's sort of useful to ping pong back and forth between these two different viewpoints. so then, sort of taking this k-nearest neighbor and going back to the images you can see that it's actually not very good.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\nK-Nearest Neighbors\\n28\\nInstead of copying label from nearest neighbor, \\ntake majority vote from K closest points\\nK = 1\\nK = 3\\nK = 5\\n', 'pdf_images': ['../crops/10/28/1.png', '../crops/10/28/2.png', '../crops/10/28/3.png']}, '29': {'start': 1373.8800048828125, 'end': 1392.3800048828125, 'text': \"here i've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor. and you can see that it's really not very good. but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row.\", 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n29\\nWhat does this look like?\\n', 'pdf_images': ['../crops/10/29/1.png']}, '30': {'start': 1392.3800048828125, 'end': 1399.739990234375, 'text': 'and you could imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way. ', 'pdf_text': 'Fei-Fei Li & Justin Johnson & Serena Yeung\\nLecture 2 - \\nApril 6, 2017\\n30\\nWhat does this look like?\\n', 'pdf_images': ['../crops/10/30/1.png']}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    images_info = page.get_image_info(xrefs=True)  \n",
    "    image_path = os.path.join(CROP, str(project_id), str(page_num + 1)) \n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    crop_images = []\n",
    "    for image_index, img_info in enumerate(images_info):\n",
    "        xref = img_info['xref']  # 이미지의 xref 값\n",
    "        base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "        image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "        crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")  \n",
    "        crop_images.append(crop_path)\n",
    "        # 이미지 저장\n",
    "        with open(crop_path, \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        \n",
    "    page_info[str(page_num+1)][\"pdf_images\"] = crop_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../crops/10/10/4.png\n",
      "../crops/10/10/2.png\n",
      "../crops/10/10/3.png\n",
      "../crops/10/10/1.png\n",
      "Similarity results:\n",
      "2.png: 0.3069\n",
      "1.png: 0.2714\n",
      "4.png: 0.2637\n",
      "3.png: 0.2379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def calculate_clip_similarity(image_dir, search_query):\n",
    "    # CLIP 모델 및 프로세서 로드\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # 검색어를 텍스트 토큰으로 변환\n",
    "    text_inputs = processor(text=[search_query], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    similarities = {}\n",
    "\n",
    "    # 이미지 디렉터리 내의 모든 이미지 파일에 대해 유사도 계산\n",
    "    for image_name in os.listdir(image_dir):\n",
    "        if image_name.lower().endswith('png'):\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            print(image_path)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # 이미지를 CLIP 모델의 입력 형식으로 변환\n",
    "            image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # CLIP 모델을 사용하여 이미지와 텍스트의 특성 벡터 추출\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**image_inputs)\n",
    "                text_features = model.get_text_features(**text_inputs)\n",
    "\n",
    "            # 이미지와 텍스트 특성 간의 코사인 유사도 계산\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (image_features @ text_features.T).item()  # 유사도 계산\n",
    "\n",
    "            similarities[image_name] = similarity\n",
    "\n",
    "    # 유사도 결과를 내림차순으로 정렬\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"Similarity results:\")\n",
    "    for image_name, similarity in sorted_similarities:\n",
    "        print(f\"{image_name}: {similarity:.4f}\")\n",
    "\n",
    "# 사용 예시\n",
    "image_dir = \"../crops/10/10\"  # 이미지가 저장된 디렉터리\n",
    "search_query = \"a standing cat\"  # 검색어\n",
    "\n",
    "calculate_clip_similarity(image_dir, search_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.3068891167640686\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"../crops/10/10/2.png\")).unsqueeze(0).to(device) # 원하는 이미지 경로로 수정\n",
    "text = clip.tokenize(\"a standing cat\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    # logits_per_image, logits_per_text = model(image, text)\n",
    "    # probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # 코사인 유사도를 계산\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (image_features @ text_features.T).item()  # 유사도 값 계산\n",
    "\n",
    "# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8880940675735474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 두 텍스트 정의\n",
    "text1 = \"This is a sample sentence.\"\n",
    "text2 = \"This sentence is similar to a sample sentence.\"\n",
    "\n",
    "# 텍스트 임베딩 계산\n",
    "embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# 모델 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')  # Sentence Transformers 모델\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def calculate_similarity(data, query):\n",
    "    # Query 텍스트 임베딩 계산\n",
    "    query_embedding = text_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for page, content in data.items():\n",
    "        page_result = {}\n",
    "        \n",
    "        # text 유사도 계산\n",
    "        text_embedding = text_model.encode(content[\"script\"], convert_to_tensor=True)\n",
    "        page_result[\"script\"] = util.pytorch_cos_sim(query_embedding, text_embedding).item()\n",
    "\n",
    "        # pdf_text 유사도 계산\n",
    "        pdf_text_embedding = text_model.encode(content[\"pdf_text\"], convert_to_tensor=True)\n",
    "        page_result[\"pdf_text\"] = util.pytorch_cos_sim(query_embedding, pdf_text_embedding).item()\n",
    "\n",
    "        # pdf_images 유사도 계산 (CLIP 사용)\n",
    "        image_similarities = []\n",
    "        for image_path in content[\"pdf_images\"]:\n",
    "            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Query 텍스트 임베딩도 CLIP을 사용해 계산\n",
    "                text_tokens = clip.tokenize([query]).to(device)\n",
    "                text_features = clip_model.encode_text(text_tokens)\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                similarity = (image_features @ text_features.T).item()\n",
    "                image_similarities.append(similarity)\n",
    "\n",
    "        # 여러 이미지가 있을 경우 평균 유사도 계산\n",
    "        if image_similarities:\n",
    "            page_result[\"pdf_image\"] = sum(image_similarities) / len(image_similarities)\n",
    "\n",
    "        results[page] = page_result\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "# 데이터 예시\n",
    "data = page_info\n",
    "\n",
    "query = \"administration\"  # 검색어\n",
    "\n",
    "result = {}\n",
    "# 유사도 계산\n",
    "similarities = calculate_similarity(data, query)\n",
    "result[\"query\"] = query\n",
    "result[\"similarities\"] = similarities\n",
    "\n",
    "SIM = \"../sims\"\n",
    "search_id = 1\n",
    "sim_path = os.path.join(SIM, str(project_id))\n",
    "os.makedirs(sim_path, exist_ok=True)\n",
    "sim_json_path = os.path.join(sim_path, f\"{search_id}.json\")\n",
    "\n",
    "with open(sim_json_path, 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strr = \"administration\"\n",
    "\n",
    "len(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text from OCR:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def perform_ocr_on_image(image_path):\n",
    "    # 이미지 파일을 열기\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # OCR을 수행하여 이미지에서 텍스트 추출\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    \n",
    "    # 추출된 텍스트 출력\n",
    "    print(\"Extracted Text from OCR:\")\n",
    "    print(extracted_text)\n",
    "\n",
    "image_path = \"../annotations/11/drawing_1_processed.png\"  \n",
    "perform_ocr_on_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([[84, 54], [230, 54], [230, 176], [84, 176]], 'APple', 0.30573700128243714)]\n",
      "APple\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# EasyOCR Reader 초기화 (한국어 및 영어 지원)\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# 이미지 경로 설정\n",
    "image_path = \"../annotations/11/drawing_1_processed.png\"  \n",
    "\n",
    "# 이미지에서 텍스트 추출\n",
    "result = reader.readtext(image_path)\n",
    "\n",
    "print(result)\n",
    "# 결과 출력\n",
    "for res in result:\n",
    "    print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = \"\"\n",
    "\n",
    "if strr:\n",
    "    print(\"True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "SPM = \"../spms\"\n",
    "SIMILARITY = \"../similarity\"\n",
    "\n",
    "project_id = 10\n",
    "search_id = 1\n",
    "search_type = \"keyword\" # \"semantic\" 또는 \"keyword\"\n",
    "spm_path = os.path.join(SPM, f\"{project_id}_page_info.json\")\n",
    "search_path = os.path.join(SIMILARITY, str(project_id), f\"{search_id}_{search_type}.json\")\n",
    "page_from_script = []\n",
    "page_from_pdf_text = []\n",
    "page_from_annotation = []\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "query = \"apple\"  # 검색어\n",
    "\n",
    "result = {}\n",
    "result[\"query\"] = query\n",
    "result[\"source\"] = defaultdict(list)\n",
    "\n",
    "for page, content in page_info[\"pages\"].items():\n",
    "    # script에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"script\"].lower():\n",
    "        result[\"source\"][\"script\"].append(page)\n",
    "    # pdf_text에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"pdf_text\"].lower():\n",
    "        result[\"source\"][\"pdf_text\"].append(page)\n",
    "    # annotation에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"annotation\"].lower():\n",
    "        result[\"source\"][\"annotation\"].append(page)\n",
    "\n",
    "with open(search_path, 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
