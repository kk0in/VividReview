{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "SCRIPT = './scripts'\n",
    "\n",
    "project_id = 10\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Assistant for Mapping Lecture Script to Lecture Notes\",\n",
    "  instructions=\"You are a helpful assistant designed to output JSON. Use your knowledge base to distribute the lecture script content accurately to each page of the lecture notes.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "\n",
    "PDF = '../pdfs'\n",
    "project_id = 11\n",
    "pdf_file = [\n",
    "        file\n",
    "        for file in os.listdir(PDF)\n",
    "        if file.startswith(f\"{project_id}_\") and file.endswith(\".pdf\")\n",
    "    ]\n",
    "original_pdf_path = os.path.join(PDF, pdf_file[0])\n",
    "\n",
    "reader = PdfReader(original_pdf_path)\n",
    "print(f\"PDF의 페이지 수: {len(reader.pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay so welcome to lecture two of CS231N On Tuesday we just recall we sort of gave you the big picture view of what is computer vision what is the history and a little bit of the overview of the class and today we're really going to dive in for the first time into the details and we'll start to see in much more depth exactly how some of these learning algorithms actually work in practice So the first lecture of the class is probably the sort of the largest big picture vision and the majority of the lectures in this class will be much more detail oriented much more focused on the specific mechanics of these different algorithms So today we'll see our first learning algorithm and that'll be really exciting I think But before we get to that I wanted to talk about a couple administrative issues One is Piazza so when I checked yesterday it seemed like we had maybe 500 students signed up on Piazza which means that there are several hundred of you who are not yet there So we really want Piazza to be the main source of communication between the students and the course staff So we've gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance and any sort of questions like that should really go to Piazza You'll probably get answers to your questions faster on Piazza because all the TAs are knowing to check that And it's sort of easy for emails to get lost in the shuffle if you just send to the course list It's also come to my attention that some SCPD students are having a bit of a hard time signing up for Piazza If you SCPD students are supposed to receive a stanford atstanford edu email address So once you get that email address then you can use the Stanford email to sign into Piazza Probably that doesn't affect those of you who are sitting in the room right now but for those students listening on SCPD The next administrative issue is about assignment one Assignment one will be up later today probably sometime this afternoon but I promise before I go to sleep tonight it'll be up But if you're getting a little bit antsy and really want to start working on it right now then you can look at last year's version of assignment one It'll be pretty much the same content We're just reshuffling it a little bit to make it for example upgrading to work with Python 3 rather than Python 2 7 and some of these minor cosmetic changes But the content of the assignment will still be the same as last year So in this assignment you'll be implementing your own k nearest neighbor classifier which we're going to talk about in this lecture You'll also implement several different linear classifiers including the SVM and Softmax as well as a simple two layer neural network And we'll cover all of this content over the next couple of lectures So all of our assignments are using Python and NumPy If you aren't familiar with Python or NumPy then we have written a tutorial that you can find on the course website to try and get you up to speed But this is actually pretty important NumPy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code So this is super important for pretty much all aspects of numerical computing and machine learning and everything like that is efficiently implementing these vectorized operations And you'll get a lot of practice with this on the first assignment So for those of you who don't have a lot of experience with MATLAB or NumPy or other types of vectorized tensor computation I recommend that you start looking at this assignment pretty early and also read carefully through the tutorial The other thing I wanted to talk about is that we're happy to announce that we got we're officially supported through Google Cloud for this class So Google Cloud is somewhat similar to Amazon AWS You can go and start virtual machines up in the cloud These virtual machines can have GPUs So we have a well we're working on the tutorial for exactly how to use Google Cloud and get it to work for the assignments But our intention is that you'll be able to just download some image and it'll be very seamless for you to work on the assignment on one of these instances on the cloud And because Google has very generously supported this course we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class So you can feel free to use these for the assignments and also for the course projects when you want to start using GPUs and larger machines and whatnot We'll post more details about that probably on Piazza later today But I just wanted to mention because I know there had been a couple of questions about can I use my laptop Do I have to run on Corn Do I have to whatever And the answer is that you'll be able to run on Google Cloud and we'll provide you some coupons for that Yeah so those are kind of the major administrative issues I wanted to talk about today And then let's dive into the content So the last lecture we talked a little bit about this task of image classification which is really a core task in computer vision And this is something that we'll really focus on throughout the course of the class is exactly how do we work on this image classification task So a little bit more concretely when you're doing image classification you receive your system receives some input image which is this cute cat in this example And the system is aware of some predetermined set of categories or labels So these might be like a dog or a cat or a truck or a plane And there's some fixed set of category labels And the job of the computer is to look at the picture and assign it one of these fixed category labels This seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks But this is actually a really really hard problem for a machine So if you dig in and think about actually what does a computer see when it looks at this image It definitely doesn't get this holistic idea of a cat that you see when you look at it And the computer really is representing the image as this gigantic grid of numbers So you just so the image might be something like 800 by 600 pixels And each pixel is represented by three numbers giving the red green and blue values for that pixel So to the computer this is just a gigantic grid of numbers And it's very difficult to distill the cat ness out of this giant array of thousands or whatever very many different numbers And this so we refer to this problem as the semantic gap That this idea of a cat or this label of a cat is a semantic label that we're assigning to this image And there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing And this is a really hard problem because you can change the picture in very small subtle ways that will cause this pixel grid to change entirely So for example if we took this same cat and if the cat happened to sit still and not even twitch not move a muscle which is never gonna happen but we moved the camera to the other side then every single grid every single pixel in this giant grid of numbers would be completely different But somehow it's still representing the same cat And our algorithms need to be robust to this But not only viewpoint is one problem another is illumination There can be different lighting conditions going on in the scene Whether the cat is appearing in this very dark moody scene or in this very bright sunlit scene it's still a cat And our algorithms need to be robust to that Objects can also deform I think cats are maybe among the more deformable of animals that you might see out there And cats can really assume a lot of different varied poses and positions And our algorithms should be robust to these different kinds of transforms There can also be problems of occlusion where you might only see part of a cat like just the face or in this extreme example just a tail peeking out from under the couch cushion But in these cases it's pretty easy for you as a person to realize that this is probably a cat and you still recognize these images as cats And this is something that our algorithms also must be robust to which is quite difficult I think There can also be problems of background clutter where maybe the foreground object the could actually look quite similar in appearance to the background And this is another thing that we need to handle There's also this problem of inter class variation that this one notion of catness actually spans a lot of different visual appearances And cats can come in different shapes and sizes and colors and ages And our algorithm again needs to work and handle all these different variations So this is actually a really really challenging problem And it's sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things But now if we want our computer programs to deal with all of these problems all simultaneously and not just for cats by the way but for just about any object category you could imagine this is a fantastically challenging problem And it's actually somewhat miraculous that this works at all in my opinion But actually not only does it work but these things work very close to human accuracy in some limited situations And take maybe only hundreds of milliseconds to do so So this is some pretty amazing incredible technology in my opinion And over the course of the rest of the class we'll really see what kinds of advancements have made this possible So now if you kind of think about what is the API for writing an image classifier you might sit down and try to write a method in Python like this where you want to take in an image and then do some crazy magic and then eventually spit out this class label to say cat or dog or whatnot And there's really no obvious way to do this right Like if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or even do something like RSA encryption you sort of can write down an algorithm and enumerate all the steps that need to happen in order for these things to work But when we're trying to recognize objects or recognize cats or images there's no really clear explicit algorithm that makes intuitive sense for how you might go about recognizing these objects So this is again quite challenging If you think about like if you knew nothing if it was your first day programming and you had to sit down and write this function I think most people would be in trouble That being said people have definitely made explicit attempts to try to write sort of hand coded rules for recognizing different animals So we touched on this a little bit in the last lecture But maybe one idea for cats is that you know we know that cats have ears and eyes and mouths and noses and we know that edges are from Hubel and Wiesel we know that edges are pretty important when it comes to visual recognition So one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries and say that you know if we have maybe three lines meeting this way then it might be a corner in an ear has one corner here and one corner there and one corner there and then kind of write down this explicit set of rules for recognizing cats But this turns out not to work very well One it's super brittle and two say if you want to start over for another object category and maybe not worry about cats but talk about trucks or dogs or fishes or something else then you need to start all over again So this is really not a very scalable approach We want to come up with some algorithm or some method for these recognition tasks which scales much more naturally to all the variety of objects in the world So the insight that sort of makes this all work is this idea of the data driven approach is that rather than sitting down and writing these hand specified rules to try to craft exactly what is a cat or a fish or what have you instead we'll go out onto the internet and collect a large data set of many many cats and many many airplanes and many many deer and different things like this And we can actually use tools like Google Image Search or something like that to do that We can use tools like that to go out and collect a very large number of examples of these different categories By the way this actually takes quite a lot of effort to go out and actually collect these data sets but luckily there's a lot of really good high quality data sets out there already for you to use Then once we get this data set we train this machine learning classifier that is gonna ingest all of the data summarize it in some way and then spit out a model that summarizes the knowledge of how to recognize these different object categories Then finally we'll use this train model and apply it on new images that will then be able to recognize cats and dogs and whatnot So here our API has changed a little bit Rather than a single function that just inputs an image and recognizes a cat we have these two functions One that's gonna called train that's gonna input images and labels and then output a model and then separately another function called predict which will input the model and then make predictions for images This is kind of the key insight that allowed all these things to start working really well in the last over the last 10 20 years or so So the first so this class is primarily about neural networks and convolutional neural networks and deep learning and all that but there's this idea of a data driven approach is much more general than just deep learning and I think it's useful to sort of step through this process for a very simple classifier first before we get to these big complex ones So probably the simplest classifier you can imagine is something we call nearest neighbor The algorithm is pretty dumb honestly So during the training step we won't do anything We'll just memorize all of the training data So this is very simple And now during the prediction step we're gonna take some new image and go and try to find the most similar image in the training data to that new image and now predict the label of that most similar image Very simple algorithm but it sort of has a lot of these nice properties with respect to data drivenness and whatnot So to be a little bit more concrete you might imagine working on this dataset called CIFAR 10 which is very commonly used in machine learning as kind of a small test case and you'll be working with this dataset on your homework So the CIFAR 10 dataset gives you 10 different classes airplanes and automobiles and birds and cats and different things like that and for each of those 10 categories it provides 10 000 sorry it provides 50 000 training images roughly evenly distributed across these 10 categories and then 10 000 additional testing images that you're supposed to test your algorithm on So now if you think about so here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR 10 So on this grid on the right for the leftmost column gives a test image in the CIFAR 10 dataset and now on the right we see we've sorted the training images and show the most similar training images to each of these test examples and you can see that they look kind of visually similar to the training images although they are not always correct right So maybe on this second row we see that the testing this is kind of hard to see because these images are 32 by 32 pixels you need to really dive in there and try to make your best guess but this image is a dog and its nearest neighbor is also a dog but this next one I think is actually a deer or a horse or something else but you can see that it looks quite visually similar because there's kind of a white blob in the middle and whatnot So if we're applying the nearest neighbor algorithm to this image we'll find the closest example in the training set and now the closest example we know it's label because it comes from the training set and now we'll simply say that this testing image is also a dog You can see kind of from these examples that this is probably not gonna work very well but it's still kind of a nice example to work through But then one detail that we need to know is given a pair of images how can we actually compare them Because if we're gonna take our test image and compare it to all the training images we actually have many different choices for exactly what that comparison function should look like So in the example in the previous slide we've used what's called the L1 distance also sometimes called the Manhattan distance So this is a really sort of simple easy idea for comparing images and that's that we're gonna take the just compare individual pixels in these images So supposing that our test image is maybe just a tiny four by four image of pixel values then we're gonna take this upper left hand pixel of the test image subtract off the value in the training image take the absolute value and get the difference in that pixel between the two images And then sum all these up across all the pixels in the image So this is kind of a stupid way to compare images but it does some reasonable things sometimes But this gives us a very concrete way to measure the difference between two images And in this case we have this difference of 456 between these two images So here's some full Python code for implementing this nearest neighbor classifier And you can see it's actually pretty short and pretty concise because we've made use of many of these vectorized operations offered by NumPy So here we can see that the training this train function that we talked about earlier is again very simple in the case of nearest neighbor You just memorize the training data There's not really much to do here And now at test time we're gonna take in our image and then go in and compare using this L1 distance function our test image to each of these training examples and find the most similar example in the training set And you can see that we're actually able to do this in just one or two lines of Python code by utilizing these vectorized operations in NumPy So this is something that you'll get practice with on the first assignment So now a couple of questions about this simple classifier First if we have n examples in our training set then how fast can we expect training and testing to be Well training is probably constant because we don't really need to do anything We just need to memorize the data And if you're just copying a pointer that's gonna be constant time no matter how big your data set is But now at test time we need to do this comparison step and compare our test image to each of the n training examples in the data set And this is actually quite slow So this is actually somewhat backwards if you think about it because in practice we want our classifiers to be slow at training time and then fast at testing time because you might imagine that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good But then when you go and deploy the classifier at test time you want it to run on your mobile phone or in the browser or some other low power device and you really want the testing time performance of your classifier to be quite fast So from this perspective this nearest neighbor algorithm is actually a little bit backwards and we'll see that once we move to convolutional neural networks and other types of parametric models they'll be the reverse of this where you'll spend a lot of compute at training time but then they'll be quite fast at testing time So then the question is what exactly does this nearest neighbor algorithm look like when you apply it in practice So here we've drawn what we call the decision regions of a nearest neighbor classifier So here our training set consists of these points in the two dimensional plane where the color of the point represents the category or the class label of that point So here we see we have five classes and some blue ones up in the corner here some purple ones in the upper right hand corner and now for each pixel in this entire plane we've gone and computed what is the nearest training what is the nearest example in these training data and then colored the point of the background corresponding to what is the class label So you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points But this classifier is maybe not so great and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier For one this central region actually contains mostly green points but one little yellow point in the middle But because we're just looking at the nearest neighbor this causes a little yellow island to appear in this middle of the green cluster and that's maybe not so great Maybe those points actually should have been green And then similarly we also see these sort of fingers of the different like the green region pushing into the blue region again due to the presence of one point which may have been noisy or spurious So this kind of motivates a slight generalization of this algorithm called k nearest neighbors So rather than just looking for the single nearest neighbor instead we'll do something a little bit fancier and find k of our nearest neighbors according to our distance metric and then take a vote among each of our neighbors and then predict the majority vote among our neighbors You can imagine slightly more complex ways of doing this Maybe you vote weighted on the distance or something like that but the simplest thing that tends to work pretty well is just taking a majority vote So here we've shown the exact same set of points using this k equals one nearest neighbor classifier as well as k equals three and k equals five in the middle and on the right And once we move to k equals three you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow Now this entire green portion in the middle is all being classified as green You can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting And then once we move to the k equals five case then these decision boundaries between the blue and red regions have become quite smooth and quite nice So this is generally something so generally when you're using nearest neighbor classifiers you almost always want to use some value of k which is larger than one because this tends to smooth out your decision boundaries and lead to better results So if we to kind of oh yeah question Yeah so the question is what is the deal with these white regions And those are the white regions are where there where there was no majority among the k nearest neighbors You could imagine maybe doing something slightly fancier and maybe taking a guess or randomly randomly selecting among the majority winners But for this simple example we're just coloring it white to indicate that there was no nearest neighbor in those places So we already kind of saw so I like to whenever we're thinking about computer vision I think it's really useful to kind of flip back and forth between several different viewpoints One is this idea of high dimensional points in the plane and then the other is actually looking at concrete images because the pixels of the image actually allow us to think of these images as high dimensional vectors And it's sort of useful to ping pong back and forth between these two different viewpoints So then when we're thinking about sort of taking this k nearest neighbor and going back to the images you can see that it's actually not very good Here I've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor And you can see that it's really not very good But maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row And you could imagine that that would end up being a lot more robust to some of these some of this noise that we see when retrieving neighbors in this way \n"
     ]
    }
   ],
   "source": [
    "SCRIPT = '../scripts'\n",
    "import os\n",
    "import json\n",
    "\n",
    "word_timestamp_path = os.path.join(SCRIPT, f\"test_text_word.json\")\n",
    "seg_timestamp_path = os.path.join(SCRIPT, f\"test_text_segment.json\")\n",
    "\n",
    "with open(word_timestamp_path, \"r\") as f:\n",
    "    word_timestamp = json.load(f)\n",
    "\n",
    "with open(seg_timestamp_path, \"r\") as f:\n",
    "    seg_timestamp = json.load(f)\n",
    "\n",
    "# print(f\"단어 타임스탬프 개수: {len(word_timestamp)}\")\n",
    "count = 0\n",
    "for seg in seg_timestamp:\n",
    "    text = seg[\"text\"]\n",
    "    count += len(text.split())\n",
    "\n",
    "script = \"\"\n",
    "for word in word_timestamp:\n",
    "    text = word[\"word\"]\n",
    "    script += text + \" \"\n",
    "\n",
    "print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"flexudy/t5-small-wav2vec2-grammar-fixer\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "input_text = script\n",
    "\n",
    "# Split the input text into chunks if it exceeds the model's max length\n",
    "max_len = 512  # T5 max token length\n",
    "tokens = tokenizer.encode(\"fix: \" + input_text, return_tensors=\"pt\")\n",
    "\n",
    "if len(tokens[0]) > max_len:\n",
    "    # Truncate or split the text into smaller chunks\n",
    "    chunks = [input_text[i:i+max_len] for i in range(0, len(input_text), max_len)]\n",
    "else:\n",
    "    chunks = [input_text]\n",
    "\n",
    "full_output = \"\"\n",
    "\n",
    "for chunk in chunks:\n",
    "    input_ids = tokenizer.encode(\"fix: \" + chunk, return_tensors=\"pt\", max_length=max_len, truncation=True)\n",
    "    outputs = model.generate(input_ids, max_length=max_len, num_beams=4, early_stopping=True)\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    full_output += output_text + \" \"\n",
    "\n",
    "print(\"Text with Punctuation:\\n\", full_output.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(script.split()), len(full_output.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# SpaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 단어 단위로 조합한 텍스트\n",
    "script = \"This is an example text without punctuation marks And here is another sentence\"\n",
    "\n",
    "# SpaCy로 텍스트 처리\n",
    "doc = nlp(script)\n",
    "\n",
    "# 문장 기호 추가\n",
    "processed_text = \"\"\n",
    "for sent in doc.sents:\n",
    "    processed_text += sent.text.strip() + \". \"  # 문장 끝에 온점을 추가하고 공백 추가\n",
    "\n",
    "print(processed_text.strip())  # 마지막 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"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\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "content = (\n",
    "    \"Please add punctuation to the following script. \"\n",
    "    f\"Script: {script}\"\n",
    ")\n",
    "\n",
    "\n",
    "# OpenAI GPT-4 API 요청 준비\n",
    "payload = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 8000,\n",
    "}\n",
    "\n",
    "# GPT-4 API 호출\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "response_data = response.json()\n",
    "\n",
    "# 응답에서 변환된 결과 추출\n",
    "if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "    transformed_result = response_data['choices'][0]['message']['content']\n",
    "    print(transformed_result)\n",
    "else:\n",
    "    print(\"Error: 'choices' key not found in the response\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(script.split()), len(transformed_result.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Mapping Lecture Script to Lecture Notes\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "\n",
    "file_paths = [os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(os.path.join(SCRIPT, f\"{project_id}_transcription.txt\"), \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "content = [{\"type\": \"text\", \"text\": (\n",
    "            \"Given the following lecture notes(pdf file) and the corresponding lecture script(txt file), \"\n",
    "            \"please distribute the script content accurately to each page of the lecture notes. \"\n",
    "            \"The output should be in the format: {\\\"1\\\": \\\"script\\\", \\\"2\\\": \\\"script\\\", ...}. \"\n",
    "            \"Each key should correspond to the page number in the lecture notes where the script content appears, \"\n",
    "            \"and the value should be the first sentence of the script content for that page. \"\n",
    "            \"The number of keys must be equal to the number of pages in the lecture notes. \"\n",
    "            \"Make sure there are no missing parts in the script\"\n",
    "        )}]\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": content,\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "message_content = messages[0].content[0].text\n",
    "\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "arr = [{\"pageNumber\": 1, \"start\": 0, \"end\": 1000}, {\"pageNumber\": 2, \"start\": 1000, \"end\": 3000}, {\"pageNumber\": 1, \"start\": 3000, \"end\": 4000}, {\"pageNumber\": 2, \"start\": 4000, \"end\": 5000}, {\"pageNumber\": 3, \"start\": 5000, \"end\": 6000}] \n",
    "\n",
    "dic = defaultdict(list)\n",
    "for ele in arr:\n",
    "    dic[ele[\"pageNumber\"]].append({\"start\": ele[\"start\"] / 100, \"end\": ele[\"end\"] / 100})\n",
    "\n",
    "print(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "arr = [\n",
    "    {\"pageNumber\": 1, \"start\": 0, \"end\": 1000},\n",
    "    {\"pageNumber\": 2, \"start\": 1000, \"end\": 2000},\n",
    "    {\"pageNumber\": 1, \"start\": 2000, \"end\": 4000},\n",
    "    {\"pageNumber\": 2, \"start\": 4000, \"end\": 5000},\n",
    "    {\"pageNumber\": 3, \"start\": 5000, \"end\": 6000}\n",
    "]\n",
    "\n",
    "# defaultdict를 사용하여 pageNumber별로 리스트 생성\n",
    "dic = defaultdict(list)\n",
    "for ele in arr:\n",
    "    dic[ele[\"pageNumber\"]].append(ele)\n",
    "\n",
    "# start와 end의 차이가 가장 큰 요소만 남기기\n",
    "result = {}\n",
    "for page_number, elements in dic.items():\n",
    "    # start와 end의 차이를 계산하여 최대값을 찾기\n",
    "    max_diff_element = max(elements, key=lambda x: x[\"end\"] - x[\"start\"])\n",
    "    result[str(page_number)] = {\"start\": max_diff_element[\"start\"], \"end\": max_diff_element[\"end\"]}\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n",
    "\n",
    "# to get the list of pre-trained models\n",
    "PunctuationCapitalizationModel.list_available_models()\n",
    "\n",
    "# Download and load the pre-trained BERT-based model\n",
    "model = PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_bert\")\n",
    "\n",
    "# try the model on a few examples\n",
    "model.add_punctuation_capitalization(['how are you', 'great how about you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-09-02 02:15:19 nemo_logging:349] /Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task punctuation-restoration, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 문장부호 복원 파이프라인 생성\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m punctuation_restoration \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpunctuation-restoration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moliverguhr/fullstop-punctuation-multilang-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 문장부호를 추가할 텍스트\u001b[39;00m\n\u001b[1;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere is a long text without punctuation it is meant to test how well simple rules can perform in adding punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:780\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    777\u001b[0m             class_ref, model, revision\u001b[38;5;241m=\u001b[39mrevision, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token\n\u001b[1;32m    778\u001b[0m         )\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 780\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:499\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict, Any]:\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m \n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1215\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1215\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task punctuation-restoration, available tasks are ['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-segmentation', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 문장부호 복원 파이프라인 생성\n",
    "punctuation_restoration = pipeline(\n",
    "    \"punctuation-restoration\",\n",
    "    model=\"oliverguhr/fullstop-punctuation-multilang-large\"\n",
    ")\n",
    "\n",
    "# 문장부호를 추가할 텍스트\n",
    "text = \"here is a long text without punctuation it is meant to test how well simple rules can perform in adding punctuation\"\n",
    "\n",
    "# 문장부호 복원\n",
    "punctuated_text = punctuation_restoration(text)\n",
    "\n",
    "print(punctuated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 텍스트를 토큰화합니다.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 모델을 통해 토큰화된 텍스트의 토큰들을 예측합니다.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:301\u001b[0m, in \u001b[0;36mBatchEncoding.tokens\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mReturn the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03minteger indices) at a given batch index (only works for the output of a fast tokenizer).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    `List[str]`: The list of tokens at that index.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[0;31mValueError\u001b[0m: tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# 사전에 훈련된 토큰 분류 모델을 로드합니다 (여기서는 대체 모델을 사용해야 할 수도 있습니다).\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"here is an example without proper punctuation can you help me fix it\"\n",
    "\n",
    "# 텍스트를 토큰화합니다.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", is_split_into_words=True)\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "# 모델을 통해 토큰화된 텍스트의 토큰들을 예측합니다.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 결과를 해석하여 문장부호를 추가합니다.\n",
    "# 이 코드는 예시로, 실제 사용 시 각 토큰의 ID에 맞는 문장부호 매핑이 필요합니다.\n",
    "output_tokens = [token for token, pred in zip(tokens, predictions[0]) if pred != 0]\n",
    "punctuated_text = ' '.join(output_tokens)\n",
    "\n",
    "print(punctuated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-09-02 02:27:48 nemo_logging:349] /Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Clara and I live in Berkeley, California. Ist das eine Frage, Frau Müller?\n"
     ]
    }
   ],
   "source": [
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "model = PunctuationModel()\n",
    "text = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\n",
    "result = model.restore_punctuation(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4572 4572\n"
     ]
    }
   ],
   "source": [
    "result = model.restore_punctuation(script)\n",
    "print(len(script.split()), len(result.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
