{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf\n",
    "import json\n",
    "\n",
    "PDF = \"../pdfs\"\n",
    "SPM = \"../spms\"\n",
    "CROP = \"../crops\"\n",
    "ANNOTATIONS = \"../annotations\"  \n",
    "\n",
    "project_id = 10\n",
    "pdf_path = os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")\n",
    "spm_path = os.path.join(SPM, f\"15_page_info.json\")\n",
    "annnotation_path = os.path.join(ANNOTATIONS, f\"{project_id}_annotation.json\")\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "with open(annnotation_path, \"r\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "output = page_info\n",
    "\n",
    "doc = pymupdf.open(pdf_path)\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    text = page.get_text(\"text\")  \n",
    "    images_info = page.get_image_info(xrefs=True)  \n",
    "    image_path = os.path.join(CROP, str(project_id), str(page_num + 1)) \n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    crop_images = []\n",
    "    for image_index, img_info in enumerate(images_info):\n",
    "        xref = img_info['xref']  # 이미지의 xref 값\n",
    "        base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "        image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "        crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")  \n",
    "        crop_images.append(crop_path)\n",
    "        # 이미지 저장\n",
    "        with open(crop_path, \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "\n",
    "    output[\"pages\"][str(page_num+1)][\"pdf_text\"] = text    \n",
    "    output[\"pages\"][str(page_num+1)][\"pdf_images\"] = crop_images\n",
    "    output[\"pages\"][str(page_num+1)][\"annotation\"] = annotations[str(page_num+1)]\n",
    "\n",
    "with open(spm_path, 'w') as file:\n",
    "    json.dump(output, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS = \"../annotations\"  \n",
    "\n",
    "annnotation_path = os.path.join(ANNOTATIONS, f\"{project_id}_annotation.json\")\n",
    "\n",
    "with open(annnotation_path, \"r\") as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 이미지를 로드하고 마스크를 적용\u001b[39;00m\n\u001b[1;32m     27\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(BytesIO(image_bytes))\n\u001b[0;32m---> 28\u001b[0m mask_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m image\u001b[38;5;241m.\u001b[39mputalpha(mask_image)  \u001b[38;5;66;03m# 마스크 적용\u001b[39;00m\n\u001b[1;32m     31\u001b[0m image\u001b[38;5;241m.\u001b[39msave(crop_path)  \u001b[38;5;66;03m# 덮어쓰기\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc.load_page(page_num)\n",
    "    images_info = page.get_image_info(xrefs=True)  \n",
    "    image_path = os.path.join(CROP, str(project_id), str(page_num + 1)) \n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    crop_images = []\n",
    "    for image_index, img_info in enumerate(images_info):\n",
    "        xref = img_info['xref']  # 이미지의 xref 값\n",
    "        base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "        image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "        crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")  \n",
    "        crop_images.append(crop_path)\n",
    "        # 이미지 저장\n",
    "        with open(crop_path, \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "        \n",
    "    print(page_num, crop_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../crops/10/10/4.png\n",
      "../crops/10/10/2.png\n",
      "../crops/10/10/3.png\n",
      "../crops/10/10/1.png\n",
      "Similarity results:\n",
      "2.png: 0.3069\n",
      "1.png: 0.2714\n",
      "4.png: 0.2637\n",
      "3.png: 0.2379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def calculate_clip_similarity(image_dir, search_query):\n",
    "    # CLIP 모델 및 프로세서 로드\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # 검색어를 텍스트 토큰으로 변환\n",
    "    text_inputs = processor(text=[search_query], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    similarities = {}\n",
    "\n",
    "    # 이미지 디렉터리 내의 모든 이미지 파일에 대해 유사도 계산\n",
    "    for image_name in os.listdir(image_dir):\n",
    "        if image_name.lower().endswith('png'):\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            print(image_path)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # 이미지를 CLIP 모델의 입력 형식으로 변환\n",
    "            image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # CLIP 모델을 사용하여 이미지와 텍스트의 특성 벡터 추출\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**image_inputs)\n",
    "                text_features = model.get_text_features(**text_inputs)\n",
    "\n",
    "            # 이미지와 텍스트 특성 간의 코사인 유사도 계산\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (image_features @ text_features.T).item()  # 유사도 계산\n",
    "\n",
    "            similarities[image_name] = similarity\n",
    "\n",
    "    # 유사도 결과를 내림차순으로 정렬\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"Similarity results:\")\n",
    "    for image_name, similarity in sorted_similarities:\n",
    "        print(f\"{image_name}: {similarity:.4f}\")\n",
    "\n",
    "# 사용 예시\n",
    "image_dir = \"../crops/10/10\"  # 이미지가 저장된 디렉터리\n",
    "search_query = \"a standing cat\"  # 검색어\n",
    "\n",
    "calculate_clip_similarity(image_dir, search_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.3068891167640686\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"../crops/10/10/2.png\")).unsqueeze(0).to(device) # 원하는 이미지 경로로 수정\n",
    "text = clip.tokenize(\"a standing cat\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    # logits_per_image, logits_per_text = model(image, text)\n",
    "    # probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    # 코사인 유사도를 계산\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (image_features @ text_features.T).item()  # 유사도 값 계산\n",
    "\n",
    "# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8880940675735474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 두 텍스트 정의\n",
    "text1 = \"This is a sample sentence.\"\n",
    "text2 = \"This sentence is similar to a sample sentence.\"\n",
    "\n",
    "# 텍스트 임베딩 계산\n",
    "embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "print(f\"Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# 모델 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')  # Sentence Transformers 모델\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def calculate_similarity(data, query):\n",
    "    # Query 텍스트 임베딩 계산\n",
    "    query_embedding = text_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for page, content in data.items():\n",
    "        page_result = {}\n",
    "        \n",
    "        # text 유사도 계산\n",
    "        text_embedding = text_model.encode(content[\"script\"], convert_to_tensor=True)\n",
    "        page_result[\"script\"] = util.pytorch_cos_sim(query_embedding, text_embedding).item()\n",
    "\n",
    "        # pdf_text 유사도 계산\n",
    "        pdf_text_embedding = text_model.encode(content[\"pdf_text\"], convert_to_tensor=True)\n",
    "        page_result[\"pdf_text\"] = util.pytorch_cos_sim(query_embedding, pdf_text_embedding).item()\n",
    "\n",
    "        # pdf_images 유사도 계산 (CLIP 사용)\n",
    "        image_similarities = []\n",
    "        for image_path in content[\"pdf_images\"]:\n",
    "            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(image)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Query 텍스트 임베딩도 CLIP을 사용해 계산\n",
    "                text_tokens = clip.tokenize([query]).to(device)\n",
    "                text_features = clip_model.encode_text(text_tokens)\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                similarity = (image_features @ text_features.T).item()\n",
    "                image_similarities.append(similarity)\n",
    "\n",
    "        # 여러 이미지가 있을 경우 평균 유사도 계산\n",
    "        if image_similarities:\n",
    "            page_result[\"pdf_image\"] = sum(image_similarities) / len(image_similarities)\n",
    "\n",
    "        results[page] = page_result\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "# 데이터 예시\n",
    "data = page_info\n",
    "\n",
    "query = \"administration\"  # 검색어\n",
    "\n",
    "result = {}\n",
    "# 유사도 계산\n",
    "similarities = calculate_similarity(data, query)\n",
    "result[\"query\"] = query\n",
    "result[\"similarities\"] = similarities\n",
    "\n",
    "SIM = \"../sims\"\n",
    "search_id = 1\n",
    "sim_path = os.path.join(SIM, str(project_id))\n",
    "os.makedirs(sim_path, exist_ok=True)\n",
    "sim_json_path = os.path.join(sim_path, f\"{search_id}.json\")\n",
    "\n",
    "with open(sim_json_path, 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strr = \"administration\"\n",
    "\n",
    "len(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text from OCR:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def perform_ocr_on_image(image_path):\n",
    "    # 이미지 파일을 열기\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # OCR을 수행하여 이미지에서 텍스트 추출\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    \n",
    "    # 추출된 텍스트 출력\n",
    "    print(\"Extracted Text from OCR:\")\n",
    "    print(extracted_text)\n",
    "\n",
    "image_path = \"../annotations/11/drawing_1_processed.png\"  \n",
    "perform_ocr_on_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([[84, 54], [230, 54], [230, 176], [84, 176]], 'APple', 0.30573700128243714)]\n",
      "APple\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "# EasyOCR Reader 초기화 (한국어 및 영어 지원)\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# 이미지 경로 설정\n",
    "image_path = \"../annotations/11/drawing_1_processed.png\"  \n",
    "\n",
    "# 이미지에서 텍스트 추출\n",
    "result = reader.readtext(image_path)\n",
    "\n",
    "print(result)\n",
    "# 결과 출력\n",
    "for res in result:\n",
    "    print(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = \"\"\n",
    "\n",
    "if strr:\n",
    "    print(\"True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "SPM = \"../spms\"\n",
    "SIMILARITY = \"../similarity\"\n",
    "\n",
    "project_id = 10\n",
    "search_id = 1\n",
    "search_type = \"keyword\" # \"semantic\" 또는 \"keyword\"\n",
    "spm_path = os.path.join(SPM, f\"{project_id}_page_info.json\")\n",
    "search_path = os.path.join(SIMILARITY, str(project_id), f\"{search_id}_{search_type}.json\")\n",
    "page_from_script = []\n",
    "page_from_pdf_text = []\n",
    "page_from_annotation = []\n",
    "\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "query = \"apple\"  # 검색어\n",
    "\n",
    "result = {}\n",
    "result[\"query\"] = query\n",
    "result[\"source\"] = defaultdict(list)\n",
    "\n",
    "for page, content in page_info[\"pages\"].items():\n",
    "    # script에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"script\"].lower():\n",
    "        result[\"source\"][\"script\"].append(page)\n",
    "    # pdf_text에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"pdf_text\"].lower():\n",
    "        result[\"source\"][\"pdf_text\"].append(page)\n",
    "    # annotation에서 검색어가 나타나는 페이지\n",
    "    if query.lower() in content[\"annotation\"].lower():\n",
    "        result[\"source\"][\"annotation\"].append(page)\n",
    "\n",
    "with open(search_path, 'w') as file:\n",
    "    json.dump(result, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import json\n",
    "import pymupdf\n",
    "\n",
    "PDF = \"../pdfs\"\n",
    "SPM = \"../spms\"\n",
    "CROP = \"../crops\"\n",
    "ANNOTATIONS = \"../annotations\"\n",
    "\n",
    "def get_pdf_text_and_image(para_id, pdf_path):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    page_num = int(para_id) - 1\n",
    "    page = doc.load_page(page_num)\n",
    "    text = page.get_text(\"text\")\n",
    "    images_info = page.get_image_info(xrefs=True)\n",
    "    image_path = os.path.join(CROP, str(project_id), str(page_num + 1))\n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "    crop_images = []\n",
    "    for image_index, img_info in enumerate(images_info):\n",
    "        xref = img_info[\"xref\"]  # 이미지의 xref 값\n",
    "        base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "        image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "        crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")\n",
    "        crop_images.append(crop_path)\n",
    "        # 이미지 저장\n",
    "        with open(crop_path, \"wb\") as img_file:\n",
    "            img_file.write(image_bytes)\n",
    "            \n",
    "    print(text)\n",
    "    print(crop_images)\n",
    "    return text, crop_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Downloads/cs231n_2017_lecture2 (1)-pages.pdf\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "1\n",
      "\n",
      "[]\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Administrative: Piazza\n",
      "For questions about midterm, poster session, projects, \n",
      "use Piazza instead of staff list!\n",
      "SCPD students: Use your @stanford.edu address to register for Piazza; contact \n",
      "scpd-customerservice@stanford.edu for help.\n",
      "2\n",
      "\n",
      "[]\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Administrative: Assignment 1\n",
      "Out tonight, due 4/18 11:59pm\n",
      "- K-Nearest Neighbor\n",
      "- Linear classifiers: SVM, Softmax\n",
      "- Two-layer neural network\n",
      "- Image features\n",
      "3\n",
      "\n",
      "[]\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Administrative: Python + Numpy\n",
      "4\n",
      "http://cs231n.github.io/python-numpy-tutorial/ \n",
      "\n",
      "['../crops/15/4/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Administrative: Google Cloud\n",
      "5\n",
      "http://cs231n.github.io/gce-tutorial/ \n",
      "\n",
      "['../crops/15/5/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Image Classification: A core task in Computer Vision\n",
      "6\n",
      "cat\n",
      "(assume given set of discrete labels)\n",
      "{dog, cat, truck, plane, ...}\n",
      "This image by Nikita is \n",
      "licensed under CC-BY 2.0\n",
      "\n",
      "['../crops/15/6/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "This image by Nikita is \n",
      "licensed under CC-BY 2.0\n",
      "The Problem: Semantic Gap\n",
      "7\n",
      "What the computer sees\n",
      "An image is just a big grid of \n",
      "numbers between [0, 255]:\n",
      "e.g. 800 x 600 x 3\n",
      "(3 channels RGB)\n",
      "\n",
      "['../crops/15/7/1.png', '../crops/15/7/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Challenges: Viewpoint variation\n",
      "8\n",
      "All pixels change when \n",
      "the camera moves!\n",
      "This image by Nikita is \n",
      "licensed under CC-BY 2.0\n",
      "\n",
      "['../crops/15/8/1.png', '../crops/15/8/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Challenges: Illumination\n",
      "9\n",
      "This image is CC0 1.0 public domain\n",
      "This image is CC0 1.0 public domain\n",
      "This image is CC0 1.0 public domain\n",
      "This image is CC0 1.0 public domain\n",
      "\n",
      "['../crops/15/9/1.png', '../crops/15/9/2.png', '../crops/15/9/3.png', '../crops/15/9/4.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Challenges: Deformation\n",
      "10\n",
      "This image by Umberto Salvagnin \n",
      "is licensed under CC-BY 2.0\n",
      "This image by Tom Thai is \n",
      "licensed under CC-BY 2.0 \n",
      "This image by sare bear is \n",
      "licensed under CC-BY 2.0\n",
      "This image by Umberto Salvagnin \n",
      "is licensed under CC-BY 2.0\n",
      "\n",
      "['../crops/15/10/1.png', '../crops/15/10/2.png', '../crops/15/10/3.png', '../crops/15/10/4.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Challenges: Occlusion\n",
      "11\n",
      "This image is CC0 1.0 public domain\n",
      "This image by jonsson is licensed \n",
      "under CC-BY 2.0\n",
      "This image is CC0 1.0 public domain\n",
      "\n",
      "['../crops/15/11/1.png', '../crops/15/11/2.png', '../crops/15/11/3.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "12\n",
      "This image is CC0 1.0 public domain\n",
      "Challenges: Background Clutter\n",
      "This image is CC0 1.0 public domain\n",
      "\n",
      "['../crops/15/12/1.png', '../crops/15/12/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Challenges: Intraclass variation\n",
      "13\n",
      "This image is CC0 1.0 public domain\n",
      "\n",
      "['../crops/15/13/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "An image classifier\n",
      "14\n",
      "Unlike e.g. sorting a list of numbers,\n",
      " \n",
      "no obvious way to hard-code the algorithm for \n",
      "recognizing a cat, or other classes.\n",
      "\n",
      "['../crops/15/14/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Attempts have been made\n",
      "15\n",
      "John Canny, “A Computational Approach to Edge Detection”, IEEE TPAMI 1986\n",
      "Find edges\n",
      "Find corners\n",
      "?\n",
      "\n",
      "['../crops/15/15/1.png', '../crops/15/15/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Data-Driven Approach\n",
      "16\n",
      "1. Collect a dataset of images and labels\n",
      "2. Use Machine Learning to train a classifier\n",
      "3. Evaluate the classifier on new images\n",
      "Example training set\n",
      "\n",
      "['../crops/15/16/1.png', '../crops/15/16/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "First classifier: Nearest Neighbor\n",
      "17\n",
      "Memorize all \n",
      "data and labels\n",
      "Predict the label \n",
      "of the most similar \n",
      "training image\n",
      "\n",
      "['../crops/15/17/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Example Dataset: CIFAR10\n",
      "18\n",
      " Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images”, Technical Report, 2009.\n",
      "10 classes\n",
      "50,000 training images\n",
      "10,000 testing images\n",
      "\n",
      "['../crops/15/18/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Example Dataset: CIFAR10\n",
      "19\n",
      " Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images”, Technical Report, 2009.\n",
      "10 classes\n",
      "50,000 training images\n",
      "10,000 testing images\n",
      "Test images and nearest neighbors\n",
      "\n",
      "['../crops/15/19/1.png', '../crops/15/19/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "Distance Metric to compare images\n",
      "20\n",
      "L1 distance:\n",
      "add\n",
      "\n",
      "['../crops/15/20/1.png', '../crops/15/20/2.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "21\n",
      "Nearest Neighbor classifier\n",
      "\n",
      "['../crops/15/21/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "22\n",
      "Nearest Neighbor classifier\n",
      "Memorize training data\n",
      "\n",
      "['../crops/15/22/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "23\n",
      "Nearest Neighbor classifier\n",
      "For each test image:\n",
      "  Find closest train image\n",
      "  Predict label of nearest image\n",
      "\n",
      "['../crops/15/23/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "24\n",
      "Nearest Neighbor classifier\n",
      "Q: With N examples, \n",
      "how fast are training \n",
      "and prediction?\n",
      "\n",
      "['../crops/15/24/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "25\n",
      "Nearest Neighbor classifier\n",
      "Q: With N examples, \n",
      "how fast are training \n",
      "and prediction?\n",
      "A: Train O(1),\n",
      "     predict O(N)\n",
      "\n",
      "['../crops/15/25/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "26\n",
      "Nearest Neighbor classifier\n",
      "Q: With N examples, \n",
      "how fast are training \n",
      "and prediction?\n",
      "A: Train O(1),\n",
      "     predict O(N)\n",
      "This is bad: we want \n",
      "classifiers that are fast \n",
      "at prediction; slow for \n",
      "training is ok\n",
      "\n",
      "['../crops/15/26/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "What does this look like?\n",
      "27\n",
      "\n",
      "['../crops/15/27/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "K-Nearest Neighbors\n",
      "28\n",
      "Instead of copying label from nearest neighbor, \n",
      "take majority vote from K closest points\n",
      "K = 1\n",
      "K = 3\n",
      "K = 5\n",
      "\n",
      "['../crops/15/28/1.png', '../crops/15/28/2.png', '../crops/15/28/3.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "29\n",
      "What does this look like?\n",
      "\n",
      "['../crops/15/29/1.png']\n",
      "Fei-Fei Li & Justin Johnson & Serena Yeung\n",
      "Lecture 2 - \n",
      "April 6, 2017\n",
      "30\n",
      "What does this look like?\n",
      "\n",
      "['../crops/15/30/1.png']\n"
     ]
    }
   ],
   "source": [
    "project_id = 15\n",
    "\n",
    "pdf_file = [\n",
    "        file\n",
    "        for file in os.listdir(PDF)\n",
    "        if file.startswith(f\"{project_id}_\") and file.endswith(\".pdf\")\n",
    "    ]    \n",
    "pdf_path = os.path.join(PDF, pdf_file[0])\n",
    "\n",
    "pdf_path = '/Users/yikim/Downloads/cs231n_2017_lecture2 (1)-pages.pdf'\n",
    "\n",
    "matched_file_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_file_path, \"r\") as matched_file:\n",
    "    matched_data = json.load(matched_file)\n",
    "\n",
    "print(pdf_path)\n",
    "\n",
    "for para_id, paragraph_text in matched_data.items():\n",
    "    get_pdf_text_and_image(para_id, pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Economics Lectures\n",
      "Part 1: Introduction\n",
      "Raj Chetty and Gregory A. Bruich\n",
      "Harvard University\n",
      "Fall 2012\n",
      "Public Economics Lectures\n",
      "()\n",
      "Part 1: Introduction\n",
      "1 / 49\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import json\n",
    "\n",
    "project_id = 13\n",
    "PDF = \"../pdfs\"\n",
    "SPM = \"../spms\"\n",
    "CROP = \"../crops\"\n",
    "ANNOTATIONS = \"../annotations\"\n",
    "\n",
    "pdf_file = [\n",
    "        file\n",
    "        for file in os.listdir(PDF)\n",
    "        if file.startswith(f\"{project_id}_\") and file.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "pdf_path = os.path.join(PDF, pdf_file[0])\n",
    "\n",
    "\n",
    "def get_pdf_text_and_image(project_id, pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to open the PDF file: {e}\")\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        images_info = page.get_images(full=True)\n",
    "        image_path = os.path.join(CROP, str(project_id), str(page_num + 1))\n",
    "        os.makedirs(image_path, exist_ok=True)\n",
    "\n",
    "        crop_images = []\n",
    "        for image_index, img_info in enumerate(images_info):\n",
    "            xref = img_info[0]  # 이미지의 xref 값, 첫 번째 요소로 위치를 가져옴\n",
    "            try:\n",
    "                base_image = doc.extract_image(xref)  # 이미지 데이터 추출\n",
    "                image_bytes = base_image[\"image\"]  # 이미지 바이트 데이터\n",
    "                crop_path = os.path.join(image_path, f\"{image_index + 1}.png\")\n",
    "                crop_images.append(crop_path)\n",
    "                # 이미지 저장\n",
    "                with open(crop_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_bytes)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping invalid xref {xref} on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return text, crop_images\n",
    "\n",
    "\n",
    "text, crop_images = get_pdf_text_and_image(project_id, pdf_path)\n",
    "\n",
    "print(text)\n",
    "print(crop_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "project_id = 13\n",
    "spm_path = os.path.join(SPM, f\"{project_id}_page_info.json\")\n",
    "with open(spm_path, 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "for para_id, para_info in page_info[\"pages\"].items():\n",
    "    if para_info[\"gpt_timestamp\"][\"start\"] < para_info[\"gpt_timestamp\"][\"end\"]:\n",
    "        print(para_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
