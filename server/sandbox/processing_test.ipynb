{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"keyword\": [\"computer vision\", \"learning algorithms\", \"lecture details\"],\n",
      "  \"formal\": \"Welcome to Lecture Two of CS231n. On Tuesday, we provided an overview of computer vision, its history, and a brief overview of this course. Today, we will explore the details for the first time. We will delve into the mechanics of some of these learning algorithms and see how they work in practice. The first lecture offered a broad overview, while most subsequent lectures will focus on specific details of various algorithms, including today's introduction to our first learning algorithm. Before proceeding, there are several administrative issues to address.\"\n",
      "}\n",
      "Script for page 1 saved to ../keywords/10/1_spm.json\n",
      "{\n",
      "  \"keyword\": [\"Piazza\", \"students\", \"communication\"],\n",
      "  \"formal\": \"One important aspect of our course communication is Piazza. Currently, around 500 students have signed up on Piazza, but there are still several hundred students who have not yet registered. We aim for Piazza to be the primary platform for communication between students and the core staff. Many inquiries regarding project ideas, midterm attendance, and poster session attendance have been sent to the staff list. Such questions should be directed to Piazza, as responses are likely to be faster due to the teaching assistants regularly monitoring it. Email inquiries might get lost in the shuffle if sent to the course list.\\n\\nIt has come to our attention that some SCPD students are experiencing difficulty signing up for Piazza. SCPD students are required to use a @stanford.edu email address. Once this email address is obtained, they can use it to sign into Piazza. This issue may not impact those attending in person, but it is relevant for students participating through SCPD.\"\n",
      "}\n",
      "Script for page 2 saved to ../keywords/10/2_spm.json\n",
      "{\n",
      "  \"keyword\": [\"assignment one\", \"k-nearest neighbor classifier\", \"linear classifiers\", \"svm\", \"softmax\", \"two-layer neural network\"],\n",
      "  \"formal\": \"The forthcoming administrative matter pertains to Assignment One. The first assignment will be released later today, likely in the afternoon; however, I assure you that it will be available before the end of the day. In the meantime, if you are eager to begin, you may refer to last year's version of Assignment One. The content will remain substantially the same, with minor adjustments such as updating to Python 3 from Python 2.7 and some minor cosmetic changes. The core content of the assignment remains unchanged from last year. \\n\\nKey points: \\n- In this assignment, students are required to implement their own k-nearest neighbor classifier, a topic that will be further discussed in today's lecture. \\n- Additionally, students will implement various linear classifiers, including Support Vector Machines (SVM) and Softmax classifiers, as well as a basic two-layer neural network. \\n- The content for these topics will be covered in the upcoming lectures.\"\n",
      "}\n",
      "Script for page 3 saved to ../keywords/10/3_spm.json\n",
      "{\n",
      "    \"keyword\": [\"python\", \"numpy\", \"vectorized operations\", \"numerical computing\", \"machine learning\"],\n",
      "    \"formal\": \"The utilization of Python and NumPy is essential for our coursework. For individuals unfamiliar with Python or NumPy, a comprehensive tutorial is available on the course website to facilitate your understanding. The significance of NumPy cannot be overstated, as it enables the execution of highly efficient vectorized operations, which allow significant computations to be performed with minimal lines of code. This efficiency is crucial for numerous aspects of numerical computing and machine learning. You will gain substantial practice in implementing these vectorized operations in the first assignment. Therefore, for those lacking experience with MATLAB, NumPy, or other forms of vectorized tensor computation, it is advisable to commence this assignment early and thoroughly review the tutorial.\"\n",
      "}\n",
      "Script for page 4 saved to ../keywords/10/4_spm.json\n",
      "{\n",
      "  \"keyword\": [\"Google Cloud\", \"virtual machines\", \"coupons\", \"GPUs\", \"course projects\"],\n",
      "  \"formal\": \"The primary topic I wish to address is the announcement that our class is now officially supported by Google Cloud. Google Cloud, which offers functionalities similar to Amazon AWS, allows users to initiate virtual machines in the cloud. These virtual machines are capable of utilizing GPUs. We are currently preparing a tutorial to guide you in using Google Cloud effectively for the course assignments. Our objective is to ensure a seamless process whereby you can download an image and effortlessly complete your assignments on a cloud instance. Owing to Google’s generous support for this course, each of you will receive coupons enabling free use of Google Cloud credits for the class. You are encouraged to utilize these credits for both assignments and course projects, particularly when GPU and larger machine resources are required. Further details will be posted on Piazza later today. This announcement addresses some recurring inquiries concerning the necessity of using personal laptops or specific university resources like Cornell servers. In summary, the administrative update covers the use of Google Cloud, and we will now proceed to the course content.\"\n",
      "}\n",
      "Script for page 5 saved to ../keywords/10/5_spm.json\n",
      "{\n",
      "  \"keyword\": [\"image classification\", \"computer vision\", \"visual recognition\", \"categories\", \"labels\"],\n",
      "  \"formal\": \"In the previous lecture, we discussed the task of image classification, which is a fundamental aspect of computer vision. This will be a focal point throughout the course. Specifically, we will explore the methodologies for addressing this task. \\n\\n- Concretely, image classification involves the following: \\n  - The system receives an input image, for instance, an image of a cat. \\n  - The system has access to a predefined set of categories or labels, which could include: \\n    - Dog \\n    - Cat \\n    - Truck \\n    - Plane \\n  - The objective of the system is to analyze the image and assign it one of these fixed category labels. \\n\\nAlthough this task may appear simple, as human visual systems are inherently adept at performing such recognition tasks, it presents significant challenges for machines.\"\n",
      "}\n",
      "Script for page 6 saved to ../keywords/10/6_spm.json\n",
      "{\n",
      "  \"keyword\": [\"semantic gap\", \"image representation\", \"pixel values\"],\n",
      "  \"formal\": \"The topic at hand involves the concept of the 'semantic gap' in computer vision. When a computer analyzes an image, it does not perceive the holistic concept of a 'cat' as a human observer would. Instead, the computer interprets the image as an extensive grid of numbers. For instance, an image with dimensions of 800 by 600 pixels would have each pixel represented by three numerical values corresponding to the red, green, and blue color channels. From the computer's perspective, this vast array of numbers constitutes the entire image. Extracting the 'cat-ness' from such an extensive numerical array is highly challenging. This challenge is referred to as the semantic gap, which represents the substantial disparity between the human-assigned semantic label of 'cat' and the pixel values the computer processes. This issue is exacerbated by the fact that even subtle modifications to the image can result in significant changes to the pixel grid.\"\n",
      "}\n",
      "Script for page 7 saved to ../keywords/10/7_spm.json\n",
      "{\n",
      "  \"keyword\": [\"camera\", \"pixel\", \"algorithm\", \"viewpoint\", \"illumination\"],\n",
      "  \"formal\": \"For instance, if we consider a scenario where a cat remains perfectly still, without any movement whatsoever—although this is highly unlikely—and we reposition the camera to the opposite side, every single element within this extensive grid of numbers, every single pixel, would be markedly different. Nonetheless, this grid would still portray the same cat. Thus, it is crucial for our algorithms to be adaptable to such changes. Moreover, viewpoint variability is merely one of the challenges; illumination also presents a significant issue.\"\n",
      "}\n",
      "Script for page 8 saved to ../keywords/10/8_spm.json\n",
      "{\n",
      "    \"keyword\": [\"lighting conditions\", \"cat\", \"algorithms\"],\n",
      "    \"formal\": \"There can be various lighting conditions present in any given scene. Whether the cat appears in a very dark and moody scene or in a very bright and sunlit setting, it remains a cat. Our algorithms need to demonstrate robustness under these varying conditions.\"\n",
      "}\n",
      "Script for page 9 saved to ../keywords/10/9_spm.json\n",
      "{\n",
      "  \"keyword\": [\"objects\", \"deform\", \"cats\", \"algorithms\", \"transforms\"],\n",
      "  \"formal\": \"Objects possess the capacity to deform. Cats, in particular, exemplify deformability among animals. They can adopt a wide range of poses and positions. Consequently, our algorithms must be robust and adaptable to these various transformations.\"\n",
      "}\n",
      "Script for page 10 saved to ../keywords/10/10_spm.json\n",
      "{\n",
      "    \"keyword\": [\"occlusion\", \"algorithms\"],\n",
      "    \"formal\": \"There can also be issues related to occlusion, wherein only a portion of a cat is visible; for instance, only the face or, in extreme cases, merely the tail protruding from beneath a couch cushion. However, it is relatively easy for humans to infer that this is likely a cat, and thus, they still identify these images as cats. It is also crucial for our algorithms to be robust enough to handle such scenarios, which, admittedly, is a challenging task.\"\n",
      "}\n",
      "Script for page 11 saved to ../keywords/10/11_spm.json\n",
      "{\n",
      "  \"keyword\": [\"background clutter\"],\n",
      "  \"formal\": \"**Challenges in Visual Recognition**\\n\\n- **Background Clutter**: Issues may arise when the background and the foreground object, such as a cat, have a similar appearance. This similarity can complicate the process of distinguishing the foreground object from the background.\\n- **Handling Mechanisms**: It is crucial to address such challenges to improve visual recognition systems.\"\n",
      "}\n",
      "Script for page 12 saved to ../keywords/10/12_spm.json\n",
      "{\n",
      "  \"keyword\": [\"intraclass variation\", \"visual appearances\", \"image classifier\"],\n",
      "  \"formal\": \"In addressing the complexities of image classification, one must consider the significant issue of intraclass variation. The concept of 'cat-ness,' for instance, encompasses a wide array of visual appearances, as cats exhibit diverse shapes, sizes, colors, and ages. An effective algorithm must accommodate and process these variations effectively, which is a notably challenging problem. \\n\\nOne may tend to overlook the difficulty of this task, given the human brain's adeptness at recognizing such variations effortlessly. However, developing computer programs capable of simultaneously managing these variances for all object categories is an exceedingly intricate endeavor. It is, in fact, quite astonishing that current technology can accomplish this task, achieving near-human accuracy under certain conditions within milliseconds. This remarkable capability is a testament to the advancements in this field, which will be explored in greater detail throughout the course.\\n\\nWhen conceptualizing the API required to develop an image classifier, one might envision a method in a programming language like Python. This method would involve inputting an image, performing complex processing, and subsequently outputting a class label such as 'cat' or 'dog.' However, devising a clear-cut approach to achieve this remains a formidable challenge.\"\n",
      "}\n",
      "Script for page 13 saved to ../keywords/10/13_spm.json\n",
      "{\n",
      "  \"keyword\": [\"algorithms class\", \"sort numbers\", \"compute a convex hull\", \"rsa encryption\", \"recognize objects\", \"recognize cats\", \"images\", \"programming\"],\n",
      "  \"formal\": \"When enrolled in an algorithms class, the task may involve sorting numbers, computing a convex hull, or implementing RSA encryption. These tasks allow one to write down an algorithm enumerating all necessary steps for functionality. However, recognizing objects or identifying cats in images lacks a clear, intuitive algorithm. This poses a significant challenge, especially for beginners in programming, as developing such functions without predefined algorithms can be quite daunting. Despite these difficulties, efforts have been made to establish high-level coded rules for recognizing various animals.\"\n",
      "}\n",
      "Script for page 14 saved to ../keywords/10/14_spm.json\n",
      "{\n",
      "  \"keyword\": [\"edges\", \"data-driven approach\", \"machine learning classifier\", \"dataset\"],\n",
      "  \"formal\": \"In the preceding lecture, we briefly touched upon the concept; however, we shall delve deeper into the notion of visual recognition, particularly in relation to felines. It is well understood that cats possess distinct anatomical features such as ears, eyes, mouths, and noses. According to the seminal work of Hubel and Wiesel, edges play a crucial role in visual recognition.\\n\\n- **Edge Computation for Image Analysis:** One proposed method involves computing the edges of a given image and subsequently categorizing various corners and boundaries. For instance, if three lines converge at a specific point, it may signify a corner. Consequently, one might hypothesize that an ear consists of multiple corners.\\n    - Challenge: This method is highly brittle and lacks versatility. Implementing it for different object categories (e.g., trucks, dogs, fishes) necessitates restarting the entire process, thereby rendering the approach non-scalable.\\n\\n- **Need for Scalable Algorithms:** There is a pressing need to develop algorithms or methodologies that can handle the vast diversity of objects in existence more naturally.\\n\\n- **Data-Driven Approach:** Rather than crafting hand-specified rules to define specific objects, a more efficacious strategy involves leveraging a data-driven paradigm. This entails:\\n    - Collecting extensive datasets from the internet, comprising numerous examples of various categories such as cats, airplanes, and deer. Tools like Google Image Search can facilitate this process.\\n        - Nota Bene: Accumulating such datasets demands considerable effort; nevertheless, many high-quality datasets are readily available for utilization.\\n\\n- **Training a Machine Learning Classifier:** Once an ample dataset is acquired, a machine learning classifier is trained to ingest and process the data, subsequently producing a model that encapsulates the knowledge required to recognize different object categories.\\n    - Deployment: The trained model is then applied to new images, empowering the system to recognize cats, dogs, and other objects.\\n\\nIn summary, the data-driven approach, bolstered by machine learning classifiers and extensive datasets, offers a scalable and efficient solution for object recognition.\"\n",
      "}\n",
      "Script for page 15 saved to ../keywords/10/15_spm.json\n",
      "{\n",
      "  \"keyword\": [\"API\", \"train\", \"predict\", \"model\", \"neural networks\", \"convolutional neural networks\", \"deep learning\", \"nearest neighbor\"],\n",
      "  \"formal\": \"The structure of our Application Programming Interface (API) has undergone some modifications. Instead of a singular function that inputs an image to recognize a cat, the API now comprises two distinct functions: \\n\\n- 'train': This function inputs images and their corresponding labels to produce a model.\\n- 'predict': This separate function takes the generated model and makes predictions for given images.\\n\\nThis bifurcation represents a critical innovation that has significantly enhanced performance over the past two decades. The focus of this class will be on neural networks, convolutional neural networks, and deep learning. However, the concept of a data-driven approach extends beyond deep learning, and it is beneficial to first understand this process through a simpler classifier before delving into more complex ones.\\n\\nFor instance, the most rudimentary classifier we can consider is known as the 'nearest neighbor' classifier.\"\n",
      "}\n",
      "Script for page 16 saved to ../keywords/10/16_spm.json\n",
      "{\n",
      "  \"keyword\": [\"algorithm\", \"training data\", \"prediction step\"],\n",
      "  \"formal\": \"The algorithm exhibits simplicity in its operation. \\n\\n- Training Step: \\n  - No specific actions are performed during the training phase. \\n  - The algorithm merely memorizes all of the training data provided. \\n\\n- Prediction Step: \\n  - The algorithm processes a new image and searches for the most similar image within the training data. \\n  - The label of this most similar image is predicted as the label for the new image. \\n\\nOverall, this straightforward algorithm demonstrates appealing properties related to data-drivenness.\"\n",
      "}\n",
      "Script for page 17 saved to ../keywords/10/17_spm.json\n",
      "{\n",
      "  \"keyword\": [\"cifar-10\", \"machine learning\", \"dataset\"],\n",
      "  \"formal\": \"The CIFAR-10 dataset is frequently employed in machine learning as a standard, small-scale test case. You will encounter this dataset in your homework assignments. The CIFAR-10 dataset comprises the following elements: \\n\\n- Ten distinct classes such as airplanes, automobiles, birds, cats, etc.\\n- Fifty thousand training images distributed roughly evenly across the ten classes\\n- Ten thousand supplementary testing images for algorithm evaluation\"\n",
      "}\n",
      "Script for page 18 saved to ../keywords/10/18_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor classifier\", \"cifar-10 dataset\", \"training images\", \"test images\"],\n",
      "  \"formal\": \"In this example, we apply the simple nearest neighbor classifier to a selection of test images from the CIFAR-10 dataset. The grid on the right illustrates the following sequence: \\n\\n- The leftmost column presents a test image from the CIFAR-10 dataset. \\n- The subsequent columns display training images, sorted by their visual similarity to the corresponding test image. \\n\\nObservation indicates that, although the training images appear visually similar to the test images, they are not always accurate matches. For instance: \\n\\n- The second row’s test image is a dog and its nearest neighbor in the training set is also a dog. However, the next image could be a deer or a horse, evidenced by similar visual features such as a central white blob. \\n\\nWhen employing the nearest neighbor algorithm: \\n\\n- We identify the closest example in the training set for a given test image. \\n- The label of the closest training example is then assigned to the test image.\\n\\nDespite the limitations in accuracy demonstrated by this method, it serves as a practical example for understanding the nearest neighbor approach. A critical detail to consider is the function used to compare pairs of images. When comparing a test image to the training images, there are multiple options available for defining this comparison function.\"\n",
      "}\n",
      "Script for page 19 saved to ../keywords/10/19_spm.json\n",
      "{\n",
      "  \"keyword\": [\"l1 distance\", \"manhattan distance\", \"comparing images\", \"pixel values\", \"test image\", \"training image\", \"absolute value\", \"difference\", \"sum\", \"measure\"],\n",
      "  \"formal\": \"In the example provided in the preceding slide, we utilized the L1 distance, also known as the Manhattan distance. This method offers a straightforward and accessible technique for comparing images. Essentially, this approach involves comparing individual pixels in the images. For instance, if our test image is a small four-by-four pixel image, we execute the following steps: \\n\\n- We examine the upper-left-hand pixel of the test image.\\n- We subtract the corresponding pixel value in the training image.\\n- We compute the absolute value of the difference between these corresponding pixels.\\n- We sum the absolute differences across all pixels in the image.\\n\\nAlthough this method might seem rather simplistic and rudimentary, it can occasionally yield reasonable results. Importantly, it provides a tangible metric for quantifying the difference between two images. In this specific example, the quantified difference between the two images is 456.\"\n",
      "}\n",
      "Script for page 20 saved to ../keywords/10/20_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor classifier\", \"training function\", \"numpy\", \"l1 distance function\"],\n",
      "  \"formal\": \"Below is a full Python implementation of the nearest neighbor classifier, showcasing the conciseness achieved by utilizing vectorized operations provided by numpy.\\n\\n- Training Function: This earlier discussed component is straightforward in the context of the nearest neighbor classifier. It essentially involves memorizing the training data, with minimal additional processes.\\n- Test Time Operation: During this phase, an image is taken in and compared with each of the training examples using the L1 distance function. The objective is to identify the most similar example within the training set.\"\n",
      "}\n",
      "Script for page 21 saved to ../keywords/10/21_spm.json\n",
      "{\n",
      "  \"keyword\": [\"vectorized operations\", \"numpy\"],\n",
      "  \"formal\": \"In this lecture, it is demonstrated that the task at hand can be efficiently accomplished using just one or two lines of Python code by leveraging vectorized operations in NumPy. This concept will be further practiced in the first assignment.\"\n",
      "}\n",
      "Script for page 22 saved to ../keywords/10/22_spm.json\n",
      "{\n",
      "    \"keyword\": [\"simple classifier\", \"training set\", \"training\", \"testing\"],\n",
      "    \"formal\": \"Key considerations regarding the simple classifier include the following points:\\n\\n- Question 1: Given that there are n examples in our training set, what is the expected speed for training the model?\\n\\n- Question 2: Given the same circumstances, what is the expected speed for testing the model?\"\n",
      "}\n",
      "Script for page 23 saved to ../keywords/10/23_spm.json\n",
      "{\n",
      "    \"keyword\": [\"training\", \"test time\", \"comparison\"],\n",
      "    \"formal\": \"The training process is likely constant as it primarily involves memorizing the data without additional operations. Copying a pointer will take constant time regardless of the dataset size. However, during the testing phase, we must compare the test image against each of the n training examples in the dataset, which can be quite slow. This approach appears somewhat counterintuitive when considered thoroughly.\"\n",
      "}\n",
      "Script for page 24 saved to ../keywords/10/24_spm.json\n",
      "{\n",
      "  \"keyword\": [\"classifiers\", \"training time\", \"testing time\"],\n",
      "  \"formal\": \"In practical applications, it is preferable for classifiers to exhibit slower training times and faster testing times. This approach allows for extensive computation during the training phase, often conducted in a data center, thereby optimizing the classifier's performance. Subsequently, at deployment during testing, the classifier needs to operate efficiently on low-power devices such as mobile phones or web browsers. Ensuring rapid performance during testing is crucial.\"\n",
      "}\n",
      "Script for page 25 saved to ../keywords/10/25_spm.json\n",
      "{\n",
      "    \"keyword\": [\"nearest neighbor algorithm\", \"convolutional neural networks\", \"parametric models\", \"decision regions\"],\n",
      "    \"formal\": \"From this perspective, the nearest neighbor algorithm appears somewhat counterintuitive. Once we transition to convolutional neural networks and other types of parametric models, it will become evident that the approach is reversed. These models require significant computational effort during the training phase but operate efficiently during the testing phase. The next question to address is: What does the nearest neighbor algorithm look like in practical application? Here, we have illustrated the decision regions of a nearest neighbor classifier.\"\n",
      "}\n",
      "Script for page 26 saved to ../keywords/10/26_spm.json\n",
      "{\n",
      "  \"keyword\": [\"training set\", \"nearest neighbor classifier\", \"class label\", \"k-nearest neighbors\"],\n",
      "  \"formal\": \"In this context, our training set comprises several data points positioned in a two-dimensional plane. The hue of each point indicates its category or class label. As observed, there are five distinct classes, with a few blue points located in the upper corner and several purple points in the upper-right hand corner. For each pixel across the entire plane, we have computed the nearest example from our training data and subsequently colored the background point according to the corresponding class label. \\n\\n- The nearest neighbor classifier delineates and colors the space based on the proximity of neighboring points. \\n- This classifier, however, has notable limitations, as evidenced by the visual representation: \\n  - The central region primarily consists of green points, with a solitary yellow point prominently located in the center. This singular yellow point results in the formation of a small yellow island within the predominantly green cluster, which is suboptimal since the points arguably should be green. \\n  - Similarly, the green region extends into the blue region due to one potentially noisy or anomalous point. \\n\\nThese observations highlight the limitations of the nearest neighbor classifier, thereby motivating the exploration of a more refined approach known as k-nearest neighbors.\"\n",
      "}\n",
      "Script for page 27 saved to ../keywords/10/27_spm.json\n",
      "{\n",
      "  \"keyword\": [\"k-nearest neighbors\", \"majority vote\", \"distance metric\", \"decision boundaries\"],\n",
      "  \"formal\": \"In this lecture, we explored the concept of k-nearest neighbors (k-NN) classification, a fundamental method in machine learning. The key points are structured as follows:\\n\\n- **Single Nearest Neighbor vs. k-Nearest Neighbors**:\\n  - Rather than only identifying the single nearest neighbor, a more sophisticated approach involves finding k-nearest neighbors based on a chosen distance metric.\\n  - A majority vote among these neighbors is taken to predict the final classification.\\n  - More complex voting schemes could involve weighted voting based on distance, but a simple majority vote is often effective.\\n\\n- **Examples with k Values**:\\n  - **k=1**: Shows the classifier where only the single nearest neighbor is considered.\\n  - **k=3**: Eliminates spurious points within clusters, leading to a more accurate classification of the region.\\n  - **k=5**: Results in smoother and more consistent decision boundaries between different regions.\\n\\n- **General Recommendation**:\\n  - Generally, using a k-value larger than one is recommended as it smooths out decision boundaries and produces better results.\\n\\n- **Handling Ambiguities**:\\n  - White regions in the example denote areas where there was no majority among the k-nearest neighbors.\\n  - While more sophisticated methods could predict these ambiguous regions, the example uses a simple approach of indicating ambiguity with white.\\n\\n- **Computer Vision Example**:\\n  - It is beneficial to alternate between viewing high dimensional points and concrete images, as pixel data can be seen as high dimensional vectors.\\n  - Applying k-NN to images reveals the limitations of the method, highlighting that it may not always be effective.\"\n",
      "}\n",
      "Script for page 28 saved to ../keywords/10/28_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor\", \"classified correctly\", \"larger value of k\"],\n",
      "  \"formal\": \"In this analysis, diagrams have been marked in red and green to indicate whether images are classified correctly or incorrectly according to their nearest neighbor algorithm. It is evident that the results are suboptimal. However, a potential improvement could be achieved by increasing the value of k, thereby involving a voting process among the top three, top five, or potentially the entire row of nearest neighbors.\"\n",
      "}\n",
      "Script for page 29 saved to ../keywords/10/29_spm.json\n",
      "{\n",
      "  \"keyword\": [\"robust\", \"noise\", \"retrieving neighbors\"],\n",
      "  \"formal\": \"The following modifications could potentially enhance the robustness against the noise encountered during the retrieval of neighbors:\\n\\n- Imagine that implementing this approach would result in a significantly more robust system.\\n- Consider the reduction in noise when retrieving neighbors by utilizing this method.\"\n",
      "}\n",
      "Script for page 30 saved to ../keywords/10/30_spm.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "PDF = '../pdfs'\n",
    "TOC = '../tocs'\n",
    "IMAGE = '../images'\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "KEYWORD = '../keywords'\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        return json.load(script_file)\n",
    "\n",
    "# Path to your image and script\n",
    "project_id = 10\n",
    "script_path = os.path.join(SPM, \"test_matched_paragraphs.json\")\n",
    "keyword_path = os.path.join(KEYWORD, f\"{str(project_id)}\")\n",
    "os.makedirs(keyword_path, exist_ok=True)\n",
    "\n",
    "# Read the script\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Function to make API request for each page\n",
    "def make_api_request(page_number, script_segment):    \n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                \"Given the lecture script, identify at least one important keywords. \"\n",
    "                \"Next, transform the script into a more formal tone, breaking it down into a bullet point structure where appropriate. \"\n",
    "                \"The output should be in JSON format with the following structure: \"\n",
    "                \"{\\\"keyword\\\": [\\\"string\\\", \\\"string\\\", ...], \\\"formal\\\": \\\"string\\\"} \"\n",
    "                f\"lecture script: {script_segment} \"\n",
    "            )\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def filter_script_data(script_data):\n",
    "    allowed_keys = {\"keyword\", \"formal\"}\n",
    "    return {key: value for key, value in script_data.items() if key in allowed_keys}\n",
    "\n",
    "# Encode images and make API requests\n",
    "for page_num in script_content.keys():\n",
    "    page_number = str(page_num)\n",
    "    script_segment = script_content[page_number]\n",
    "    response_data = make_api_request(page_number, script_segment)\n",
    "    \n",
    "    # Process the response data\n",
    "    if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "        script_text = response_data['choices'][0]['message']['content']\n",
    "        print(script_text)\n",
    "        # Convert the script text to JSON format\n",
    "        try:\n",
    "            script_data = json.loads(script_text)\n",
    "            script_data = filter_script_data(script_data)\n",
    "            script_data[\"original\"] = script_segment\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for page {page_number}: {e}\")\n",
    "            script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "    else:\n",
    "        print(f\"Error: 'choices' key not found in the response for page {page_number}\")\n",
    "        print(response_data)\n",
    "        script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "    # Save the script data as a JSON file\n",
    "    script_json_path = os.path.join(keyword_path, f\"{page_number}_spm.json\")\n",
    "    with open(script_json_path, \"w\") as json_file:\n",
    "        json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Script for page {page_number} saved to {script_json_path}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
