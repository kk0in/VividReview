{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725356677.531870 59045724 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No text detected.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision_v1 import types\n",
    "\n",
    "def detect_handwritten_text(image_path):\n",
    "    # 환경 변수로 Google Cloud 서비스 계정 키 경로 설정\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../disco-beach-433010-q6-bf0ff037eb46.json'\n",
    "\n",
    "    # Google Cloud Vision 클라이언트 설정\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # 이미지 파일을 읽고 Vision API에 전송\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "    image_context = vision.ImageContext(language_hints=[\"en-t-i0-handwrit\"])\n",
    "\n",
    "    # 손글씨 인식 수행\n",
    "    response = client.document_text_detection(image=image, image_context=image_context)\n",
    "\n",
    "    # print(response)\n",
    "\n",
    "    # for page in response.full_text_annotation.pages:\n",
    "    #     for block in page.blocks:\n",
    "    #         print(f\"\\nBlock confidence: {block.confidence}\\n\")\n",
    "\n",
    "    #         for paragraph in block.paragraphs:\n",
    "    #             print(\"Paragraph confidence: {}\".format(paragraph.confidence))\n",
    "\n",
    "    #             for word in paragraph.words:\n",
    "    #                 word_text = \"\".join([symbol.text for symbol in word.symbols])\n",
    "    #                 print(\n",
    "    #                     \"Word text: {} (confidence: {})\".format(\n",
    "    #                         word_text, word.confidence\n",
    "    #                     )\n",
    "    #                 )\n",
    "\n",
    "    #                 for symbol in word.symbols:\n",
    "    #                     print(\n",
    "    #                         \"\\tSymbol: {} (confidence: {})\".format(\n",
    "    #                             symbol.text, symbol.confidence\n",
    "    #                         )\n",
    "    #                     )\n",
    "\n",
    "    # if response.error.message:\n",
    "    #     raise Exception(\n",
    "    #         \"{}\\nFor more info on error messages, check: \"\n",
    "    #         \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "    #     )\n",
    "\n",
    "    # 결과 출력\n",
    "    if response.text_annotations:\n",
    "        print(response.text_annotations[0].description)\n",
    "    else:\n",
    "        print('No text detected.')\n",
    "    \n",
    "    # 오류 처리    \n",
    "    if response.error.message:\n",
    "        raise Exception(f'{response.error.message}')\n",
    "\n",
    "image_path = \"../annotations/16/1.png\"  \n",
    "\n",
    "detect_handwritten_text(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def remove_transparency(image_path):\n",
    "    # PNG 이미지를 불러오기 (투명 배경 포함)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # 알파 채널이 있는지 확인\n",
    "    if image.shape[2] == 4:\n",
    "        # 알파 채널 분리\n",
    "        b, g, r, a = cv2.split(image)\n",
    "        \n",
    "        # 투명한 영역을 흰색 배경으로 채우기\n",
    "        alpha_inv = cv2.bitwise_not(a)\n",
    "        white_background = np.full_like(a, 255)\n",
    "        b = cv2.bitwise_or(b, white_background, mask=alpha_inv)\n",
    "        g = cv2.bitwise_or(g, white_background, mask=alpha_inv)\n",
    "        r = cv2.bitwise_or(r, white_background, mask=alpha_inv)\n",
    "\n",
    "        # 다시 합쳐서 BGR 이미지로 변환\n",
    "        image = cv2.merge([b, g, r])\n",
    "\n",
    "    # 저장된 이미지 반환\n",
    "    cv2.imwrite(image_path, image)\n",
    "    \n",
    "\n",
    "image_path = \"../annotations/11/drawing_2.png\"  \n",
    "remove_transparency(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dic = {\"1\":1, \"2\":2}\n",
    "\n",
    "print(len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
