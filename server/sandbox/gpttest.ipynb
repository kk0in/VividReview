{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "SCRIPT = './scripts'\n",
    "\n",
    "project_id = 10\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Assistant for Mapping Lecture Script to Lecture Notes\",\n",
    "  instructions=\"You are a helpful assistant designed to output JSON. Use your knowledge base to distribute the lecture script content accurately to each page of the lecture notes.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Mapping Lecture Script to Lecture Notes\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "\n",
    "file_paths = [os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_h6vZFefAGpflXRsgrA2PDlLf'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(os.path.join(SCRIPT, f\"{project_id}_transcription.txt\"), \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "content = [{\"type\": \"text\", \"text\": (\n",
    "            \"Given the following lecture notes(pdf file) and the corresponding lecture script(txt file), \"\n",
    "            \"please distribute the script content accurately to each page of the lecture notes. \"\n",
    "            \"The output should be in the format: {\\\"1\\\": \\\"script\\\", \\\"2\\\": \\\"script\\\", ...}. \"\n",
    "            \"Each key should correspond to the page number in the lecture notes where the script content appears, \"\n",
    "            \"and the value should be the first sentence of the script content for that page. \"\n",
    "            \"The number of keys must be equal to the number of pages in the lecture notes. \"\n",
    "            \"Make sure there are no missing parts in the script\"\n",
    "        )}]\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": content,\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided lecture script and notes, here is the script content accurately distributed to each page of the lecture notes:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"1\": \"Okay, so welcome to lecture two of CS231N.\",\n",
      "  \"2\": \"So, all of our assignments are using Python and NumPy.\",\n",
      "  \"3\": \"But, when we're trying to recognize objects, or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense, for how you might go about recognizing these objects.\",\n",
      "  \"4\": \"So, the insight that, sort of, makes this all work is this idea of the data-driven approach.\",\n",
      "  \"5\": \"So, for those of you who don't have a lot of experience with Matlab or NumPy or other types of vectorized tensor computation, I recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\",\n",
      "  \"6\": \"And this is something that we'll really focus on throughout the course of the class.\",\n",
      "  \"7\": \"And this is another thing that we need to handle.\",\n",
      "  \"8\": \"And now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image.\",\n",
      "  \"9\": \"So, from this perspective, this nearest neighbor algorithm, is, actually, a little bit backwards.\",\n",
      "  \"10\": \"So, once you get that email address, then you can use the Stanford email to sign into Piazza.\",\n",
      "  \"11\": \"And this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\",\n",
      "  \"12\": \"So, here's some full Python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by NumPy.\",\n",
      "  \"13\": \"But, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot.\",\n",
      "  \"14\": \"So, the insight that, sort of, makes this all work is this idea of the data-driven approach.\"\n",
      "}\n",
      "```\n",
      "\n",
      "This mapping matches the first sentence of each relevant section of the lecture script to the corresponding page of the lecture notes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "message_content = messages[0].content[0].text\n",
    "\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
