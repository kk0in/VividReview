{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"keyword\": [\"computer vision\", \"learning algorithms\", \"administrative issues\"],\n",
      "  \"formal\": \"Welcome to the second lecture of CS231N. On Tuesday, we presented an overview including the history and fundamental concepts of computer vision, along with an outline of the course. Today, we will delve into the specifics of this subject for the first time. \\n\\n- **Lecture Overview**: \\n  - Summary of Tuesday's lecture: \\n    - Provided a broad perspective on computer vision. \\n    - Discussed the history and an overview of the course. \\n  - Today's focus: \\n    - Deeper exploration into the details of certain topics. \\n    - An introduction to learning algorithms and their practical applications. \\n    - This lecture marks a shift from a broad overview to a detailed examination of specific algorithms. \\n    - The first learning algorithm will be introduced today, expected to be highly engaging. \\n\\n- **Administrative Issues**: \\n  - Addressing logistical and administrative concerns prior to diving into the main content.\"\n",
      "}\n",
      "Script for page 1 saved to ../keywords/10/1_spm.json\n",
      "{\n",
      "  \"keyword\": [\"Piazza\", \"communication\", \"attendance\", \"SCPD\"],\n",
      "  \"formal\": \"The primary point of discussion here is the usage of Piazza. As of the most recent assessment, approximately 500 students have registered on Piazza. This indicates that there remains a significant number of students who have yet to join. We strongly encourage the use of Piazza as the central platform for all communications between students and core staff.\\n\\n- Many inquiries concerning project ideas, midterm attendance, and poster session attendance have been directed to the staff list. Such questions should be posted on Piazza for a more prompt response, as all Teaching Assistants are regularly monitoring this platform.\\n- Email communications directed at the course list may be overlooked amidst the volume of messages received, thus Piazza remains the more reliable forum.\\n\\nAdditionally, it has come to our attention that some students enrolled in the Stanford Center for Professional Development (SCPD) are experiencing difficulties in registering for Piazza. SCPD students are meant to receive a @stanford.edu email address; subsequent to obtaining this email, it should be used to sign into Piazza. While this message may not be pertinent to those physically present in this room, it is important information for SCPD students who are participating remotely.\"\n",
      "}\n",
      "Script for page 2 saved to ../keywords/10/2_spm.json\n",
      "{\n",
      "    \"keyword\": [\"assignment\", \"k-nearest neighbor classifier\", \"SVM\", \"Softmax\", \"neural network\"],\n",
      "    \"formal\": \"The next administrative matter pertains to Assignment One. Assignment One will be made available later today, likely this afternoon. However, I assure you it will be posted before the end of the day. For those eager to begin immediately, last year's version of Assignment One is available for reference. The content remains largely unchanged, except for minor updates such as transitioning from Python 2.7 to Python 3 and other minor cosmetic adjustments. The essence of the assignment will stay consistent with the previous year. This task involves the following components: \\n- Implementing a k-nearest neighbor classifier, which will be discussed in today's lecture. \\n- Implementing several linear classifiers, including the SVM and Softmax. \\n- Creating a simple two-layer neural network. \\nThis material will be covered in the upcoming lectures.\"\n",
      "}\n",
      "Script for page 3 saved to ../keywords/10/3_spm.json\n",
      "{\n",
      "  \"keyword\": [\"Python\", \"NumPy\", \"vectorized operations\", \"numerical computing\", \"machine learning\"],\n",
      "  \"formal\": \"All of the assignments in this course utilize Python and NumPy. If you are not already familiar with Python or NumPy, a tutorial has been provided on the course website to aid in your understanding. Mastery of these tools is crucial as NumPy enables the implementation of highly efficient vectorized operations, allowing for substantial computations with minimal code. This efficiency is integral to numerical computing and machine learning, among other fields. You will gain significant practice with these operations in the first assignment. For those who do not possess considerable experience with Matlab, NumPy, or other types of vectorized tensor computation, it is advisable to begin this assignment early and meticulously review the tutorial.\"\n",
      "}\n",
      "Script for page 4 saved to ../keywords/10/4_spm.json\n",
      "{\n",
      "  \"keyword\": [\"Google Cloud\", \"virtual machines\", \"GPUs\", \"coupons\"],\n",
      "  \"formal\": \"The following information includes key administrative announcements and clarifications regarding the course infrastructure and support:\\n\\n- Official Support by Google Cloud:\\n  - We are pleased to announce that the course is now officially supported by Google Cloud.\\n  - Similar to Amazon AWS, Google Cloud allows you to initiate virtual machines in the cloud.\\n  - These virtual machines are equipped with GPU capabilities.\\n  - We are in the process of creating a tutorial that will guide you through the use of Google Cloud for course assignments.\\n  - Our aim is to ensure that you can effortlessly download an image and work seamlessly on your assignments using cloud instances.\\n\\n- Complimentary Google Cloud Credits:\\n  - Generous support from Google enables us to distribute coupons to each student, allowing free access to Google Cloud credits specifically for this class.\\n  - These credits can be utilized for both assignments and course projects, especially when requiring GPUs and larger machines.\\n\\n- Information Dissemination:\\n  - Further details regarding this support will be shared on Piazza later today.\\n\\n- Clarifications:\\n  - To address some queries, you are not required to use your laptop or connect to the 'corn' network.\\n  - You will have the capability to run your assignments and projects on Google Cloud, facilitated by the provided coupons.\"\n",
      "}\n",
      "Script for page 5 saved to ../keywords/10/5_spm.json\n",
      "{\n",
      "  \"keyword\": [\"image classification\", \"computer vision\", \"visual recognition\"],\n",
      "  \"formal\": \n",
      "  \"In the previous lecture, we discussed the task of image classification, a fundamental task within the field of computer vision. This concept will be a primary focus throughout the duration of the course. The central question we aim to address is how to effectively execute the image classification task. Concretely, image classification involves several key components: \\n- The system receives an input image, such as an image of a cat. \\n- The system has knowledge of a predetermined set of categories or labels, which may include labels such as dog, cat, truck, or plane. \\n- The objective of the system is to analyze the input image and assign it to one of these fixed categories. \\nWhile this problem may appear straightforward, especially since human visual systems are naturally adept at visual recognition tasks, it poses significant challenges for computational systems.\"\n",
      "}\n",
      "Script for page 6 saved to ../keywords/10/6_spm.json\n",
      "{\n",
      "  \"keyword\": [\"computer vision\", \"semantic gap\", \"pixel values\"],\n",
      "  \"formal\": \"If we consider what a computer perceives when it processes an image, it is evident that the computer does not comprehend the holistic concept of a cat as a human observer does. Instead, the computer represents the image as an extensive grid of numbers. For instance, an image with dimensions 800 by 600 pixels is represented by three numerical values per pixel, corresponding to the red, green, and blue values of that pixel. Therefore, to the computer, this image is merely a large array of numbers. Distilling the essence of a 'cat' from this vast array of numbers is a challenging task. This difficulty is referred to as the 'semantic gap.' The semantic label 'cat' assigned to the image highlights a significant discrepancy between the semantic concept of a cat and the pixel values perceived by the computer. This problem is further complicated by the fact that minor alterations to the image can significantly alter the entire pixel grid.\"\n",
      "}\n",
      "Script for page 7 saved to ../keywords/10/7_spm.json\n",
      "{\n",
      "  \"keyword\": [\"cat\", \"camera\", \"grid\", \"pixel\", \"algorithms\", \"viewpoint\", \"illumination\"],\n",
      "  \"formal\": \"Consider the scenario in which a cat remains motionless, without even the slightest twitch. While this is highly improbable in reality, let us assume we move the camera to the opposite side. Consequently, each pixel within this extensive grid of numbers would be completely different. Despite this, it still represents the same cat. Our algorithms need to demonstrate robustness to this variability. Beyond viewpoint changes, another crucial factor to consider is illumination.\"\n",
      "}\n",
      "Script for page 8 saved to ../keywords/10/8_spm.json\n",
      "{\n",
      "  \"keyword\": [\"lighting conditions\", \"robust algorithms\"],\n",
      "  \"formal\": \"- The scene can feature various lighting conditions:\\n  - A very dark, moody environment\\n  - A very bright, sunlit environment\\n- Irrespective of the lighting conditions, the subject, such as a cat, remains identifiable.\\n- Our algorithms must be designed to be robust and perform effectively under different lighting scenarios.\"\n",
      "}\n",
      "Script for page 9 saved to ../keywords/10/9_spm.json\n",
      "{\n",
      "  \"keyword\": [\"deform\", \"cats\", \"algorithms\", \"transforms\"],\n",
      "  \"formal\": \"Objects possess the capacity for deformation. Among animals, cats appear to be notably deformable, capable of adopting a diverse range of poses and positions. It is imperative that our algorithms exhibit robustness to accommodate these varied transformations.\"\n",
      "}\n",
      "Script for page 10 saved to ../keywords/10/10_spm.json\n",
      "{\n",
      "  \"keyword\": [\"occlusion\"],\n",
      "  \"formal\": \"Occlusion can present challenges, such as only viewing part of a cat—for instance, just the face or, in an extreme case, merely the tail peeking out from under a couch cushion. However, humans can effortlessly identify these as images of a cat. Similarly, our algorithms must also be robust against such cases, a task that is notably difficult.\"\n",
      "}\n",
      "Script for page 11 saved to ../keywords/10/11_spm.json\n",
      "{\n",
      "  \"keyword\": [\"background clutter\", \"foreground object\"],\n",
      "  \"formal\": \"The phenomenon of background clutter poses significant challenges. It arises when the foreground object, such as a cat, closely resembles elements in the background. This similarity in appearance necessitates strategic handling to accurately delineate the object from the background.\"\n",
      "}\n",
      "Script for page 12 saved to ../keywords/10/12_spm.json\n",
      "{\n",
      "  \"keyword\": [\"intraclass variation\", \"cat-ness\", \"visual appearances\", \"algorithm\", \"challenging problem\", \"human accuracy\", \"image classifier\", \"technology\"],\n",
      "  \"formal\": \"The issue of intraclass variation must be addressed, as the concept of 'cat-ness' encompasses a wide array of visual appearances. Cats can vary significantly in shapes, sizes, colors, and ages. Consequently, our algorithm must proficiently manage these diverse variations. This poses a formidable challenge, often underestimated due to the human brain's specialization in processing such complexities. Developing computer programs capable of addressing these issues concurrently, not only for cats but for any conceivable object category, is a remarkably challenging endeavor. It is somewhat miraculous that this technology functions at all, let alone with near-human accuracy in certain limited contexts, and in mere hundreds of milliseconds. Such technological advancements are indeed noteworthy, and throughout this course, we shall explore the developments that have made this possible.\"\n",
      "\n",
      "  \n",
      "\n",
      "  }\n",
      "\n",
      "Script for page 13 saved to ../keywords/10/13_spm.json\n",
      "{\n",
      "  \"keyword\": [\"algorithms\", \"object recognition\", \"cats\", \"images\", \"RSA encryption\"],\n",
      "  \"formal\": \"If one is enrolled in an algorithms course and is tasked with sorting numbers, computing a convex hull, or performing RSA encryption, it is feasible to delineate an algorithm and enumerate the steps necessary for these tasks to function. Conversely, in the context of object recognition, such as recognizing cats in images, there is no explicit algorithm that intuitively guides this process. This poses a significant challenge. Consider the scenario where a novice programmer is required to compose such a function on their first day; it is likely they would encounter substantial difficulties. Nonetheless, there have been deliberate attempts to establish sophisticated coded rules aimed at recognizing various animals.\"\n",
      "}\n",
      "Script for page 14 saved to ../keywords/10/14_spm.json\n",
      "{\n",
      "  \"keyword\": [\"visual recognition\", \"data-driven approach\", \"machine learning classifier\", \"datasets\"],\n",
      "  \"formal\": \"In the preceding lecture, we briefly explored the topic, but let us delve into one particular concept. It is a well-established fact that cats possess ears, eyes, mouths, and noses. According to the work of Hubel and Wiesel, edges play a critical role in visual recognition. Hence, an initial approach might involve computing the edges of an image and subsequently attempting to categorize various corners and boundaries. For instance, intersecting lines could indicate a corner, and an ear might be identified by locating specific corners. This process entails drafting a comprehensive set of rules for the recognition of cats. However, such an approach proves to be highly fragile and lacks scalability. Specifically, if one's objective shifts to the recognition of different object categories like trucks, dogs, or fish, the entire process must be recommenced. Therefore, this method is not efficient for scaling to the multitude of objects prevalent in the world. The pivotal insight that drives success in this domain is the implementation of a data-driven approach. Instead of crafting hand-specified rules to define objects such as cats or fish, one can leverage the internet to compile extensive datasets encompassing numerous instances of varied categories, including, but not limited to, cats, airplanes, and deer. Tools like Google Image Search facilitate the acquisition of a substantial number of examples within these categories. This undertaking demands considerable effort to amass these datasets; fortunately, numerous high-quality datasets are readily available. Upon acquiring the dataset, a machine learning classifier is trained to process the data, distill it into comprehensible knowledge, and produce a model capable of recognizing different object categories. The trained model is subsequently applied to novel images, enabling the recognition of cats, dogs, and other objects.\"\n",
      "}\n",
      "Script for page 15 saved to ../keywords/10/15_spm.json\n",
      "{\n",
      "    \"keyword\": [\"API\", \"neural networks\", \"convolutional neural networks\", \"deep learning\", \"data-driven approach\", \"classifier\", \"nearest neighbor\"],\n",
      "    \"formal\": \"The API has undergone modifications. Instead of a single function that inputs an image and identifies a cat, we now have two distinct functions. The first function, 'train,' inputs images and labels to generate a model. The second function, 'predict,' takes the model as input and subsequently makes predictions on images. This bifurcation represents a critical insight that has significantly enhanced performance over the past decade or two.\\n\\nThis lecture primarily focuses on neural networks, specifically convolutional neural networks and deep learning. However, it is crucial to recognize that the concept of a data-driven approach extends well beyond deep learning alone. To illustrate this process effectively, we will begin with a simple classifier before progressing to more complex ones.\\n\\nKey points include:\\n\\n- Modification of API: Introduction of two functions, 'train' and 'predict'.\\n- Importance of this bifurcated approach in recent advancements.\\n- Focus of the lecture: Neural networks, convolutional neural networks, and deep learning.\\n- Broader applicability of the data-driven approach.\\n- Introduction to a simple classifier: Nearest Neighbor.\\n\"\n",
      "}\n",
      "Script for page 16 saved to ../keywords/10/16_spm.json\n",
      "{\n",
      "  \"keyword\": [\"algorithm\", \"training data\", \"prediction step\", \"image\"],\n",
      "  \"formal\": \"The algorithm employed exhibits a lack of complexity. \\n\\n- During the training phase, no operations are performed beyond memorizing all the training data. \\n- This approach is exceedingly straightforward. \\n- In the prediction phase, when presented with new image data, the algorithm searches for the most similar image within the training dataset and assigns the label from that most similar image to the new image.\\n\\nThis algorithm, despite its simplicity, possesses several advantageous properties, particularly in terms of data-drivenness.\"\n",
      "}\n",
      "Script for page 17 saved to ../keywords/10/17_spm.json\n",
      "{\n",
      "  \"keyword\": [\"CIFAR-10\", \"dataset\", \"machine learning\"],\n",
      "  \"formal\": \"The CIFAR-10 dataset is a commonly utilized dataset in the field of machine learning, often employed as a small test case in various applications. This dataset, which will also be used in your homework, comprises ten distinct classes, including airplanes, automobiles, birds, cats, among others. For each of these ten categories, the dataset provides approximately 50,000 training images that are roughly evenly distributed across the categories. Additionally, it includes 10,000 testing images intended for algorithm evaluation.\"\n",
      "}\n",
      "Script for page 18 saved to ../keywords/10/18_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor classifier\", \"CIFAR-10\", \"training images\", \"test images\"],\n",
      "  \"formal\": \"This lecture script describes the application of the nearest neighbor classifier to test images within the CIFAR-10 dataset. The process involves the following steps:\\n\\n- A grid is presented with test images in the leftmost column, sourced from the CIFAR-10 dataset.\\n- To the right of each test image, the training images are sorted and displayed, showing the most similar ones to each test example.\\n- It is observed that although these images appear visually similar to the training images, the classification is not always accurate.\\n- For instance, in the second row, a test image of a dog is correctly matched with a dog from the training set, while the subsequent test image, which might be a deer or horse, is incorrectly identified due to similar visual features like a central white blob.\\n\\nThe nearest neighbor algorithm operates by identifying the closest example in the training set, based on a certain comparison function. The label of the closest example, known from the training set, is then assigned to the test image. This method, while illustrative, may not always yield high accuracy.\\n\\nAdditionally, it is crucial to determine the methodology for comparing pairs of images since multiple options exist for defining this comparison function.\"\n",
      "}\n",
      "Script for page 19 saved to ../keywords/10/19_spm.json\n",
      "{\n",
      "    \"keyword\": [\"L1 distance\", \"Manhattan distance\", \"comparing images\", \"pixels\", \"test image\", \"training image\"],\n",
      "    \"formal\": \"In the previous slide, the example illustrated the utilization of the L1 distance, also referred to as the Manhattan distance. This is a straightforward and intuitive method for comparing images, wherein individual pixels within the images are compared. The process can be outlined as follows: \\n\\n- Assume our test image is a small four-by-four grid of pixel values.\\n- For each pixel in the test image, take the corresponding pixel in the training image.\\n- Subtract the value of the training image's pixel from the test image's pixel.\\n- Calculate the absolute value of this difference to obtain the pixel-wise difference between the two images.\\n- Sum these absolute differences across all the pixels in the image.\\n\\nAlthough this method may seem simplistic and may not always provide the best comparison, it offers a concrete metric to measure the difference between two images. In this specific instance, the resulting difference between the two images is computed as 456.\"\n",
      "}\n",
      "Script for page 20 saved to ../keywords/10/20_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor classifier\", \"Python code\", \"NumPy\", \"training function\", \"L1 distance function\"],\n",
      "  \"formal\": \"The following is an implementation of the nearest neighbor classifier using Python, which is succinct and efficient owing to the use of vectorized operations provided by NumPy. Key points are as follows:\\n\\n- **Training Function**: The function is straightforward, as it solely involves memorizing the training data since the nearest neighbor classifier does not require any substantial computation during the training phase.\\n- **Testing Phase**: During the test phase, an input image is compared to each training example using the L1 distance function to identify the most similar instance within the training set.\"\n",
      "}\n",
      "Script for page 21 saved to ../keywords/10/21_spm.json\n",
      "{\n",
      "  \"keyword\": [\"vectorized operations\", \"NumPy\", \"Python code\"],\n",
      "  \"formal\": \"The script illustrates the efficiency of performing tasks using vectorized operations in NumPy, which can be accomplished with minimal Python code. This concept will be practiced in the forthcoming assignment. Key points to note include:\\n- Utilization of vectorized operations in NumPy\\n- Efficiency in performing tasks with minimal lines of Python code\\n- Hands-on practice in the first assignment\"\n",
      "}\n",
      "Script for page 22 saved to ../keywords/10/22_spm.json\n",
      "{\n",
      "  \"keyword\": [\"simple classifier\", \"training set\", \"training\", \"testing\"],\n",
      "  \"formal\": \"A few key inquiries regarding this simplified classifier arise: \\n- Given N examples within our training set, what speed can we anticipate for the training process? \\n- Additionally, what efficiency can we expect during the testing phase?\"\n",
      "}\n",
      "Script for page 23 saved to ../keywords/10/23_spm.json\n",
      "{\n",
      "  \"keyword\": [\"training\", \"test time\", \"dataset\"],\n",
      "  \"formal\": \"Training is likely constant because there is no active processing required; it is solely a matter of memorizing the data. When copying a pointer, the time complexity remains constant regardless of the dataset's size. However, during testing, an image must be compared to each of the N training examples in the dataset, which is notably time-consuming. This process seems counterintuitive when considered structurally.\"\n",
      "}\n",
      "Script for page 24 saved to ../keywords/10/24_spm.json\n",
      "{\n",
      "  \"keyword\": [\"training time\", \"testing time\", \"classifier\", \"data center\", \"mobile phone\", \"low power device\"],\n",
      "  \"formal\": \"In practical applications, it is desirable for classifiers to exhibit slow performance during the training phase and swift performance during the testing phase. This requirement arises from the fact that classifiers are typically trained in data centers, where extensive computational resources can be afforded to optimize their performance. Once the classifier has been effectively trained, it is then deployed for testing, which often occurs on devices with limited computational power, such as mobile phones or web browsers. Consequently, it is imperative for the classifier to demonstrate rapid performance during the testing phase.\"\n",
      "}\n",
      "Script for page 25 saved to ../keywords/10/25_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor algorithm\", \"convolutional neural networks\", \"decision regions\", \"classifier\"],\n",
      "  \"formal\": \"From a certain perspective, the nearest neighbor algorithm may appear somewhat counterintuitive. As we will observe when we transition to convolutional neural networks and other parametric models, these models will exhibit an inverse pattern. Specifically, they require significant computational resources during the training phase but perform efficiently during the testing phase. Therefore, the pertinent question arises: What does the application of the nearest neighbor algorithm entail in practice? Herein, we present a graphical illustration depicting the decision regions of a nearest neighbor classifier.\"\n",
      "}\n",
      "Script for page 26 saved to ../keywords/10/26_spm.json\n",
      "{\n",
      "  \"keyword\": [\"training set\", \"two dimensional plane\", \"class label\", \"nearest neighbor classifier\", \"k-nearest neighbors\"],\n",
      "  \"formal\": \"The training set comprises of points located within a two-dimensional plane. The color of each point designates the corresponding category, or class label, of that point. There are five distinct classes observed, with some blue points located in the corner and some purple points in the upper-right corner. For each pixel across this entire plane, the nearest example within the training data has been computed, and the point on the background is colored according to the class label of the nearest example. This illustrates how the nearest neighbor classifier partitions and colors the space in correspondence with the nearby points. However, upon inspection, potential shortcomings of the nearest neighbor classifier become evident. For instance, the central region predominantly contains green points, yet a small yellow point in the midst creates a yellow island within the green cluster. This suggests that the majority of those points should be green. Similarly, we observe green regions extending into the blue regions, likely due to the influence of one possibly noisy or spurious point. These observations motivate a generalized approach known as k-nearest neighbors.\"\n",
      "}\n",
      "Script for page 27 saved to ../keywords/10/27_spm.json\n",
      "{\n",
      "  \"keyword\": [\"K-nearest neighbors\", \"distance metric\", \"majority vote\", \"decision boundaries\", \"computer vision\", \"high dimensional vectors\"],\n",
      "  \"formal\": \"In the context of K-nearest neighbors classifiers, instead of identifying only a single nearest neighbor, we incorporate a more sophisticated approach by considering K nearest neighbors based on a specified distance metric. Following this, a vote among these neighbors is conducted, and the prediction is determined by the majority vote. It is possible to introduce more intricate methods, such as weighting the vote according to distance; however, the straightforward majority vote approach generally proves adequate.\\n\\nKey Observations: \\n- The application of the K=1 nearest neighbor classifier compared to K=3 and K=5 reveals distinct classification outcomes. \\n- In the transition to K=3, the anomalous yellow point within the green cluster, previously misclassifying nearby points as yellow, is now correctly classified as green. Also, the boundaries of the red and blue regions exhibit smoothing due to majority voting. \\n- In the case of K=5, the decision boundaries between the blue and red regions become remarkably smooth and well-defined.\\n\\nGeneral Guidelines: \\n- Utilizing a K value greater than one in nearest neighbors classifiers is almost always advisable, as this practice typically smooths decision boundaries and enhances classification performance.\\n\\nStudent Query: \\n- Regarding the appearance of white regions, these are areas where no majority exists among the k-nearest neighbors. Although more complex solutions could be implemented, such as making an educated guess or randomly selecting among the majority winners, in this example, white indicates an absence of a majority vote.\\n\\nOn Computer Vision: \\n- It is beneficial to alternate perspectives between viewing images as high dimensional points and examining actual images. This dual perspective aids in understanding, as the pixels in images can be perceived as high dimensional vectors.\"\n",
      "}\n",
      "Script for page 28 saved to ../keywords/10/28_spm.json\n",
      "{\n",
      "  \"keyword\": [\"nearest neighbor\", \"classified correctly\", \"classified incorrectly\", \"larger value of K\"],\n",
      "  \"formal\": \"- Here, images have been color-coded in red or green to denote whether they are classified correctly or incorrectly based on their nearest neighbor.\\n- It is evident that the current classification approach is not very effective.\\n- One potential improvement would be to use a larger value of K, which would involve voting among possibly the top three, top five, or even the entire row of nearest neighbors.\"\n",
      "}\n",
      "Script for page 29 saved to ../keywords/10/29_spm.json\n",
      "{\n",
      "  \"keyword\": [\"robust\", \"noise\", \"retrieving neighbors\"],\n",
      "  \"formal\": \"The method described is expected to be considerably more robust to the noise encountered during the retrieval of neighbors in this manner.\"\n",
      "}\n",
      "Script for page 30 saved to ../keywords/10/30_spm.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "PDF = '../pdfs'\n",
    "TOC = '../tocs'\n",
    "IMAGE = '../images'\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "KEYWORD = '../keywords'\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        return json.load(script_file)\n",
    "\n",
    "# Path to your image and script\n",
    "project_id = 10\n",
    "script_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "keyword_path = os.path.join(KEYWORD, f\"{str(project_id)}\")\n",
    "os.makedirs(keyword_path, exist_ok=True)\n",
    "\n",
    "# Read the script\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Function to make API request for each page\n",
    "def make_api_request(page_number, script_segment):    \n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                \"Given the lecture script, identify at least one important keywords. \"\n",
    "                \"Next, transform the script into a more formal tone, breaking it down into a bullet point structure where appropriate. \"\n",
    "                \"The output should be in JSON format with the following structure: \"\n",
    "                \"{\\\"keyword\\\": [\\\"string\\\", \\\"string\\\", ...], \\\"formal\\\": \\\"string\\\"} \"\n",
    "                f\"lecture script: {script_segment} \"\n",
    "            )\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def filter_script_data(script_data):\n",
    "    allowed_keys = {\"keyword\", \"formal\"}\n",
    "    return {key: value for key, value in script_data.items() if key in allowed_keys}\n",
    "\n",
    "# Encode images and make API requests\n",
    "for page_num in script_content.keys():\n",
    "    page_number = str(page_num)\n",
    "    script_segment = script_content[page_number]\n",
    "    response_data = make_api_request(page_number, script_segment)\n",
    "    \n",
    "    # Process the response data\n",
    "    if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "        script_text = response_data['choices'][0]['message']['content']\n",
    "        print(script_text)\n",
    "        # Convert the script text to JSON format\n",
    "        try:\n",
    "            script_data = json.loads(script_text)\n",
    "            script_data = filter_script_data(script_data)\n",
    "            script_data[\"original\"] = script_segment\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for page {page_number}: {e}\")\n",
    "            script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "    else:\n",
    "        print(f\"Error: 'choices' key not found in the response for page {page_number}\")\n",
    "        print(response_data)\n",
    "        script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "    # Save the script data as a JSON file\n",
    "    script_json_path = os.path.join(keyword_path, f\"{page_number}_spm.json\")\n",
    "    with open(script_json_path, \"w\") as json_file:\n",
    "        json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Script for page {page_number} saved to {script_json_path}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
