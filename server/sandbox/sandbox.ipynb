{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import speech\n",
    "\n",
    "RECORDING = './recordings'\n",
    "SCRIPT = './scripts'\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./watchful-lotus-383310-7782daea2dc1.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stt(gcs_uri, text_output_path=None):\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    # with open(audio_path, \"rb\") as audio_file:\n",
    "    #     content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "        enable_word_time_offsets=True\n",
    "    )\n",
    "\n",
    "    res = client.long_running_recognize(config=config, audio=audio)\n",
    "    response = res.result()\n",
    "\n",
    "    results_with_timestamps = []\n",
    "\n",
    "    for result in response.results:\n",
    "        alternative = result.alternatives[0]\n",
    "        for word_info in alternative.words:\n",
    "            word = word_info.word\n",
    "            start_time = word_info.start_time.total_seconds()\n",
    "            end_time = word_info.end_time.total_seconds()\n",
    "            results_with_timestamps.append({\n",
    "                \"word\": word,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time\n",
    "            })\n",
    "\n",
    "    return results_with_timestamps\n",
    "    \n",
    "    \n",
    "    # stt_result = \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "    \n",
    "    # with open(text_output_path, \"w\") as f:\n",
    "    #     f.write(stt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stt(gcs_uri, text_output_path=None):\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    # with open(audio_path, \"rb\") as audio_file:\n",
    "    #     content = audio_file.read()\n",
    "\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.FLAC,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "        enable_word_time_offsets=True\n",
    "    )\n",
    "\n",
    "    res = client.long_running_recognize(config=config, audio=audio)\n",
    "    response = res.result()\n",
    "\n",
    "    transcriptions = []\n",
    "    for result in response.results:\n",
    "        alternative = result.alternatives[0]\n",
    "        start_time = None\n",
    "        end_time = None\n",
    "        sentence = []\n",
    "\n",
    "        for word_info in alternative.words:\n",
    "            if start_time is None:\n",
    "                start_time = word_info.start_time\n",
    "            end_time = word_info.end_time\n",
    "            sentence.append(word_info.word)\n",
    "            \n",
    "            # Detect sentence boundaries using common sentence-ending punctuation\n",
    "            if word_info.word.endswith(('.', '!', '?')):\n",
    "                transcriptions.append({\n",
    "                    \"sentence\": ' '.join(sentence),\n",
    "                    \"start_time\": start_time.total_seconds(),\n",
    "                    \"end_time\": end_time.total_seconds()\n",
    "                })\n",
    "                sentence = []\n",
    "                start_time = None\n",
    "                end_time = None\n",
    "\n",
    "        # Capture any trailing sentence fragments\n",
    "        if sentence:\n",
    "            transcriptions.append({\n",
    "                \"sentence\": ' '.join(sentence),\n",
    "                \"start_time\": start_time.total_seconds(),\n",
    "                \"end_time\": end_time.total_seconds()\n",
    "            })\n",
    "    \n",
    "    return transcriptions\n",
    "    \n",
    "    \n",
    "    # stt_result = \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "    \n",
    "    # with open(text_output_path, \"w\") as f:\n",
    "    #     f.write(stt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722971621.442267 29582425 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay (0.0s - 0.2s)\n",
      "so (0.2s - 0.4s)\n",
      "welcome (0.4s - 0.8s)\n",
      "to (0.8s - 0.9s)\n",
      "lecture (0.9s - 1.1s)\n",
      "to (1.1s - 1.5s)\n",
      "of (1.5s - 1.8s)\n",
      "cs231n (1.8s - 2.6s)\n",
      "on (2.6s - 3.6s)\n",
      "Tuesday (3.6s - 4.1s)\n",
      "we (4.1s - 4.3s)\n",
      "just (4.3s - 4.8s)\n",
      "recall (4.8s - 5.0s)\n",
      "we (5.0s - 5.3s)\n",
      "sort (5.3s - 5.6s)\n",
      "of (5.6s - 5.6s)\n",
      "gave (5.6s - 5.8s)\n",
      "you (5.8s - 5.9s)\n",
      "the (5.9s - 6.0s)\n",
      "big (6.0s - 6.2s)\n",
      "picture (6.2s - 6.4s)\n",
      "of (6.4s - 6.6s)\n",
      "you (6.6s - 6.7s)\n",
      "of (6.7s - 6.9s)\n",
      "what (6.9s - 7.3s)\n",
      "is (7.3s - 7.4s)\n",
      "computer (7.4s - 7.8s)\n",
      "vision (7.8s - 7.9s)\n",
      "what (7.9s - 8.3s)\n",
      "is (8.3s - 8.4s)\n",
      "the (8.4s - 8.5s)\n",
      "history (8.5s - 8.8s)\n",
      "and (8.8s - 9.4s)\n",
      "a (9.4s - 9.5s)\n",
      "little (9.5s - 9.6s)\n",
      "bit (9.6s - 9.8s)\n",
      "of (9.8s - 9.8s)\n",
      "an (9.8s - 10.0s)\n",
      "overview (10.0s - 10.2s)\n",
      "of (10.2s - 10.3s)\n",
      "the (10.3s - 10.4s)\n",
      "class (10.4s - 10.7s)\n",
      "and (10.7s - 11.5s)\n",
      "today (11.5s - 11.8s)\n",
      "we're (11.8s - 11.9s)\n",
      "going (11.9s - 12.1s)\n",
      "to (12.1s - 12.3s)\n",
      "dive (12.3s - 12.6s)\n",
      "in (12.6s - 12.8s)\n",
      "for (12.8s - 12.9s)\n",
      "the (12.9s - 13.0s)\n",
      "first (13.0s - 13.3s)\n",
      "time (13.3s - 13.5s)\n",
      "into (13.5s - 13.6s)\n",
      "the (13.6s - 13.7s)\n",
      "details (13.7s - 14.0s)\n",
      "and (14.0s - 14.7s)\n",
      "we'll (14.7s - 14.9s)\n",
      "start (14.9s - 15.1s)\n",
      "to (15.1s - 15.2s)\n",
      "see (15.2s - 15.3s)\n",
      "in (15.3s - 15.6s)\n",
      "much (15.6s - 15.8s)\n",
      "more (15.8s - 16.0s)\n",
      "tap (16.0s - 16.3s)\n",
      "on (16.3s - 16.6s)\n",
      "exactly (16.6s - 17.0s)\n",
      "how (17.0s - 17.1s)\n",
      "some (17.1s - 17.9s)\n",
      "of (17.9s - 17.9s)\n",
      "these (17.9s - 18.0s)\n",
      "learning (18.0s - 18.5s)\n",
      "algorithms (18.5s - 18.9s)\n",
      "actually (18.9s - 19.1s)\n",
      "work (19.1s - 19.4s)\n",
      "in (19.4s - 19.4s)\n",
      "practice (19.4s - 19.9s)\n",
      "so (19.9s - 21.1s)\n",
      "the (21.1s - 21.2s)\n",
      "first (21.2s - 21.5s)\n",
      "section (21.5s - 21.7s)\n",
      "of (21.7s - 21.8s)\n",
      "the (21.8s - 21.8s)\n",
      "class (21.8s - 22.1s)\n",
      "is (22.1s - 22.3s)\n",
      "probably (22.3s - 22.4s)\n",
      "the (22.4s - 22.8s)\n",
      "largest (22.8s - 23.6s)\n",
      "big (23.6s - 24.1s)\n",
      "picture (24.1s - 24.4s)\n",
      "vision (24.4s - 24.8s)\n",
      "and (24.8s - 25.2s)\n",
      "the (25.2s - 25.3s)\n",
      "majority (25.3s - 25.5s)\n",
      "of (25.5s - 25.7s)\n",
      "the (25.7s - 25.8s)\n",
      "lecturers (25.8s - 26.2s)\n",
      "in (26.2s - 26.3s)\n",
      "this (26.3s - 26.6s)\n",
      "class (26.6s - 26.9s)\n",
      "will (26.9s - 27.0s)\n",
      "be (27.0s - 27.2s)\n",
      "much (27.2s - 27.3s)\n",
      "more (27.3s - 27.4s)\n",
      "detail (27.4s - 27.9s)\n",
      "oriented (27.9s - 28.1s)\n",
      "much (28.1s - 28.8s)\n",
      "more (28.8s - 29.0s)\n",
      "focused (29.0s - 29.1s)\n",
      "on (29.1s - 29.4s)\n",
      "the (29.4s - 29.6s)\n",
      "specific (29.6s - 30.0s)\n",
      "mechanics (30.0s - 30.4s)\n",
      "of (30.4s - 30.5s)\n",
      "these (30.5s - 30.7s)\n",
      "different (30.7s - 30.8s)\n",
      "algorithms (30.8s - 31.1s)\n",
      "so (31.1s - 32.9s)\n",
      "today (32.9s - 33.2s)\n",
      "will (33.2s - 33.3s)\n",
      "see (33.3s - 33.4s)\n",
      "our (33.4s - 33.5s)\n",
      "first (33.5s - 33.8s)\n",
      "learning (33.8s - 34.0s)\n",
      "algorithm (34.0s - 34.4s)\n",
      "and (34.4s - 34.6s)\n",
      "that'll (34.6s - 34.8s)\n",
      "be (34.8s - 34.8s)\n",
      "really (34.8s - 35.1s)\n",
      "exciting (35.1s - 35.5s)\n",
      "I (35.5s - 35.6s)\n",
      "think (35.6s - 35.9s)\n",
      "but (35.9s - 36.6s)\n",
      "before (36.6s - 36.9s)\n",
      "we (36.9s - 37.0s)\n",
      "get (37.0s - 37.1s)\n",
      "to (37.1s - 37.2s)\n",
      "that (37.2s - 37.3s)\n",
      "I (37.3s - 38.0s)\n",
      "wanted (38.0s - 38.2s)\n",
      "to (38.2s - 38.3s)\n",
      "talk (38.3s - 38.4s)\n",
      "about (38.4s - 38.6s)\n",
      "a (38.6s - 38.9s)\n",
      "couple (38.9s - 38.9s)\n",
      "administrative (38.9s - 39.7s)\n",
      "issues (39.7s - 40.0s)\n",
      "one (40.0s - 40.9s)\n",
      "is (40.9s - 41.2s)\n",
      "a (41.2s - 41.5s)\n",
      "Piazza (41.5s - 42.2s)\n",
      "so (42.2s - 42.9s)\n",
      "I (42.9s - 43.2s)\n",
      "sought (43.2s - 43.8s)\n",
      "my (43.8s - 43.8s)\n",
      "check (43.8s - 44.2s)\n",
      "yesterday (44.2s - 44.4s)\n",
      "it (44.4s - 44.8s)\n",
      "seemed (44.8s - 45.0s)\n",
      "like (45.0s - 45.2s)\n",
      "we (45.2s - 45.3s)\n",
      "had (45.3s - 45.4s)\n",
      "maybe (45.4s - 45.7s)\n",
      "five (45.7s - 46.0s)\n",
      "hundred (46.0s - 46.2s)\n",
      "students (46.2s - 46.7s)\n",
      "signed (46.7s - 47.6s)\n",
      "up (47.6s - 47.7s)\n",
      "on (47.7s - 47.7s)\n",
      "which (47.7s - 48.6s)\n",
      "means (48.6s - 48.7s)\n",
      "that (48.7s - 48.9s)\n",
      "there (48.9s - 49.0s)\n",
      "are (49.0s - 49.1s)\n",
      "several (49.1s - 49.4s)\n",
      "hundred (49.4s - 49.7s)\n",
      "of (49.7s - 49.8s)\n",
      "you (49.8s - 49.9s)\n",
      "who (49.9s - 50.0s)\n",
      "are (50.0s - 50.1s)\n",
      "not (50.1s - 50.3s)\n",
      "yet (50.3s - 50.3s)\n",
      "they're (50.3s - 50.7s)\n",
      "so (50.7s - 51.8s)\n",
      "we (51.8s - 52.2s)\n",
      "really (52.2s - 52.4s)\n",
      "want (52.4s - 52.8s)\n",
      "Piazza (52.8s - 53.1s)\n",
      "to (53.1s - 53.3s)\n",
      "be (53.3s - 53.4s)\n",
      "the (53.4s - 53.5s)\n",
      "main (53.5s - 53.6s)\n",
      "source (53.6s - 54.6s)\n",
      "of (54.6s - 54.7s)\n",
      "communication (54.7s - 54.8s)\n",
      "between (54.8s - 55.5s)\n",
      "the (55.5s - 55.8s)\n",
      "students (55.8s - 56.1s)\n",
      "and (56.1s - 56.2s)\n",
      "the (56.2s - 56.4s)\n",
      "core (56.4s - 56.6s)\n",
      "staff (56.6s - 57.0s)\n",
      "so (57.0s - 57.6s)\n",
      "for (57.6s - 58.0s)\n",
      "we (58.0s - 58.4s)\n",
      "got (58.4s - 58.5s)\n",
      "a (58.5s - 58.6s)\n",
      "lot (58.6s - 58.7s)\n",
      "of (58.7s - 58.8s)\n",
      "questions (58.8s - 59.2s)\n",
      "to (59.2s - 59.3s)\n",
      "the (59.3s - 59.4s)\n",
      "to (59.4s - 59.7s)\n",
      "the (59.7s - 59.8s)\n",
      "West (59.9s - 60.5s)\n",
      "about (60.5s - 60.7s)\n",
      "a (60.7s - 61.0s)\n",
      "project (61.0s - 61.9s)\n",
      "ideas (61.9s - 62.5s)\n",
      "or (62.5s - 62.8s)\n",
      "questions (62.8s - 63.3s)\n",
      "about (63.3s - 63.5s)\n",
      "midterm (63.5s - 63.8s)\n",
      "attendance (63.8s - 64.3s)\n",
      "or (64.3s - 64.5s)\n",
      "poster (64.5s - 64.8s)\n",
      "session (64.8s - 65.2s)\n",
      "attendance (65.2s - 65.7s)\n",
      "and (65.7s - 66.2s)\n",
      "any (66.2s - 66.4s)\n",
      "sort (66.4s - 66.6s)\n",
      "of (66.6s - 66.7s)\n",
      "questions (66.7s - 67.1s)\n",
      "like (67.1s - 67.2s)\n",
      "that (67.2s - 67.3s)\n",
      "should (67.3s - 67.7s)\n",
      "really (67.7s - 67.8s)\n",
      "go (67.8s - 68.0s)\n",
      "to (68.0s - 68.2s)\n",
      "Piazza (68.2s - 68.5s)\n",
      "you'll (68.5s - 69.4s)\n",
      "probably (69.4s - 69.6s)\n",
      "get (69.6s - 69.8s)\n",
      "answers (69.8s - 70.3s)\n",
      "to (70.3s - 70.4s)\n",
      "your (70.4s - 70.5s)\n",
      "questions (70.5s - 70.9s)\n",
      "faster (70.9s - 71.3s)\n",
      "on (71.3s - 71.5s)\n",
      "Piazza (71.5s - 71.8s)\n",
      "because (71.8s - 72.2s)\n",
      "all (72.2s - 72.3s)\n",
      "the (72.3s - 72.4s)\n",
      "chairs (72.4s - 72.7s)\n",
      "are (72.7s - 72.8s)\n",
      "are (72.8s - 72.9s)\n",
      "going (72.9s - 73.4s)\n",
      "to (73.4s - 73.5s)\n",
      "check (73.5s - 73.8s)\n",
      "that (73.8s - 73.8s)\n",
      "and (73.8s - 74.8s)\n",
      "it's (74.8s - 74.9s)\n",
      "sort (74.9s - 75.1s)\n",
      "of (75.1s - 75.2s)\n",
      "easy (75.2s - 75.4s)\n",
      "for (75.4s - 75.6s)\n",
      "emails (75.6s - 76.0s)\n",
      "to (76.0s - 76.0s)\n",
      "get (76.0s - 76.2s)\n",
      "lost (76.2s - 76.4s)\n",
      "in (76.4s - 76.5s)\n",
      "the (76.5s - 76.6s)\n",
      "shuffle (76.6s - 76.6s)\n",
      "with (76.6s - 77.0s)\n",
      "you (77.0s - 77.1s)\n",
      "just (77.1s - 77.3s)\n",
      "sent (77.3s - 77.5s)\n",
      "to (77.5s - 77.5s)\n",
      "the (77.5s - 77.6s)\n",
      "course (77.6s - 77.9s)\n",
      "list (77.9s - 78.2s)\n",
      "it's (78.2s - 79.6s)\n",
      "also (79.6s - 79.8s)\n",
      "come (79.8s - 80.0s)\n",
      "to (80.0s - 80.1s)\n",
      "my (80.1s - 80.1s)\n",
      "attention (80.1s - 80.2s)\n",
      "that (80.2s - 80.7s)\n",
      "some (80.7s - 81.0s)\n",
      "scpd (81.0s - 81.6s)\n",
      "students (81.6s - 82.1s)\n",
      "are (82.1s - 82.2s)\n",
      "having (82.2s - 82.4s)\n",
      "a (82.4s - 82.6s)\n",
      "bit (82.6s - 82.8s)\n",
      "of (82.8s - 82.9s)\n",
      "a (82.9s - 83.0s)\n",
      "hard (83.0s - 83.1s)\n",
      "time (83.1s - 83.3s)\n",
      "signing (83.3s - 83.6s)\n",
      "up (83.6s - 83.9s)\n",
      "for (83.9s - 84.0s)\n",
      "Piazza (84.0s - 84.3s)\n",
      "it (84.3s - 85.8s)\n",
      "if (85.8s - 85.9s)\n",
      "you (85.9s - 86.1s)\n",
      "SEC (86.1s - 87.2s)\n",
      "students (87.2s - 87.7s)\n",
      "are (87.7s - 87.8s)\n",
      "supposed (87.8s - 88.1s)\n",
      "to (88.1s - 88.2s)\n",
      "receive (88.2s - 88.6s)\n",
      "a. (88.6s - 88.9s)\n",
      "Stanford (88.9s - 89.4s)\n",
      "at (89.4s - 89.8s)\n",
      "stanford.edu (89.8s - 90.6s)\n",
      "email (90.6s - 91.2s)\n",
      "address (91.2s - 91.5s)\n",
      "so (91.5s - 92.1s)\n",
      "once (92.1s - 92.5s)\n",
      "you (92.5s - 92.6s)\n",
      "get (92.6s - 92.7s)\n",
      "that (92.7s - 93.0s)\n",
      "email (93.0s - 93.4s)\n",
      "address (93.4s - 93.5s)\n",
      "then (93.5s - 94.0s)\n",
      "you (94.0s - 94.1s)\n",
      "can (94.1s - 94.3s)\n",
      "use (94.3s - 94.5s)\n",
      "the (94.5s - 94.5s)\n",
      "Stanford (94.5s - 95.0s)\n",
      "email (95.0s - 95.3s)\n",
      "to (95.3s - 95.4s)\n",
      "sign (95.4s - 95.6s)\n",
      "into (95.6s - 95.8s)\n",
      "the (95.8s - 95.9s)\n",
      "probably (95.9s - 97.5s)\n",
      "that (97.5s - 97.6s)\n",
      "doesn't (97.6s - 98.0s)\n",
      "the (98.0s - 98.0s)\n",
      "fact (98.0s - 98.1s)\n",
      "that (98.1s - 98.3s)\n",
      "both (98.3s - 98.6s)\n",
      "of (98.6s - 98.7s)\n",
      "you (98.7s - 98.9s)\n",
      "were (98.9s - 98.9s)\n",
      "sitting (98.9s - 99.2s)\n",
      "in (99.2s - 99.3s)\n",
      "the (99.3s - 99.4s)\n",
      "room (99.4s - 99.6s)\n",
      "right (99.6s - 99.7s)\n",
      "now (99.7s - 99.8s)\n",
      "but (99.8s - 100.3s)\n",
      "for (100.3s - 100.4s)\n",
      "those (100.4s - 100.5s)\n",
      "students (100.5s - 100.8s)\n",
      "listening (100.8s - 101.3s)\n",
      "on (101.3s - 101.8s)\n",
      "ncpd (101.8s - 102.3s)\n",
      "the (104.8s - 105.4s)\n",
      "next (105.4s - 105.6s)\n",
      "the (105.6s - 105.8s)\n",
      "next (105.8s - 105.9s)\n",
      "investment (105.9s - 106.4s)\n",
      "issue (106.4s - 106.9s)\n",
      "is (106.9s - 107.0s)\n",
      "about (107.0s - 107.1s)\n",
      "assignment (107.1s - 107.7s)\n",
      "one (107.7s - 108.0s)\n",
      "assignment (108.0s - 109.0s)\n",
      "one (109.0s - 109.4s)\n",
      "will (109.4s - 109.8s)\n",
      "be (109.8s - 109.9s)\n",
      "up (109.9s - 110.1s)\n",
      "later (110.1s - 110.5s)\n",
      "today (110.5s - 110.7s)\n",
      "I'll (110.7s - 111.2s)\n",
      "probably (111.2s - 111.4s)\n",
      "sometime (111.4s - 111.9s)\n",
      "this (111.9s - 112.0s)\n",
      "afternoon (112.0s - 112.2s)\n",
      "but (112.2s - 112.8s)\n",
      "I (112.8s - 113.1s)\n",
      "promise (113.1s - 113.6s)\n",
      "before (113.6s - 113.7s)\n",
      "I (113.7s - 113.9s)\n",
      "go (113.9s - 114.0s)\n",
      "to (114.0s - 114.1s)\n",
      "sleep (114.1s - 114.1s)\n",
      "tonight (114.1s - 114.4s)\n",
      "it'll (114.4s - 114.9s)\n",
      "be (114.9s - 115.0s)\n",
      "up (115.0s - 115.1s)\n",
      "but (115.1s - 116.5s)\n",
      "if (116.5s - 116.8s)\n",
      "you're (116.8s - 117.0s)\n",
      "getting (117.0s - 117.1s)\n",
      "a (117.1s - 117.2s)\n",
      "little (117.2s - 117.3s)\n",
      "bit (117.3s - 117.5s)\n",
      "antsy (117.5s - 117.7s)\n",
      "and (117.7s - 118.0s)\n",
      "really (118.0s - 118.3s)\n",
      "want (118.3s - 118.5s)\n",
      "to (118.5s - 118.5s)\n",
      "start (118.5s - 118.8s)\n",
      "working (118.8s - 118.9s)\n",
      "on (118.9s - 119.2s)\n",
      "it (119.2s - 119.3s)\n",
      "right (119.3s - 119.5s)\n",
      "now (119.5s - 119.7s)\n",
      "then (119.7s - 120.4s)\n",
      "you (120.4s - 120.6s)\n",
      "can (120.6s - 120.9s)\n",
      "look (120.9s - 121.1s)\n",
      "at (121.1s - 121.4s)\n",
      "last (121.4s - 121.7s)\n",
      "year's (121.7s - 121.8s)\n",
      "version (121.8s - 122.1s)\n",
      "of (122.1s - 122.6s)\n",
      "assignment (122.6s - 123.1s)\n",
      "one (123.1s - 123.3s)\n",
      "it'll (123.3s - 123.8s)\n",
      "be (123.8s - 123.8s)\n",
      "pretty (123.8s - 124.2s)\n",
      "much (124.2s - 124.3s)\n",
      "the (124.3s - 124.4s)\n",
      "same (124.4s - 124.6s)\n",
      "content (124.6s - 125.1s)\n",
      "we're (125.1s - 125.9s)\n",
      "just (125.9s - 126.2s)\n",
      "reshuffling (126.2s - 126.7s)\n",
      "it (126.7s - 126.9s)\n",
      "a (126.9s - 127.0s)\n",
      "little (127.0s - 127.1s)\n",
      "bit (127.1s - 127.4s)\n",
      "to (127.4s - 127.5s)\n",
      "make (127.5s - 127.6s)\n",
      "it (127.6s - 127.8s)\n",
      "like (127.8s - 128.3s)\n",
      "for (128.3s - 128.4s)\n",
      "example (128.4s - 128.5s)\n",
      "operating (128.5s - 129.2s)\n",
      "to (129.2s - 129.3s)\n",
      "work (129.3s - 129.5s)\n",
      "with (129.5s - 129.6s)\n",
      "Python (129.6s - 129.8s)\n",
      "3 (129.8s - 130.2s)\n",
      "rather (130.2s - 130.5s)\n",
      "than (130.5s - 130.7s)\n",
      "Python (130.7s - 130.8s)\n",
      "2.7 (130.8s - 131.2s)\n",
      "some (131.2s - 132.4s)\n",
      "of (132.4s - 132.5s)\n",
      "these (132.5s - 132.6s)\n",
      "minor (132.6s - 132.9s)\n",
      "cosmetic (132.9s - 133.4s)\n",
      "changes (133.4s - 133.7s)\n",
      "but (133.7s - 134.3s)\n",
      "the (134.3s - 134.4s)\n",
      "content (134.4s - 134.8s)\n",
      "of (134.8s - 134.9s)\n",
      "the (134.9s - 135.0s)\n",
      "excitement (135.0s - 135.6s)\n",
      "will (135.6s - 135.7s)\n",
      "still (135.7s - 135.9s)\n",
      "be (135.9s - 135.9s)\n",
      "the (135.9s - 136.0s)\n",
      "same (136.0s - 136.1s)\n",
      "as (136.1s - 136.4s)\n",
      "last (136.4s - 136.5s)\n",
      "year (136.5s - 136.7s)\n",
      "so (136.7s - 138.0s)\n",
      "in (138.0s - 138.1s)\n",
      "this (138.1s - 138.2s)\n",
      "assignment (138.2s - 138.5s)\n",
      "you'll (138.5s - 139.1s)\n",
      "be (139.1s - 139.2s)\n",
      "implementing (139.2s - 139.9s)\n",
      "your (139.9s - 140.0s)\n",
      "own (140.0s - 140.3s)\n",
      "K (140.3s - 140.6s)\n",
      "nearest (140.6s - 140.8s)\n",
      "neighbor (140.8s - 140.9s)\n",
      "classifier (140.9s - 141.7s)\n",
      "which (141.7s - 141.9s)\n",
      "were (141.9s - 142.0s)\n",
      "going (142.0s - 142.1s)\n",
      "to (142.1s - 142.2s)\n",
      "talk (142.2s - 142.4s)\n",
      "about (142.4s - 142.5s)\n",
      "in (142.5s - 142.7s)\n",
      "this (142.7s - 142.8s)\n",
      "lecture (142.8s - 143.0s)\n",
      "you (143.0s - 143.9s)\n",
      "also (143.9s - 144.1s)\n",
      "and (144.1s - 144.3s)\n",
      "implemented (144.3s - 144.8s)\n",
      "several (144.8s - 145.2s)\n",
      "different (145.2s - 145.5s)\n",
      "linear (145.5s - 145.8s)\n",
      "classifiers (145.8s - 146.5s)\n",
      "including (146.5s - 147.0s)\n",
      "the (147.0s - 147.2s)\n",
      "Espeon (147.2s - 147.7s)\n",
      "and (147.7s - 147.8s)\n",
      "softmax (147.8s - 148.2s)\n",
      "as (148.2s - 148.9s)\n",
      "well (148.9s - 149.1s)\n",
      "as (149.1s - 149.3s)\n",
      "a (149.3s - 149.4s)\n",
      "simple (149.4s - 149.7s)\n",
      "to (149.7s - 149.9s)\n",
      "their (149.9s - 150.1s)\n",
      "neural (150.1s - 150.3s)\n",
      "network (150.3s - 150.7s)\n",
      "and (150.7s - 151.2s)\n",
      "will (151.2s - 151.5s)\n",
      "cover (151.5s - 151.7s)\n",
      "all (151.7s - 151.8s)\n",
      "this (151.8s - 151.9s)\n",
      "content (151.9s - 152.3s)\n",
      "over (152.3s - 152.5s)\n",
      "the (152.5s - 152.6s)\n",
      "next (152.6s - 152.8s)\n",
      "couple (152.8s - 152.8s)\n",
      "of (152.8s - 153.2s)\n",
      "boxers (153.2s - 153.7s)\n",
      "so (155.9s - 156.4s)\n",
      "all (156.4s - 156.7s)\n",
      "of (156.7s - 156.7s)\n",
      "our (156.7s - 156.9s)\n",
      "assignments (156.9s - 157.4s)\n",
      "are (157.4s - 157.5s)\n",
      "using (157.5s - 157.8s)\n",
      "Python (157.8s - 158.2s)\n",
      "and (158.2s - 158.5s)\n",
      "numpy (158.5s - 158.8s)\n",
      "if (158.8s - 159.7s)\n",
      "you (159.7s - 160.0s)\n",
      "aren't (160.0s - 160.6s)\n",
      "familiar (160.6s - 160.7s)\n",
      "with (160.7s - 161.3s)\n",
      "python (161.3s - 161.6s)\n",
      "or (161.6s - 161.9s)\n",
      "numpy (161.9s - 162.3s)\n",
      "then (162.3s - 162.7s)\n",
      "we (162.7s - 162.9s)\n",
      "have (162.9s - 163.1s)\n",
      "written (163.1s - 163.5s)\n",
      "a (163.5s - 163.5s)\n",
      "tutorial (163.5s - 163.8s)\n",
      "that (163.8s - 164.1s)\n",
      "you (164.1s - 164.3s)\n",
      "can (164.3s - 164.5s)\n",
      "find (164.5s - 164.8s)\n",
      "on (164.8s - 164.9s)\n",
      "the (164.9s - 165.0s)\n",
      "course (165.0s - 165.3s)\n",
      "website (165.3s - 165.4s)\n",
      "to (165.4s - 165.8s)\n",
      "try (165.8s - 166.0s)\n",
      "and (166.0s - 166.1s)\n",
      "get (166.1s - 166.2s)\n",
      "you (166.2s - 166.3s)\n",
      "up (166.3s - 166.4s)\n",
      "to (166.4s - 166.5s)\n",
      "speed (166.5s - 166.6s)\n",
      "but (166.6s - 167.9s)\n",
      "this (167.9s - 168.2s)\n",
      "is (168.2s - 168.4s)\n",
      "actually (168.4s - 168.6s)\n",
      "pretty (168.6s - 168.8s)\n",
      "important (168.8s - 169.3s)\n",
      "numpy (169.3s - 170.3s)\n",
      "lets (170.3s - 170.6s)\n",
      "you (170.6s - 170.7s)\n",
      "write (170.7s - 170.9s)\n",
      "these (170.9s - 171.0s)\n",
      "like (171.0s - 171.2s)\n",
      "very (171.2s - 171.4s)\n",
      "efficient (171.4s - 171.9s)\n",
      "that (171.9s - 172.1s)\n",
      "dries (172.1s - 172.4s)\n",
      "operations (172.4s - 173.1s)\n",
      "that (173.1s - 173.6s)\n",
      "let (173.6s - 173.8s)\n",
      "you (173.8s - 174.0s)\n",
      "do (174.0s - 174.1s)\n",
      "quite (174.1s - 174.4s)\n",
      "a (174.4s - 174.4s)\n",
      "lot (174.4s - 174.4s)\n",
      "of (174.4s - 174.7s)\n",
      "competition (174.7s - 175.3s)\n",
      "and (175.3s - 175.5s)\n",
      "just (175.5s - 175.7s)\n",
      "a (175.7s - 175.7s)\n",
      "couple (175.7s - 175.8s)\n",
      "lines (175.8s - 176.2s)\n",
      "of (176.2s - 176.3s)\n",
      "code (176.3s - 176.6s)\n",
      "so (176.6s - 177.2s)\n",
      "this (177.2s - 177.5s)\n",
      "is (177.5s - 177.7s)\n",
      "super (177.7s - 178.3s)\n",
      "important (178.3s - 178.5s)\n",
      "for (178.5s - 178.9s)\n",
      "pretty (178.9s - 179.1s)\n",
      "much (179.1s - 179.2s)\n",
      "all (179.2s - 179.4s)\n",
      "aspects (179.4s - 180.1s)\n",
      "of (180.1s - 180.1s)\n",
      "numerical (180.1s - 180.9s)\n",
      "Computing (180.9s - 181.3s)\n",
      "and (181.3s - 181.4s)\n",
      "machine (181.4s - 181.8s)\n",
      "learning (181.8s - 181.9s)\n",
      "and (181.9s - 182.3s)\n",
      "everything (182.3s - 182.6s)\n",
      "like (182.6s - 182.8s)\n",
      "that (182.8s - 182.9s)\n",
      "is (182.9s - 183.6s)\n",
      "officially (183.6s - 184.1s)\n",
      "implementing (184.1s - 184.9s)\n",
      "these (184.9s - 184.9s)\n",
      "vectorize (184.9s - 185.4s)\n",
      "operations (185.4s - 186.1s)\n",
      "and (186.1s - 187.0s)\n",
      "you'll (187.0s - 187.3s)\n",
      "get (187.3s - 187.6s)\n",
      "something (187.6s - 187.9s)\n",
      "out (187.9s - 188.1s)\n",
      "of (188.1s - 188.1s)\n",
      "practice (188.1s - 188.3s)\n",
      "with (188.3s - 188.6s)\n",
      "this (188.6s - 188.8s)\n",
      "on (188.8s - 189.0s)\n",
      "the (189.0s - 189.0s)\n",
      "first (189.0s - 189.2s)\n",
      "assignment (189.2s - 189.4s)\n",
      "so (189.4s - 190.3s)\n",
      "for (190.3s - 190.8s)\n",
      "those (190.8s - 190.9s)\n",
      "of (190.9s - 191.1s)\n",
      "you (191.1s - 191.2s)\n",
      "who (191.2s - 191.4s)\n",
      "don't (191.4s - 191.5s)\n",
      "have (191.5s - 191.8s)\n",
      "a (191.8s - 191.9s)\n",
      "lot (191.9s - 192.1s)\n",
      "of (192.1s - 192.1s)\n",
      "experience (192.1s - 192.8s)\n",
      "with (192.8s - 193.0s)\n",
      "Matlab (193.0s - 193.9s)\n",
      "or (193.9s - 194.3s)\n",
      "numpy (194.3s - 194.7s)\n",
      "or (194.7s - 195.1s)\n",
      "other (195.1s - 195.4s)\n",
      "types (195.4s - 195.6s)\n",
      "of (195.6s - 195.7s)\n",
      "vectorized (195.7s - 196.4s)\n",
      "tensor (196.4s - 197.1s)\n",
      "computation (197.1s - 197.8s)\n",
      "I (197.8s - 198.1s)\n",
      "recommend (198.1s - 198.6s)\n",
      "that (198.6s - 198.6s)\n",
      "you (198.6s - 198.8s)\n",
      "start (198.8s - 199.4s)\n",
      "looking (199.4s - 199.6s)\n",
      "at (199.6s - 199.7s)\n",
      "this (199.7s - 199.7s)\n",
      "assignment (199.7s - 200.1s)\n",
      "pretty (200.1s - 200.4s)\n",
      "early (200.4s - 200.7s)\n",
      "and (200.7s - 200.9s)\n",
      "also (200.9s - 201.1s)\n",
      "read (201.1s - 201.4s)\n",
      "carefully (201.4s - 201.6s)\n",
      "through (201.6s - 201.9s)\n",
      "the (201.9s - 201.9s)\n",
      "tutorial (201.9s - 202.4s)\n",
      "the (204.8s - 205.4s)\n",
      "other (205.4s - 205.6s)\n",
      "thing (205.6s - 205.8s)\n",
      "I (205.8s - 205.9s)\n",
      "wanted (205.9s - 206.0s)\n",
      "to (206.0s - 206.2s)\n",
      "talk (206.2s - 206.8s)\n",
      "about (206.8s - 206.9s)\n",
      "is (206.9s - 207.5s)\n",
      "that (207.5s - 207.5s)\n",
      "we're (207.5s - 208.3s)\n",
      "happy (208.3s - 208.6s)\n",
      "to (208.6s - 208.8s)\n",
      "announce (208.8s - 209.0s)\n",
      "that (209.0s - 209.2s)\n",
      "we (209.2s - 209.5s)\n",
      "got (209.5s - 209.8s)\n",
      "we're (209.8s - 210.2s)\n",
      "officially (210.2s - 210.7s)\n",
      "supported (210.7s - 211.0s)\n",
      "through (211.0s - 211.4s)\n",
      "Google (211.4s - 211.4s)\n",
      "Cloud (211.4s - 212.0s)\n",
      "for (212.0s - 212.2s)\n",
      "this (212.2s - 212.3s)\n",
      "class (212.3s - 212.5s)\n",
      "so (212.5s - 213.5s)\n",
      "Google (213.5s - 213.9s)\n",
      "cloud (213.9s - 214.3s)\n",
      "is (214.3s - 214.6s)\n",
      "somewhat (214.6s - 215.0s)\n",
      "similar (215.0s - 215.2s)\n",
      "to (215.2s - 215.4s)\n",
      "Amazon (215.4s - 216.0s)\n",
      "AWS (216.0s - 216.2s)\n",
      "you (216.2s - 216.8s)\n",
      "can (216.8s - 217.0s)\n",
      "go (217.0s - 217.2s)\n",
      "and (217.2s - 217.4s)\n",
      "start (217.4s - 217.7s)\n",
      "virtual (217.7s - 218.1s)\n",
      "machines (218.1s - 218.6s)\n",
      "up (218.6s - 218.7s)\n",
      "in (218.7s - 218.8s)\n",
      "the (218.8s - 218.9s)\n",
      "cloud (218.9s - 219.2s)\n",
      "these (219.2s - 219.9s)\n",
      "virtual (219.9s - 220.3s)\n",
      "machines (220.3s - 220.7s)\n",
      "can (220.7s - 220.8s)\n",
      "have (220.8s - 221.0s)\n",
      "gpus (221.0s - 221.6s)\n",
      "so (221.6s - 222.2s)\n",
      "we (222.2s - 222.6s)\n",
      "have (222.6s - 222.8s)\n",
      "assets (222.8s - 223.1s)\n",
      "well (223.1s - 223.4s)\n",
      "we're (223.4s - 223.8s)\n",
      "working (223.8s - 224.3s)\n",
      "on (224.3s - 224.4s)\n",
      "the (224.4s - 224.6s)\n",
      "tutorial (224.6s - 225.2s)\n",
      "for (225.2s - 225.4s)\n",
      "exactly (225.4s - 225.8s)\n",
      "how (225.8s - 226.0s)\n",
      "to (226.0s - 226.2s)\n",
      "use (226.2s - 226.4s)\n",
      "Google (226.4s - 226.5s)\n",
      "cloud (226.5s - 226.9s)\n",
      "and (226.9s - 227.1s)\n",
      "get (227.1s - 227.3s)\n",
      "it (227.3s - 227.3s)\n",
      "to (227.3s - 227.4s)\n",
      "work (227.4s - 227.6s)\n",
      "for (227.6s - 227.7s)\n",
      "the (227.7s - 227.7s)\n",
      "assignments (227.7s - 228.3s)\n",
      "but (228.3s - 228.9s)\n",
      "Our (228.9s - 229.0s)\n",
      "intention (229.0s - 229.3s)\n",
      "is (229.3s - 229.6s)\n",
      "that (229.6s - 229.7s)\n",
      "you'll (229.7s - 230.7s)\n",
      "be (230.7s - 230.8s)\n",
      "able (230.8s - 230.9s)\n",
      "to (230.9s - 231.1s)\n",
      "just (231.1s - 231.3s)\n",
      "download (231.3s - 231.6s)\n",
      "some (231.6s - 232.3s)\n",
      "some (232.3s - 232.6s)\n",
      "image (232.6s - 232.9s)\n",
      "and (232.9s - 233.1s)\n",
      "it'll (233.1s - 233.3s)\n",
      "be (233.3s - 233.3s)\n",
      "very (233.3s - 233.4s)\n",
      "seamless (233.4s - 233.9s)\n",
      "for (233.9s - 234.1s)\n",
      "you (234.1s - 234.2s)\n",
      "to (234.2s - 234.4s)\n",
      "work (234.4s - 234.6s)\n",
      "on (234.6s - 234.7s)\n",
      "your (234.7s - 234.8s)\n",
      "assignment (234.8s - 235.3s)\n",
      "on (235.3s - 235.6s)\n",
      "one (235.6s - 235.8s)\n",
      "of (235.8s - 235.9s)\n",
      "these (235.9s - 236.0s)\n",
      "instances (236.0s - 236.3s)\n",
      "on (236.3s - 236.6s)\n",
      "the (236.6s - 236.7s)\n",
      "cloud (236.7s - 237.1s)\n",
      "and (237.1s - 238.1s)\n",
      "because (238.1s - 238.5s)\n",
      "Google (238.5s - 239.0s)\n",
      "has (239.0s - 239.2s)\n",
      "very (239.2s - 239.6s)\n",
      "generously (239.6s - 239.6s)\n",
      "supported (239.6s - 240.6s)\n",
      "this (240.6s - 240.9s)\n",
      "course (240.9s - 241.3s)\n",
      "will (241.3s - 241.8s)\n",
      "be (241.8s - 241.8s)\n",
      "able (241.8s - 241.9s)\n",
      "to (241.9s - 242.1s)\n",
      "distribute (242.1s - 242.7s)\n",
      "each (242.7s - 243.2s)\n",
      "of (243.2s - 243.3s)\n",
      "you (243.3s - 243.5s)\n",
      "on (243.5s - 243.7s)\n",
      "coupons (243.7s - 244.2s)\n",
      "to (244.2s - 244.3s)\n",
      "let (244.3s - 244.5s)\n",
      "you (244.5s - 244.6s)\n",
      "use (244.6s - 244.7s)\n",
      "Google (244.7s - 245.2s)\n",
      "credits (245.2s - 245.6s)\n",
      "Google (245.6s - 246.1s)\n",
      "Cloud (246.1s - 246.3s)\n",
      "credits (246.3s - 246.7s)\n",
      "for (246.7s - 246.8s)\n",
      "free (246.8s - 247.0s)\n",
      "for (247.0s - 247.3s)\n",
      "the (247.3s - 247.3s)\n",
      "class (247.3s - 247.6s)\n",
      "so (247.6s - 248.8s)\n",
      "you (248.8s - 249.0s)\n",
      "can (249.0s - 249.2s)\n",
      "feel (249.2s - 249.5s)\n",
      "free (249.5s - 249.7s)\n",
      "to (249.7s - 249.8s)\n",
      "use (249.8s - 249.9s)\n",
      "these (249.9s - 250.1s)\n",
      "for (250.1s - 250.4s)\n",
      "the (250.4s - 250.7s)\n",
      "assignments (250.7s - 251.4s)\n",
      "and (251.4s - 252.0s)\n",
      "also (252.0s - 252.1s)\n",
      "for (252.1s - 252.4s)\n",
      "the (252.4s - 252.4s)\n",
      "course (252.4s - 252.7s)\n",
      "projects (252.7s - 253.1s)\n",
      "when (253.1s - 253.4s)\n",
      "you (253.4s - 253.5s)\n",
      "want (253.5s - 253.7s)\n",
      "to (253.7s - 253.8s)\n",
      "start (253.8s - 253.9s)\n",
      "using (253.9s - 254.1s)\n",
      "gpus (254.1s - 254.8s)\n",
      "and (254.8s - 255.0s)\n",
      "larger (255.0s - 255.4s)\n",
      "machines (255.4s - 255.8s)\n",
      "and (255.8s - 255.9s)\n",
      "whatnot (255.9s - 256.2s)\n",
      "so (256.2s - 257.3s)\n",
      "will (257.3s - 257.5s)\n",
      "post (257.5s - 257.9s)\n",
      "more (257.9s - 258.0s)\n",
      "details (258.0s - 258.1s)\n",
      "about (258.1s - 258.6s)\n",
      "that (258.6s - 259.1s)\n",
      "probably (259.1s - 259.4s)\n",
      "on (259.4s - 259.5s)\n",
      "Piazza (259.5s - 259.8s)\n",
      "later (259.8s - 260.2s)\n",
      "today (260.2s - 260.4s)\n",
      "but (260.4s - 261.2s)\n",
      "I (261.2s - 261.3s)\n",
      "just (261.3s - 261.5s)\n",
      "wanted (261.5s - 261.7s)\n",
      "to (261.7s - 261.8s)\n",
      "mention (261.8s - 261.9s)\n",
      "it (261.9s - 262.2s)\n",
      "cuz (262.2s - 262.4s)\n",
      "I (262.4s - 262.5s)\n",
      "know (262.5s - 262.7s)\n",
      "there (262.7s - 262.8s)\n",
      "had (262.8s - 263.0s)\n",
      "been (263.0s - 263.2s)\n",
      "a (263.2s - 263.6s)\n",
      "couple (263.6s - 263.8s)\n",
      "of (263.8s - 264.0s)\n",
      "questions (264.0s - 264.5s)\n",
      "about (264.5s - 264.5s)\n",
      "can (264.8s - 265.7s)\n",
      "I (265.7s - 265.8s)\n",
      "use (265.8s - 265.9s)\n",
      "my (265.9s - 266.0s)\n",
      "laptop (266.0s - 266.3s)\n",
      "do (266.3s - 266.6s)\n",
      "I (266.6s - 266.7s)\n",
      "have (266.7s - 266.8s)\n",
      "to (266.8s - 266.9s)\n",
      "run (266.9s - 267.0s)\n",
      "on (267.0s - 267.1s)\n",
      "corn (267.1s - 267.6s)\n",
      "by (267.6s - 267.7s)\n",
      "after (267.7s - 267.9s)\n",
      "whatever (267.9s - 268.5s)\n",
      "and (268.5s - 269.2s)\n",
      "the (269.2s - 269.3s)\n",
      "answer (269.3s - 269.5s)\n",
      "is (269.5s - 269.6s)\n",
      "that (269.6s - 269.8s)\n",
      "you'll (269.8s - 270.3s)\n",
      "be (270.3s - 270.3s)\n",
      "able (270.3s - 270.3s)\n",
      "to (270.3s - 270.5s)\n",
      "run (270.5s - 270.6s)\n",
      "on (270.6s - 270.7s)\n",
      "Google (270.7s - 271.1s)\n",
      "cloud (271.1s - 271.3s)\n",
      "and (271.3s - 271.6s)\n",
      "will (271.6s - 272.3s)\n",
      "provide (272.3s - 273.1s)\n",
      "you (273.1s - 273.2s)\n",
      "some (273.2s - 273.4s)\n",
      "coupons (273.4s - 273.8s)\n",
      "for (273.8s - 273.9s)\n",
      "that (273.9s - 274.1s)\n",
      "yeah (275.8s - 277.1s)\n",
      "so (277.1s - 278.0s)\n",
      "that's (278.0s - 278.8s)\n",
      "those (278.8s - 279.4s)\n",
      "are (279.4s - 279.6s)\n",
      "kind (279.6s - 279.7s)\n",
      "of (279.7s - 279.8s)\n",
      "the (279.8s - 279.9s)\n",
      "major (279.9s - 280.2s)\n",
      "administrative (280.2s - 280.7s)\n",
      "issues (280.7s - 281.2s)\n",
      "I (281.2s - 281.3s)\n",
      "wanted (281.3s - 281.5s)\n",
      "to (281.5s - 281.6s)\n",
      "talk (281.6s - 281.7s)\n",
      "about (281.7s - 281.9s)\n",
      "today (281.9s - 282.2s)\n",
      "and (282.2s - 283.0s)\n",
      "then (283.0s - 283.3s)\n",
      "let's (283.3s - 283.9s)\n",
      "dive (283.9s - 284.4s)\n",
      "into (284.4s - 284.6s)\n",
      "the (284.6s - 284.7s)\n",
      "content (284.7s - 285.2s)\n",
      "so (286.6s - 287.0s)\n",
      "the (287.0s - 287.2s)\n",
      "last (287.2s - 287.4s)\n",
      "picture (287.4s - 287.7s)\n",
      "we (287.7s - 287.9s)\n",
      "talked (287.9s - 288.1s)\n",
      "a (288.1s - 288.2s)\n",
      "little (288.2s - 288.3s)\n",
      "bit (288.3s - 288.5s)\n",
      "about (288.5s - 288.7s)\n",
      "this (288.7s - 289.2s)\n",
      "this (289.2s - 289.4s)\n",
      "task (289.4s - 289.8s)\n",
      "of (289.8s - 289.9s)\n",
      "image (289.9s - 290.2s)\n",
      "classification (290.2s - 290.5s)\n",
      "which (290.5s - 291.4s)\n",
      "is (291.4s - 291.6s)\n",
      "really (291.6s - 291.8s)\n",
      "a (291.8s - 291.8s)\n",
      "courthouse (291.8s - 292.3s)\n",
      "in (292.3s - 292.5s)\n",
      "computer (292.5s - 293.0s)\n",
      "vision (293.0s - 293.1s)\n",
      "and (293.1s - 293.8s)\n",
      "this (293.8s - 293.9s)\n",
      "is (293.9s - 294.0s)\n",
      "something (294.0s - 294.3s)\n",
      "that (294.3s - 294.4s)\n",
      "will (294.4s - 294.8s)\n",
      "really (294.8s - 295.1s)\n",
      "focus (295.1s - 295.5s)\n",
      "on (295.5s - 295.6s)\n",
      "throughout (295.6s - 295.9s)\n",
      "the (295.9s - 296.0s)\n",
      "course (296.0s - 296.3s)\n",
      "of (296.3s - 296.4s)\n",
      "the (296.4s - 296.6s)\n",
      "class (296.6s - 296.9s)\n",
      "is (296.9s - 297.3s)\n",
      "exactly (297.3s - 298.1s)\n",
      "how (298.1s - 298.2s)\n",
      "do (298.2s - 298.4s)\n",
      "we (298.4s - 298.4s)\n",
      "work (298.4s - 298.7s)\n",
      "on (298.7s - 298.8s)\n",
      "this (298.8s - 299.0s)\n",
      "image (299.0s - 299.3s)\n",
      "classification (299.3s - 299.6s)\n",
      "task (299.6s - 300.4s)\n",
      "so (300.4s - 301.1s)\n",
      "a (301.1s - 301.2s)\n",
      "little (301.2s - 301.4s)\n",
      "bit (301.4s - 301.5s)\n",
      "more (301.5s - 301.6s)\n",
      "concretely (301.6s - 302.2s)\n",
      "when (302.2s - 303.0s)\n",
      "you're (303.0s - 303.2s)\n",
      "doing (303.2s - 303.2s)\n",
      "and (303.2s - 303.4s)\n",
      "his (303.4s - 303.5s)\n",
      "classification (303.5s - 303.9s)\n",
      "you (303.9s - 304.8s)\n",
      "receive (304.8s - 305.2s)\n",
      "your (305.2s - 305.5s)\n",
      "system (305.5s - 306.0s)\n",
      "receives (306.0s - 306.4s)\n",
      "some (306.4s - 306.6s)\n",
      "input (306.6s - 307.0s)\n",
      "image (307.0s - 307.3s)\n",
      "which (307.3s - 307.7s)\n",
      "is (307.7s - 307.9s)\n",
      "this (307.9s - 308.0s)\n",
      "cute (308.0s - 308.3s)\n",
      "cat (308.3s - 308.6s)\n",
      "in (308.6s - 308.7s)\n",
      "this (308.7s - 308.8s)\n",
      "example (308.8s - 309.0s)\n",
      "and (309.0s - 310.1s)\n",
      "the (310.1s - 310.1s)\n",
      "system (310.1s - 310.5s)\n",
      "is (310.5s - 310.5s)\n",
      "aware (310.5s - 311.0s)\n",
      "of (311.0s - 311.1s)\n",
      "some (311.1s - 311.4s)\n",
      "some (311.4s - 311.8s)\n",
      "predetermined (311.8s - 312.4s)\n",
      "set (312.4s - 313.1s)\n",
      "of (313.1s - 313.3s)\n",
      "categories (313.3s - 313.7s)\n",
      "or (313.7s - 314.1s)\n",
      "labels (314.1s - 314.7s)\n",
      "so (314.7s - 315.8s)\n",
      "these (315.8s - 316.0s)\n",
      "might (316.0s - 316.1s)\n",
      "be (316.1s - 316.3s)\n",
      "like (316.3s - 316.9s)\n",
      "a (316.9s - 317.0s)\n",
      "dog (317.0s - 317.6s)\n",
      "or (317.6s - 317.7s)\n",
      "a (317.7s - 317.8s)\n",
      "cat (317.8s - 318.0s)\n",
      "or (318.0s - 318.2s)\n",
      "a (318.2s - 318.3s)\n",
      "truck (318.3s - 318.6s)\n",
      "or (318.6s - 318.8s)\n",
      "a (318.8s - 318.8s)\n",
      "plane (318.8s - 319.2s)\n",
      "and (319.2s - 319.6s)\n",
      "there's (319.6s - 319.8s)\n",
      "some (319.8s - 320.0s)\n",
      "fixed (320.0s - 320.4s)\n",
      "set (320.4s - 320.7s)\n",
      "of (320.7s - 320.9s)\n",
      "category (320.9s - 321.5s)\n",
      "labels (321.5s - 321.9s)\n",
      "and (321.9s - 322.4s)\n",
      "the (322.4s - 322.5s)\n",
      "job (322.5s - 322.6s)\n",
      "of (322.6s - 322.7s)\n",
      "the (322.7s - 322.8s)\n",
      "computer (322.8s - 323.1s)\n",
      "is (323.1s - 323.4s)\n",
      "to (323.4s - 323.6s)\n",
      "look (323.6s - 323.6s)\n",
      "at (323.6s - 323.8s)\n",
      "the (323.8s - 323.8s)\n",
      "picture (323.8s - 324.2s)\n",
      "and (324.2s - 324.6s)\n",
      "a (324.6s - 324.7s)\n",
      "sign (324.7s - 325.0s)\n",
      "at (325.0s - 325.1s)\n",
      "one (325.1s - 325.3s)\n",
      "of (325.3s - 325.4s)\n",
      "these (325.4s - 325.5s)\n",
      "fixed (325.5s - 325.9s)\n",
      "category (325.9s - 326.4s)\n",
      "labels (326.4s - 326.8s)\n",
      "this (326.8s - 328.0s)\n",
      "seems (328.0s - 328.4s)\n",
      "like (328.4s - 328.5s)\n",
      "a (328.5s - 328.6s)\n",
      "really (328.6s - 328.7s)\n",
      "easy (328.7s - 328.9s)\n",
      "problem (328.9s - 329.5s)\n",
      "because (329.5s - 330.0s)\n",
      "if (330.0s - 330.2s)\n",
      "you (330.2s - 330.4s)\n",
      "this (330.4s - 330.8s)\n",
      "because (330.8s - 331.0s)\n",
      "so (331.0s - 331.3s)\n",
      "much (331.3s - 331.5s)\n",
      "of (331.5s - 331.7s)\n",
      "your (331.7s - 331.8s)\n",
      "own (331.8s - 332.0s)\n",
      "visual (332.0s - 332.3s)\n",
      "system (332.3s - 332.8s)\n",
      "in (332.8s - 332.9s)\n",
      "your (332.9s - 333.0s)\n",
      "brain (333.0s - 333.3s)\n",
      "is (333.3s - 333.6s)\n",
      "hardwired (333.6s - 334.2s)\n",
      "for (334.2s - 334.9s)\n",
      "doing (334.9s - 335.3s)\n",
      "these (335.3s - 335.4s)\n",
      "sort (335.4s - 335.6s)\n",
      "of (335.6s - 335.7s)\n",
      "visual (335.7s - 335.9s)\n",
      "recognition (335.9s - 336.2s)\n",
      "tasks (336.2s - 336.9s)\n",
      "but (336.9s - 337.8s)\n",
      "this (337.8s - 338.0s)\n",
      "is (338.0s - 338.0s)\n",
      "actually (338.0s - 338.2s)\n",
      "a (338.2s - 338.4s)\n",
      "really (338.4s - 338.4s)\n",
      "really (338.4s - 338.9s)\n",
      "hard (338.9s - 339.0s)\n",
      "problem (339.0s - 339.6s)\n",
      "for (339.6s - 339.7s)\n",
      "a (339.7s - 339.8s)\n",
      "machine (339.8s - 340.3s)\n",
      "so (341.1s - 341.5s)\n",
      "if (341.5s - 341.7s)\n",
      "you (341.7s - 341.8s)\n",
      "take (341.8s - 342.0s)\n",
      "it (342.0s - 342.1s)\n",
      "and (342.1s - 342.2s)\n",
      "think (342.2s - 342.6s)\n",
      "about (342.6s - 342.7s)\n",
      "actually (342.7s - 343.2s)\n",
      "what (343.2s - 343.7s)\n",
      "does (343.7s - 343.8s)\n",
      "a (343.8s - 343.9s)\n",
      "computer (343.9s - 344.2s)\n",
      "see (344.2s - 344.6s)\n",
      "when (344.6s - 345.0s)\n",
      "it (345.0s - 345.0s)\n",
      "looks (345.0s - 345.2s)\n",
      "at (345.2s - 345.3s)\n",
      "this (345.3s - 345.4s)\n",
      "image (345.4s - 345.8s)\n",
      "it (345.8s - 346.4s)\n",
      "definitely (346.4s - 346.7s)\n",
      "doesn't (346.7s - 347.0s)\n",
      "get (347.0s - 347.2s)\n",
      "this (347.2s - 347.4s)\n",
      "holistic (347.4s - 347.6s)\n",
      "idea (347.6s - 348.3s)\n",
      "of (348.3s - 348.4s)\n",
      "a (348.4s - 348.5s)\n",
      "cat (348.5s - 348.8s)\n",
      "that (348.8s - 348.9s)\n",
      "you (348.9s - 349.2s)\n",
      "see (349.2s - 349.4s)\n",
      "when (349.4s - 349.6s)\n",
      "you (349.6s - 349.6s)\n",
      "look (349.6s - 349.8s)\n",
      "at (349.8s - 349.9s)\n",
      "it (349.9s - 350.0s)\n",
      "and (350.0s - 350.7s)\n",
      "the (350.7s - 350.8s)\n",
      "computer (350.8s - 351.2s)\n",
      "really (351.2s - 351.5s)\n",
      "is (351.5s - 351.6s)\n",
      "representing (351.6s - 352.1s)\n",
      "the (352.1s - 352.2s)\n",
      "image (352.2s - 352.5s)\n",
      "is (352.5s - 352.7s)\n",
      "this (352.7s - 352.8s)\n",
      "gigantic (352.8s - 353.2s)\n",
      "rid (353.2s - 353.5s)\n",
      "of (353.5s - 353.6s)\n",
      "numbers (353.6s - 354.0s)\n",
      "so (354.0s - 355.1s)\n",
      "you (355.1s - 355.5s)\n",
      "just (355.5s - 355.8s)\n",
      "so (355.8s - 356.2s)\n",
      "the (356.2s - 357.2s)\n",
      "image (357.2s - 357.6s)\n",
      "might (357.6s - 357.8s)\n",
      "be (357.8s - 357.8s)\n",
      "something (357.8s - 358.2s)\n",
      "like (358.2s - 358.3s)\n",
      "800 (358.3s - 358.9s)\n",
      "by (358.9s - 359.0s)\n",
      "600 (359.0s - 359.3s)\n",
      "pixels (359.5s - 359.7s)\n",
      "and (359.7s - 360.6s)\n",
      "each (360.6s - 360.8s)\n",
      "pixel (360.8s - 361.1s)\n",
      "is (361.1s - 361.4s)\n",
      "represented (361.4s - 362.0s)\n",
      "by (362.0s - 362.0s)\n",
      "three (362.0s - 362.3s)\n",
      "numbers (362.3s - 362.8s)\n",
      "getting (362.8s - 363.3s)\n",
      "the (363.3s - 363.5s)\n",
      "red (363.5s - 363.7s)\n",
      "green (363.7s - 364.1s)\n",
      "and (364.1s - 364.2s)\n",
      "blue (364.2s - 364.3s)\n",
      "values (364.3s - 364.8s)\n",
      "for (364.8s - 365.2s)\n",
      "that (365.2s - 365.3s)\n",
      "pixel (365.3s - 365.6s)\n",
      "so (365.6s - 366.5s)\n",
      "to (366.5s - 366.8s)\n",
      "the (366.8s - 366.9s)\n",
      "computer (366.9s - 367.2s)\n",
      "this (367.2s - 367.3s)\n",
      "is (367.3s - 367.5s)\n",
      "just (367.5s - 367.6s)\n",
      "a (367.6s - 367.7s)\n",
      "gigantic (367.7s - 368.1s)\n",
      "rid (368.1s - 368.3s)\n",
      "of (368.3s - 368.4s)\n",
      "numbers (368.4s - 368.8s)\n",
      "and (368.8s - 369.1s)\n",
      "it's (369.1s - 369.6s)\n",
      "very (369.6s - 369.8s)\n",
      "difficult (369.8s - 369.9s)\n",
      "to (369.9s - 370.3s)\n",
      "distill (370.3s - 370.8s)\n",
      "the (370.8s - 370.9s)\n",
      "Katniss (370.9s - 371.6s)\n",
      "out (371.6s - 371.8s)\n",
      "of (371.8s - 372.0s)\n",
      "this (372.0s - 372.1s)\n",
      "this (372.1s - 372.4s)\n",
      "like (372.4s - 372.7s)\n",
      "giant (372.7s - 373.2s)\n",
      "array (373.2s - 374.2s)\n",
      "of (374.2s - 374.5s)\n",
      "thousands (374.5s - 375.1s)\n",
      "or (375.1s - 375.3s)\n",
      "whatever (375.3s - 375.5s)\n",
      "a (375.5s - 375.7s)\n",
      "very (375.7s - 376.4s)\n",
      "many (376.4s - 376.5s)\n",
      "different (376.5s - 376.8s)\n",
      "number (376.8s - 377.0s)\n",
      "and (378.1s - 379.2s)\n",
      "this (379.2s - 379.4s)\n",
      "this (379.4s - 379.6s)\n",
      "so (379.6s - 380.1s)\n",
      "we (380.1s - 380.3s)\n",
      "refer (380.3s - 380.6s)\n",
      "to (380.6s - 380.6s)\n",
      "this (380.6s - 380.7s)\n",
      "problem (380.7s - 380.9s)\n",
      "is (380.9s - 381.2s)\n",
      "the (381.2s - 381.2s)\n",
      "semantic (381.2s - 381.7s)\n",
      "Gap (381.7s - 382.1s)\n",
      "that's (382.1s - 382.9s)\n",
      "the (382.9s - 383.2s)\n",
      "best (383.2s - 383.5s)\n",
      "idea (383.5s - 383.7s)\n",
      "of (383.7s - 383.9s)\n",
      "a (383.9s - 384.0s)\n",
      "cat (384.0s - 384.4s)\n",
      "or (384.4s - 384.7s)\n",
      "this (384.7s - 384.8s)\n",
      "label (384.8s - 385.1s)\n",
      "of (385.1s - 385.2s)\n",
      "a (385.2s - 385.3s)\n",
      "cat (385.3s - 385.6s)\n",
      "is (385.6s - 386.0s)\n",
      "a (386.0s - 386.0s)\n",
      "semantic (386.0s - 386.6s)\n",
      "label (386.6s - 387.0s)\n",
      "that (387.0s - 387.2s)\n",
      "were (387.2s - 387.3s)\n",
      "signing (387.3s - 387.7s)\n",
      "to (387.7s - 387.9s)\n",
      "this (387.9s - 388.0s)\n",
      "image (388.0s - 388.3s)\n",
      "and (388.3s - 388.9s)\n",
      "there's (388.9s - 389.1s)\n",
      "this (389.1s - 389.2s)\n",
      "huge (389.2s - 389.3s)\n",
      "gap (389.3s - 389.9s)\n",
      "between (389.9s - 389.9s)\n",
      "the (389.9s - 390.3s)\n",
      "semantic (390.3s - 390.7s)\n",
      "idea (390.7s - 390.9s)\n",
      "of (390.9s - 391.2s)\n",
      "a (391.2s - 391.3s)\n",
      "cat (391.3s - 391.6s)\n",
      "and (391.6s - 392.1s)\n",
      "these (392.1s - 392.4s)\n",
      "bees (392.4s - 392.6s)\n",
      "pixel (392.6s - 392.9s)\n",
      "values (392.9s - 393.4s)\n",
      "that (393.4s - 393.5s)\n",
      "the (393.5s - 393.6s)\n",
      "computer (393.6s - 393.9s)\n",
      "is (393.9s - 394.0s)\n",
      "actually (394.0s - 394.3s)\n",
      "seeing (394.3s - 394.6s)\n",
      "and (395.7s - 396.1s)\n",
      "this (396.1s - 396.2s)\n",
      "is (396.2s - 396.3s)\n",
      "a (396.3s - 396.4s)\n",
      "really (396.4s - 396.5s)\n",
      "hard (396.5s - 396.7s)\n",
      "problem (396.7s - 397.3s)\n",
      "because (397.3s - 397.8s)\n",
      "the (397.8s - 398.2s)\n",
      "you (398.2s - 398.6s)\n",
      "can (398.6s - 398.8s)\n",
      "change (398.8s - 399.2s)\n",
      "the (399.2s - 399.4s)\n",
      "picture (399.4s - 399.8s)\n",
      "in (399.8s - 400.0s)\n",
      "very (400.0s - 400.5s)\n",
      "small (400.5s - 401.0s)\n",
      "subtle (401.0s - 401.5s)\n",
      "ways (401.5s - 401.8s)\n",
      "that (401.8s - 402.1s)\n",
      "will (402.1s - 402.3s)\n",
      "cause (402.3s - 402.5s)\n",
      "the (402.5s - 402.6s)\n",
      "cause (402.6s - 402.7s)\n",
      "the (402.7s - 403.0s)\n",
      "pixel (403.0s - 403.3s)\n",
      "grid (403.3s - 403.6s)\n",
      "to (403.6s - 403.7s)\n",
      "change (403.7s - 404.0s)\n",
      "entirely (404.0s - 404.3s)\n",
      "so (404.3s - 405.1s)\n",
      "for (405.1s - 405.3s)\n",
      "example (405.3s - 405.3s)\n",
      "if (405.3s - 405.8s)\n",
      "we (405.8s - 405.9s)\n",
      "took (405.9s - 406.0s)\n",
      "the (406.0s - 406.2s)\n",
      "same (406.2s - 406.5s)\n",
      "cat (406.5s - 406.9s)\n",
      "and (406.9s - 407.2s)\n",
      "if (407.2s - 407.3s)\n",
      "it (407.3s - 407.4s)\n",
      "happened (407.4s - 408.0s)\n",
      "to (408.0s - 408.1s)\n",
      "sit (408.1s - 408.3s)\n",
      "still (408.3s - 408.6s)\n",
      "and (408.6s - 408.8s)\n",
      "not (408.8s - 409.0s)\n",
      "even (409.0s - 409.2s)\n",
      "twitch (409.2s - 409.6s)\n",
      "not (409.6s - 409.8s)\n",
      "move (409.8s - 410.0s)\n",
      "a (410.0s - 410.1s)\n",
      "muscle (410.1s - 410.1s)\n",
      "which (410.1s - 410.6s)\n",
      "is (410.6s - 410.7s)\n",
      "never (410.7s - 410.9s)\n",
      "going (410.9s - 411.1s)\n",
      "to (411.1s - 411.1s)\n",
      "happen (411.1s - 411.2s)\n",
      "but (411.2s - 412.2s)\n",
      "we (412.2s - 412.3s)\n",
      "moved (412.3s - 412.5s)\n",
      "the (412.5s - 412.7s)\n",
      "camera (412.7s - 413.0s)\n",
      "to (413.0s - 413.1s)\n",
      "the (413.1s - 413.2s)\n",
      "other (413.2s - 413.4s)\n",
      "side (413.4s - 413.6s)\n",
      "then (413.6s - 414.5s)\n",
      "every (414.5s - 415.0s)\n",
      "single (415.0s - 415.4s)\n",
      "grid (415.4s - 415.8s)\n",
      "every (415.8s - 416.2s)\n",
      "single (416.2s - 416.5s)\n",
      "Pixel (416.5s - 417.2s)\n",
      "in (417.2s - 417.3s)\n",
      "this (417.3s - 417.4s)\n",
      "giant (417.4s - 417.8s)\n",
      "grid (417.8s - 418.0s)\n",
      "of (418.0s - 418.1s)\n",
      "numbers (418.1s - 418.5s)\n",
      "would (418.5s - 418.8s)\n",
      "be (418.8s - 418.9s)\n",
      "completely (418.9s - 419.3s)\n",
      "different (419.3s - 419.7s)\n",
      "but (419.7s - 420.2s)\n",
      "somehow (420.2s - 420.4s)\n",
      "it (420.4s - 420.8s)\n",
      "still (420.8s - 420.9s)\n",
      "representing (420.9s - 421.4s)\n",
      "the (421.4s - 421.5s)\n",
      "same (421.5s - 421.8s)\n",
      "cat (421.8s - 422.1s)\n",
      "and (422.1s - 422.9s)\n",
      "our (422.9s - 423.0s)\n",
      "algorithms (423.0s - 423.3s)\n",
      "quinceanera (423.3s - 423.9s)\n",
      "busted (423.9s - 424.2s)\n",
      "us (424.2s - 424.4s)\n",
      "but (425.5s - 426.3s)\n",
      "not (426.3s - 426.5s)\n",
      "only (426.5s - 426.6s)\n",
      "Viewpoint (426.6s - 427.6s)\n",
      "is (427.6s - 427.8s)\n",
      "one (427.8s - 428.0s)\n",
      "problem (428.0s - 428.2s)\n",
      "another (428.2s - 428.5s)\n",
      "is (428.5s - 428.7s)\n",
      "illumination (428.7s - 429.2s)\n",
      "there's (429.2s - 429.5s)\n",
      "any (429.5s - 429.7s)\n",
      "different (429.7s - 430.0s)\n",
      "lighting (430.0s - 430.2s)\n",
      "conditions (430.2s - 430.8s)\n",
      "going (430.8s - 431.0s)\n",
      "on (431.0s - 431.2s)\n",
      "in (431.2s - 431.4s)\n",
      "the (431.4s - 431.5s)\n",
      "seen (431.5s - 431.8s)\n",
      "whether (431.8s - 432.7s)\n",
      "the (432.7s - 432.7s)\n",
      "cat (432.7s - 433.1s)\n",
      "is (433.1s - 433.2s)\n",
      "appearing (433.2s - 433.5s)\n",
      "in (433.5s - 433.6s)\n",
      "this (433.6s - 433.7s)\n",
      "like (433.7s - 433.9s)\n",
      "very (433.9s - 434.1s)\n",
      "dark (434.1s - 434.5s)\n",
      "Moody (434.5s - 434.8s)\n",
      "scene (434.8s - 435.3s)\n",
      "or (435.3s - 435.9s)\n",
      "like (435.9s - 436.1s)\n",
      "in (436.1s - 436.2s)\n",
      "this (436.2s - 436.3s)\n",
      "very (436.3s - 436.5s)\n",
      "bright (436.5s - 436.9s)\n",
      "Sun (436.9s - 437.2s)\n",
      "let's (437.2s - 437.4s)\n",
      "say (437.4s - 437.6s)\n",
      "it's (437.6s - 437.8s)\n",
      "still (437.8s - 438.0s)\n",
      "a (438.0s - 438.1s)\n",
      "cat (438.1s - 438.3s)\n",
      "and (438.3s - 438.7s)\n",
      "are (438.7s - 438.8s)\n",
      "all (438.8s - 438.9s)\n",
      "going (438.9s - 439.1s)\n",
      "to (439.1s - 439.4s)\n",
      "be (439.4s - 439.5s)\n",
      "robust (439.5s - 439.7s)\n",
      "to (439.7s - 439.9s)\n",
      "bat (439.9s - 440.2s)\n",
      "objects (441.1s - 442.0s)\n",
      "can (442.0s - 442.1s)\n",
      "also (442.1s - 442.4s)\n",
      "deform (442.4s - 442.9s)\n",
      "I (442.9s - 443.6s)\n",
      "think (443.6s - 443.9s)\n",
      "cats (443.9s - 444.1s)\n",
      "are (444.1s - 444.3s)\n",
      "may (444.3s - 444.4s)\n",
      "be (444.4s - 444.5s)\n",
      "among (444.5s - 444.7s)\n",
      "the (444.7s - 444.9s)\n",
      "more (444.9s - 445.0s)\n",
      "deformable (445.0s - 445.5s)\n",
      "of (445.5s - 445.7s)\n",
      "animals (445.7s - 446.2s)\n",
      "that (446.2s - 446.2s)\n",
      "you (446.2s - 446.4s)\n",
      "might (446.4s - 446.5s)\n",
      "see (446.5s - 446.7s)\n",
      "out (446.7s - 446.9s)\n",
      "there (446.9s - 447.0s)\n",
      "and (447.0s - 447.9s)\n",
      "cats (447.9s - 448.3s)\n",
      "can (448.3s - 448.4s)\n",
      "really (448.4s - 448.6s)\n",
      "assume (448.6s - 449.1s)\n",
      "a (449.1s - 449.1s)\n",
      "lot (449.1s - 449.3s)\n",
      "of (449.3s - 449.5s)\n",
      "different (449.5s - 449.8s)\n",
      "varied (449.8s - 450.5s)\n",
      "poses (450.5s - 451.0s)\n",
      "and (451.0s - 451.1s)\n",
      "positions (451.1s - 451.7s)\n",
      "and (451.7s - 452.1s)\n",
      "our (452.1s - 452.4s)\n",
      "algorithms (452.4s - 452.6s)\n",
      "should (452.6s - 453.0s)\n",
      "be (453.0s - 453.0s)\n",
      "different (453.0s - 453.9s)\n",
      "kinds (453.9s - 454.2s)\n",
      "of (454.2s - 454.3s)\n",
      "transforms (454.3s - 454.9s)\n",
      "there (454.9s - 456.7s)\n",
      "can (456.7s - 456.8s)\n",
      "also (456.8s - 457.0s)\n",
      "be (457.0s - 457.1s)\n",
      "problems (457.1s - 457.6s)\n",
      "of (457.6s - 457.7s)\n",
      "occlusion (457.7s - 458.1s)\n",
      "where (458.1s - 459.2s)\n",
      "you (459.2s - 459.7s)\n",
      "can (459.7s - 459.8s)\n",
      "only (459.8s - 460.2s)\n",
      "you (460.2s - 460.4s)\n",
      "might (460.4s - 460.6s)\n",
      "only (460.6s - 460.7s)\n",
      "see (460.7s - 460.9s)\n",
      "part (460.9s - 461.1s)\n",
      "of (461.1s - 461.3s)\n",
      "a (461.3s - 461.4s)\n",
      "cat (461.4s - 461.7s)\n",
      "like (461.7s - 461.9s)\n",
      "just (461.9s - 462.1s)\n",
      "a (462.1s - 462.2s)\n",
      "face (462.2s - 462.5s)\n",
      "or (462.5s - 463.0s)\n",
      "in (463.0s - 463.1s)\n",
      "this (463.1s - 463.2s)\n",
      "extreme (463.2s - 463.6s)\n",
      "example (463.6s - 464.0s)\n",
      "just (464.0s - 464.2s)\n",
      "a (464.2s - 464.3s)\n",
      "tale (464.3s - 464.6s)\n",
      "peeking (464.6s - 465.0s)\n",
      "out (465.0s - 465.0s)\n",
      "from (465.0s - 465.2s)\n",
      "under (465.2s - 465.4s)\n",
      "the (465.4s - 465.5s)\n",
      "couch (465.5s - 465.7s)\n",
      "cushion (465.7s - 466.2s)\n",
      "but (466.2s - 467.2s)\n",
      "in (467.2s - 467.3s)\n",
      "these (467.3s - 467.4s)\n",
      "cases (467.4s - 467.7s)\n",
      "it's (467.7s - 468.0s)\n",
      "pretty (468.0s - 468.3s)\n",
      "easy (468.3s - 468.4s)\n",
      "for (468.4s - 468.7s)\n",
      "you (468.7s - 468.8s)\n",
      "as (468.8s - 469.1s)\n",
      "a (469.1s - 469.2s)\n",
      "person (469.2s - 469.6s)\n",
      "to (469.6s - 470.0s)\n",
      "realize (470.0s - 470.5s)\n",
      "that (470.5s - 470.6s)\n",
      "this (470.6s - 470.7s)\n",
      "is (470.7s - 470.8s)\n",
      "probably (470.8s - 471.1s)\n",
      "a (471.1s - 471.4s)\n",
      "cat (471.4s - 471.7s)\n",
      "and (471.7s - 472.2s)\n",
      "you (472.2s - 472.3s)\n",
      "still (472.3s - 472.5s)\n",
      "recognize (472.5s - 472.7s)\n",
      "these (472.7s - 473.2s)\n",
      "images (473.2s - 473.5s)\n",
      "of (473.5s - 473.7s)\n",
      "cats (473.7s - 474.2s)\n",
      "and (474.2s - 475.1s)\n",
      "this (475.1s - 475.2s)\n",
      "is (475.2s - 475.4s)\n",
      "something (475.4s - 475.6s)\n",
      "that (475.6s - 475.9s)\n",
      "are (475.9s - 476.2s)\n",
      "out (476.2s - 476.8s)\n",
      "also (476.8s - 477.4s)\n",
      "must (477.4s - 477.7s)\n",
      "be (477.7s - 477.8s)\n",
      "reported (477.8s - 478.1s)\n",
      "to (478.1s - 478.4s)\n",
      "which (478.4s - 478.8s)\n",
      "is (478.8s - 479.1s)\n",
      "quite (479.1s - 479.4s)\n",
      "difficult (479.4s - 479.7s)\n",
      "I (479.7s - 480.0s)\n",
      "think (480.0s - 480.4s)\n",
      "they (480.4s - 481.7s)\n",
      "can (481.7s - 481.9s)\n",
      "also (481.9s - 482.0s)\n",
      "be (482.0s - 482.2s)\n",
      "problems (482.2s - 482.6s)\n",
      "of (482.6s - 482.7s)\n",
      "background (482.7s - 483.1s)\n",
      "clutter (483.1s - 483.3s)\n",
      "where (483.3s - 484.2s)\n",
      "maybe (484.2s - 484.6s)\n",
      "the (484.6s - 484.7s)\n",
      "foreground (484.7s - 485.1s)\n",
      "objects (485.1s - 485.6s)\n",
      "the (485.6s - 486.0s)\n",
      "cat (486.0s - 486.3s)\n",
      "could (486.3s - 486.6s)\n",
      "actually (486.6s - 486.8s)\n",
      "be (486.8s - 487.0s)\n",
      "quite (487.0s - 487.3s)\n",
      "look (487.3s - 487.6s)\n",
      "quite (487.6s - 487.9s)\n",
      "similar (487.9s - 488.2s)\n",
      "in (488.2s - 488.4s)\n",
      "appearance (488.4s - 488.5s)\n",
      "the (488.5s - 489.0s)\n",
      "background (489.0s - 489.6s)\n",
      "in (489.6s - 490.1s)\n",
      "this (490.1s - 490.2s)\n",
      "is (490.2s - 490.3s)\n",
      "another (490.3s - 490.5s)\n",
      "thing (490.5s - 490.7s)\n",
      "that (490.7s - 491.0s)\n",
      "we (491.0s - 491.5s)\n",
      "need (491.5s - 491.5s)\n",
      "to (491.5s - 491.7s)\n",
      "handle (491.7s - 492.0s)\n",
      "there's (492.6s - 493.7s)\n",
      "also (493.7s - 493.9s)\n",
      "the (493.9s - 494.0s)\n",
      "problem (494.0s - 494.1s)\n",
      "of (494.1s - 494.6s)\n",
      "inter (494.6s - 495.1s)\n",
      "inter (495.1s - 495.4s)\n",
      "class (495.4s - 495.7s)\n",
      "variation (495.7s - 496.4s)\n",
      "that (496.4s - 496.9s)\n",
      "this (496.9s - 497.3s)\n",
      "one (497.3s - 497.6s)\n",
      "notion (497.6s - 497.9s)\n",
      "of (497.9s - 498.1s)\n",
      "Katniss (498.1s - 498.7s)\n",
      "actually (498.7s - 499.3s)\n",
      "spans (499.3s - 499.7s)\n",
      "a (499.7s - 499.8s)\n",
      "lot (499.8s - 500.0s)\n",
      "of (500.0s - 500.1s)\n",
      "different (500.1s - 500.3s)\n",
      "visual (500.3s - 500.5s)\n",
      "appearances (500.5s - 501.2s)\n",
      "and (501.2s - 501.6s)\n",
      "cats (501.6s - 502.0s)\n",
      "can (502.0s - 502.2s)\n",
      "come (502.2s - 502.3s)\n",
      "in (502.3s - 502.4s)\n",
      "different (502.4s - 502.5s)\n",
      "shapes (502.5s - 502.7s)\n",
      "and (502.7s - 503.1s)\n",
      "sizes (503.1s - 503.2s)\n",
      "and (503.2s - 503.7s)\n",
      "colors (503.7s - 504.1s)\n",
      "and (504.1s - 504.3s)\n",
      "AJ (504.3s - 504.5s)\n",
      "and (504.5s - 505.6s)\n",
      "are (505.6s - 505.7s)\n",
      "out (505.7s - 506.0s)\n",
      "again (506.0s - 506.4s)\n",
      "needs (506.4s - 506.8s)\n",
      "to (506.8s - 506.9s)\n",
      "work (506.9s - 507.3s)\n",
      "and (507.3s - 507.6s)\n",
      "handle (507.6s - 507.9s)\n",
      "all (507.9s - 507.9s)\n",
      "these (507.9s - 508.1s)\n",
      "different (508.1s - 508.3s)\n",
      "variations (508.3s - 509.0s)\n",
      "so (509.0s - 509.8s)\n",
      "this (509.8s - 510.5s)\n",
      "is (510.5s - 510.5s)\n",
      "actually (510.5s - 510.8s)\n",
      "a (510.8s - 510.9s)\n",
      "really (510.9s - 510.9s)\n",
      "really (510.9s - 511.4s)\n",
      "challenging (511.4s - 511.8s)\n",
      "problem (511.8s - 512.3s)\n",
      "and (512.3s - 513.5s)\n",
      "it's (513.5s - 513.7s)\n",
      "it's (513.7s - 513.9s)\n",
      "sort (513.9s - 514.1s)\n",
      "of (514.1s - 514.2s)\n",
      "easy (514.2s - 514.5s)\n",
      "to (514.5s - 514.6s)\n",
      "forget (514.6s - 515.0s)\n",
      "how (515.0s - 515.1s)\n",
      "easy (515.1s - 515.5s)\n",
      "this (515.5s - 515.6s)\n",
      "is (515.6s - 515.7s)\n",
      "because (515.7s - 517.0s)\n",
      "so (517.0s - 517.2s)\n",
      "much (517.2s - 517.5s)\n",
      "of (517.5s - 517.6s)\n",
      "your (517.6s - 517.8s)\n",
      "brain (517.8s - 518.0s)\n",
      "is (518.0s - 518.2s)\n",
      "specifically (518.2s - 519.0s)\n",
      "tuned (519.0s - 519.4s)\n",
      "for (519.4s - 519.6s)\n",
      "dealing (519.6s - 519.9s)\n",
      "with (519.9s - 520.0s)\n",
      "these (520.0s - 520.1s)\n",
      "things (520.1s - 520.5s)\n",
      "but (520.5s - 521.2s)\n",
      "now (521.2s - 521.3s)\n",
      "if (521.3s - 521.5s)\n",
      "we (521.5s - 521.6s)\n",
      "want (521.6s - 521.7s)\n",
      "our (521.7s - 521.9s)\n",
      "computer (521.9s - 522.3s)\n",
      "programs (522.3s - 522.8s)\n",
      "to (522.8s - 522.9s)\n",
      "deal (522.9s - 523.1s)\n",
      "with (523.1s - 523.3s)\n",
      "all (523.3s - 523.7s)\n",
      "of (523.7s - 523.8s)\n",
      "these (523.8s - 523.9s)\n",
      "problems (523.9s - 524.1s)\n",
      "while (524.1s - 524.6s)\n",
      "simultaneously (524.6s - 524.8s)\n",
      "and (524.8s - 526.0s)\n",
      "not (526.0s - 526.2s)\n",
      "just (526.2s - 526.4s)\n",
      "for (526.4s - 526.5s)\n",
      "cats (526.5s - 526.8s)\n",
      "by (526.8s - 527.1s)\n",
      "the (527.1s - 527.1s)\n",
      "way (527.1s - 527.3s)\n",
      "but (527.3s - 527.5s)\n",
      "for (527.5s - 527.7s)\n",
      "just (527.7s - 527.8s)\n",
      "about (527.8s - 528.0s)\n",
      "any (528.0s - 528.3s)\n",
      "of (528.3s - 528.4s)\n",
      "the (528.4s - 528.6s)\n",
      "category (528.6s - 529.1s)\n",
      "you (529.1s - 529.1s)\n",
      "can (529.1s - 529.3s)\n",
      "imagine (529.3s - 529.6s)\n",
      "this (529.6s - 530.1s)\n",
      "is (530.1s - 530.2s)\n",
      "a (530.2s - 530.3s)\n",
      "fantastically (530.3s - 531.0s)\n",
      "challenging (531.0s - 531.4s)\n",
      "problem (531.4s - 531.9s)\n",
      "and (531.9s - 532.3s)\n",
      "it's (532.3s - 532.5s)\n",
      "actually (532.5s - 532.8s)\n",
      "somewhat (532.8s - 533.2s)\n",
      "miraculous (533.2s - 533.8s)\n",
      "that (533.8s - 533.9s)\n",
      "this (533.9s - 534.1s)\n",
      "works (534.1s - 534.3s)\n",
      "at (534.3s - 534.4s)\n",
      "all (534.4s - 534.6s)\n",
      "in (534.6s - 534.8s)\n",
      "my (534.8s - 534.9s)\n",
      "opinion (534.9s - 535.1s)\n",
      "but (535.1s - 536.3s)\n",
      "actually (536.3s - 536.6s)\n",
      "not (536.6s - 536.9s)\n",
      "only (536.9s - 537.1s)\n",
      "does (537.1s - 537.3s)\n",
      "it (537.3s - 537.3s)\n",
      "work (537.3s - 537.7s)\n",
      "but (537.7s - 538.1s)\n",
      "these (538.1s - 538.3s)\n",
      "things (538.3s - 538.5s)\n",
      "work (538.5s - 538.8s)\n",
      "very (538.8s - 539.2s)\n",
      "close (539.2s - 539.5s)\n",
      "to (539.5s - 539.6s)\n",
      "human (539.6s - 539.9s)\n",
      "accuracy (539.9s - 540.3s)\n",
      "in (540.3s - 540.5s)\n",
      "some (540.5s - 541.1s)\n",
      "limited (541.1s - 541.5s)\n",
      "situations (541.5s - 541.8s)\n",
      "and (541.8s - 542.6s)\n",
      "take (542.6s - 543.0s)\n",
      "maybe (543.0s - 543.3s)\n",
      "only (543.3s - 543.5s)\n",
      "hundreds (543.5s - 544.0s)\n",
      "of (544.0s - 544.1s)\n",
      "Ms (544.1s - 544.5s)\n",
      "to (544.5s - 544.7s)\n",
      "do (544.7s - 544.8s)\n",
      "so (544.8s - 545.0s)\n",
      "so (545.0s - 545.7s)\n",
      "this (545.7s - 546.0s)\n",
      "is (546.0s - 546.2s)\n",
      "some (546.2s - 546.3s)\n",
      "pretty (546.3s - 546.5s)\n",
      "amazing (546.5s - 546.8s)\n",
      "incredible (546.8s - 547.4s)\n",
      "technology (547.4s - 548.0s)\n",
      "in (548.0s - 548.1s)\n",
      "my (548.1s - 548.2s)\n",
      "opinion (548.2s - 548.4s)\n",
      "and (548.4s - 549.1s)\n",
      "over (549.1s - 549.5s)\n",
      "the (549.5s - 549.6s)\n",
      "course (549.6s - 549.9s)\n",
      "of (549.9s - 550.2s)\n",
      "the (550.2s - 550.3s)\n",
      "rest (550.3s - 551.0s)\n",
      "of (551.0s - 551.0s)\n",
      "the (551.0s - 551.1s)\n",
      "class (551.1s - 551.4s)\n",
      "we (551.4s - 551.6s)\n",
      "really (551.6s - 551.7s)\n",
      "see (551.7s - 552.0s)\n",
      "what (552.6s - 552.8s)\n",
      "kinds (552.8s - 553.0s)\n",
      "of (553.0s - 553.0s)\n",
      "advancements (553.0s - 553.6s)\n",
      "have (553.6s - 553.6s)\n",
      "made (553.6s - 553.8s)\n",
      "this (553.8s - 553.9s)\n",
      "possible (553.9s - 554.1s)\n",
      "so (555.8s - 556.9s)\n",
      "now (556.9s - 557.0s)\n",
      "if (557.0s - 557.2s)\n",
      "you (557.2s - 557.2s)\n",
      "could (557.2s - 557.4s)\n",
      "have (557.4s - 557.6s)\n",
      "think (557.6s - 557.7s)\n",
      "about (557.7s - 558.0s)\n",
      "what (558.0s - 558.2s)\n",
      "is (558.2s - 558.3s)\n",
      "the (558.3s - 558.4s)\n",
      "API (558.4s - 558.8s)\n",
      "for (558.8s - 559.1s)\n",
      "writing (559.1s - 559.5s)\n",
      "an (559.5s - 559.6s)\n",
      "image (559.6s - 559.7s)\n",
      "classifier (559.7s - 560.3s)\n",
      "you (560.3s - 560.9s)\n",
      "might (560.9s - 561.2s)\n",
      "sit (561.2s - 561.4s)\n",
      "down (561.4s - 561.6s)\n",
      "and (561.6s - 561.7s)\n",
      "try (561.7s - 561.9s)\n",
      "to (561.9s - 561.9s)\n",
      "write (561.9s - 562.1s)\n",
      "a (562.1s - 562.4s)\n",
      "message (562.4s - 562.7s)\n",
      "in (562.7s - 562.8s)\n",
      "Python (562.8s - 563.3s)\n",
      "like (563.3s - 563.5s)\n",
      "this (563.5s - 563.7s)\n",
      "where (563.7s - 564.4s)\n",
      "we (564.4s - 564.6s)\n",
      "want (564.6s - 564.9s)\n",
      "to (564.9s - 564.9s)\n",
      "take (564.9s - 565.1s)\n",
      "an (565.1s - 565.2s)\n",
      "image (565.2s - 565.6s)\n",
      "and (565.6s - 565.9s)\n",
      "then (565.9s - 566.2s)\n",
      "do (566.2s - 566.4s)\n",
      "some (566.4s - 566.6s)\n",
      "crazy (566.6s - 566.8s)\n",
      "magic (566.8s - 567.1s)\n",
      "and (567.1s - 567.6s)\n",
      "then (567.6s - 567.7s)\n",
      "they (567.7s - 567.7s)\n",
      "spit (567.7s - 568.2s)\n",
      "out (568.2s - 568.3s)\n",
      "this (568.3s - 568.4s)\n",
      "class (568.4s - 568.8s)\n",
      "label (568.8s - 569.2s)\n",
      "to (569.2s - 569.4s)\n",
      "say (569.4s - 569.5s)\n",
      "cat (569.5s - 569.9s)\n",
      "or (569.9s - 570.0s)\n",
      "dog (570.0s - 570.3s)\n",
      "or (570.3s - 570.4s)\n",
      "what (570.4s - 570.5s)\n",
      "not (570.5s - 570.7s)\n",
      "and (570.7s - 571.3s)\n",
      "there's (571.3s - 571.8s)\n",
      "really (571.8s - 571.9s)\n",
      "no (571.9s - 572.2s)\n",
      "obvious (572.2s - 572.5s)\n",
      "way (572.5s - 572.8s)\n",
      "to (572.8s - 572.9s)\n",
      "do (572.9s - 573.2s)\n",
      "this (573.2s - 573.4s)\n",
      "right (573.4s - 574.1s)\n",
      "like (574.1s - 574.8s)\n",
      "if (574.8s - 575.1s)\n",
      "you're (575.1s - 575.2s)\n",
      "taking (575.2s - 575.5s)\n",
      "an (575.5s - 575.6s)\n",
      "algorithm (575.6s - 575.8s)\n",
      "class (575.8s - 576.4s)\n",
      "and (576.4s - 576.7s)\n",
      "your (576.7s - 576.8s)\n",
      "task (576.8s - 577.2s)\n",
      "is (577.2s - 577.2s)\n",
      "to (577.2s - 577.3s)\n",
      "sort (577.3s - 577.6s)\n",
      "numbers (577.6s - 578.2s)\n",
      "or (578.2s - 578.3s)\n",
      "computer (578.3s - 578.8s)\n",
      "convex (578.8s - 579.2s)\n",
      "Hull (579.2s - 579.4s)\n",
      "or (579.4s - 580.2s)\n",
      "even (580.2s - 580.6s)\n",
      "do (580.6s - 580.7s)\n",
      "something (580.7s - 581.0s)\n",
      "like (581.0s - 581.1s)\n",
      "RSA (581.1s - 581.5s)\n",
      "encryption (581.5s - 581.7s)\n",
      "there's (581.7s - 582.7s)\n",
      "a (582.7s - 583.0s)\n",
      "sort (583.0s - 583.1s)\n",
      "of (583.1s - 583.2s)\n",
      "can (583.2s - 583.6s)\n",
      "write (583.6s - 583.8s)\n",
      "down (583.8s - 584.0s)\n",
      "an (584.0s - 584.1s)\n",
      "algorithm (584.1s - 584.4s)\n",
      "and (584.4s - 584.8s)\n",
      "enumerate (584.8s - 585.2s)\n",
      "all (585.2s - 585.5s)\n",
      "the (585.5s - 585.5s)\n",
      "steps (585.5s - 586.0s)\n",
      "I (586.0s - 586.0s)\n",
      "need (586.0s - 586.1s)\n",
      "to (586.1s - 586.2s)\n",
      "happen (586.2s - 586.5s)\n",
      "in (586.5s - 587.1s)\n",
      "order (587.1s - 587.2s)\n",
      "for (587.2s - 587.4s)\n",
      "these (587.4s - 587.5s)\n",
      "things (587.5s - 587.8s)\n",
      "to (587.8s - 588.0s)\n",
      "to (588.0s - 588.3s)\n",
      "work (588.3s - 588.6s)\n",
      "but (588.6s - 589.4s)\n",
      "when (589.4s - 589.5s)\n",
      "were (589.5s - 589.7s)\n",
      "trying (589.7s - 589.8s)\n",
      "to (589.8s - 589.9s)\n",
      "recognize (589.9s - 590.3s)\n",
      "objects (590.3s - 591.4s)\n",
      "or (591.4s - 591.6s)\n",
      "recognize (591.6s - 592.0s)\n",
      "Kats (592.0s - 592.4s)\n",
      "or (592.4s - 592.5s)\n",
      "images (592.5s - 593.0s)\n",
      "there's (593.0s - 593.6s)\n",
      "no (593.6s - 593.8s)\n",
      "really (593.8s - 594.1s)\n",
      "clear (594.1s - 594.6s)\n",
      "explicit (594.6s - 595.2s)\n",
      "algorithm (595.2s - 595.8s)\n",
      "that (595.8s - 596.2s)\n",
      "makes (596.2s - 596.9s)\n",
      "intuitive (596.9s - 597.2s)\n",
      "sense (597.2s - 597.4s)\n",
      "for (597.4s - 598.2s)\n",
      "how (598.2s - 598.3s)\n",
      "you (598.3s - 598.5s)\n",
      "might (598.5s - 598.8s)\n",
      "go (598.8s - 599.0s)\n",
      "about (599.0s - 599.1s)\n",
      "recognizing (599.1s - 599.5s)\n",
      "these (599.5s - 600.0s)\n",
      "objects (600.0s - 600.5s)\n",
      "so (600.5s - 601.2s)\n",
      "this (601.2s - 601.6s)\n",
      "is (601.6s - 601.8s)\n",
      "Again (601.8s - 602.2s)\n",
      "by (602.2s - 602.4s)\n",
      "challenging (602.4s - 602.9s)\n",
      "if (602.9s - 603.1s)\n",
      "you (603.1s - 603.3s)\n",
      "think (603.3s - 603.5s)\n",
      "about (603.5s - 603.5s)\n",
      "it (603.5s - 603.8s)\n",
      "if (603.8s - 604.6s)\n",
      "you (604.6s - 604.6s)\n",
      "would (604.6s - 604.8s)\n",
      "do (604.8s - 604.9s)\n",
      "nothing (604.9s - 605.1s)\n",
      "if (605.1s - 605.3s)\n",
      "it (605.3s - 605.4s)\n",
      "was (605.4s - 605.5s)\n",
      "your (605.5s - 605.6s)\n",
      "first (605.6s - 605.8s)\n",
      "day (605.8s - 605.9s)\n",
      "program (605.9s - 606.4s)\n",
      "that (606.4s - 606.5s)\n",
      "you (606.5s - 606.6s)\n",
      "need (606.6s - 606.7s)\n",
      "and (606.7s - 606.8s)\n",
      "you (606.8s - 606.9s)\n",
      "don't (606.9s - 607.1s)\n",
      "have (607.1s - 607.3s)\n",
      "to (607.3s - 607.4s)\n",
      "sit (607.4s - 607.6s)\n",
      "down (607.6s - 607.7s)\n",
      "and (607.7s - 607.8s)\n",
      "write (607.8s - 608.0s)\n",
      "this (608.0s - 608.1s)\n",
      "function (608.1s - 608.5s)\n",
      "I (608.5s - 608.6s)\n",
      "think (608.6s - 608.9s)\n",
      "most (608.9s - 609.2s)\n",
      "people (609.2s - 609.3s)\n",
      "would (609.3s - 609.5s)\n",
      "be (609.5s - 609.9s)\n",
      "in (609.9s - 610.0s)\n",
      "trouble (610.0s - 610.1s)\n",
      "that (611.3s - 612.3s)\n",
      "being (612.3s - 612.5s)\n",
      "said (612.5s - 612.8s)\n",
      "people (612.8s - 613.1s)\n",
      "have (613.1s - 613.3s)\n",
      "definitely (613.3s - 613.7s)\n",
      "made (613.7s - 614.0s)\n",
      "explicit (614.0s - 614.3s)\n",
      "attempts (614.3s - 614.9s)\n",
      "to (614.9s - 615.1s)\n",
      "try (615.1s - 615.3s)\n",
      "to (615.3s - 615.5s)\n",
      "write (615.5s - 615.8s)\n",
      "sort (615.8s - 616.1s)\n",
      "of (616.1s - 616.2s)\n",
      "fan (616.2s - 616.5s)\n",
      "coded (616.5s - 617.0s)\n",
      "rules (617.0s - 617.4s)\n",
      "for (617.4s - 618.3s)\n",
      "recognizing (618.3s - 619.0s)\n",
      "different (619.0s - 619.3s)\n",
      "animals (619.3s - 619.6s)\n",
      "so (619.6s - 620.1s)\n",
      "we (620.1s - 620.2s)\n",
      "touched (620.2s - 620.5s)\n",
      "on (620.5s - 620.6s)\n",
      "this (620.6s - 620.7s)\n",
      "a (620.7s - 620.8s)\n",
      "little (620.8s - 621.0s)\n",
      "bit (621.0s - 621.1s)\n",
      "in (621.1s - 621.3s)\n",
      "the (621.3s - 621.4s)\n",
      "last (621.4s - 621.5s)\n",
      "lecture (621.5s - 621.9s)\n",
      "but (621.9s - 622.9s)\n",
      "maybe (622.9s - 623.1s)\n",
      "one (623.1s - 623.4s)\n",
      "idea (623.4s - 623.7s)\n",
      "for (623.7s - 623.9s)\n",
      "cats (623.9s - 624.4s)\n",
      "is (624.4s - 624.5s)\n",
      "that (624.5s - 624.6s)\n",
      "you (624.6s - 624.8s)\n",
      "know (624.8s - 625.0s)\n",
      "we (625.0s - 625.4s)\n",
      "know (625.4s - 625.6s)\n",
      "that (625.6s - 625.7s)\n",
      "cats (625.7s - 626.1s)\n",
      "have (626.1s - 626.2s)\n",
      "ears (626.2s - 626.6s)\n",
      "and (626.6s - 626.7s)\n",
      "eyes (626.7s - 627.0s)\n",
      "and (627.0s - 627.4s)\n",
      "mouths (627.4s - 627.7s)\n",
      "and (627.7s - 627.9s)\n",
      "noses (627.9s - 628.2s)\n",
      "and (628.2s - 628.8s)\n",
      "we (628.8s - 629.0s)\n",
      "know (629.0s - 629.1s)\n",
      "that (629.1s - 629.2s)\n",
      "edges (629.2s - 629.8s)\n",
      "are (629.8s - 629.9s)\n",
      "from (629.9s - 630.2s)\n",
      "Cuba (630.2s - 630.5s)\n",
      "and (630.5s - 630.6s)\n",
      "weasel (630.6s - 631.0s)\n",
      "we (631.0s - 631.2s)\n",
      "know (631.2s - 631.3s)\n",
      "the (631.3s - 631.4s)\n",
      "edges (631.4s - 631.9s)\n",
      "are (631.9s - 632.0s)\n",
      "pretty (632.0s - 632.3s)\n",
      "important (632.3s - 632.5s)\n",
      "when (632.5s - 632.9s)\n",
      "it (632.9s - 632.9s)\n",
      "comes (632.9s - 633.0s)\n",
      "to (633.0s - 633.3s)\n",
      "visual (633.3s - 633.6s)\n",
      "recognition (633.6s - 633.8s)\n",
      "so (633.8s - 634.8s)\n",
      "one (634.8s - 635.0s)\n",
      "thing (635.0s - 635.0s)\n",
      "we (635.0s - 635.2s)\n",
      "might (635.2s - 635.4s)\n",
      "try (635.4s - 635.6s)\n",
      "to (635.6s - 635.6s)\n",
      "do (635.6s - 635.9s)\n",
      "is (635.9s - 636.2s)\n",
      "confuse (636.2s - 637.1s)\n",
      "the (637.1s - 637.3s)\n",
      "edges (637.3s - 637.7s)\n",
      "of (637.7s - 637.8s)\n",
      "this (637.8s - 637.9s)\n",
      "image (637.9s - 638.1s)\n",
      "and (638.1s - 638.7s)\n",
      "then (638.7s - 638.8s)\n",
      "go (638.8s - 638.9s)\n",
      "in (638.9s - 639.0s)\n",
      "and (639.0s - 639.2s)\n",
      "try (639.2s - 639.3s)\n",
      "to (639.3s - 639.4s)\n",
      "categorize (639.4s - 639.9s)\n",
      "all (639.9s - 640.3s)\n",
      "the (640.3s - 640.4s)\n",
      "different (640.4s - 640.7s)\n",
      "corners (640.7s - 641.2s)\n",
      "and (641.2s - 641.3s)\n",
      "boundaries (641.3s - 641.9s)\n",
      "and (641.9s - 642.2s)\n",
      "say (642.2s - 642.8s)\n",
      "that (642.8s - 643.1s)\n",
      "you (643.1s - 643.2s)\n",
      "know (643.2s - 643.4s)\n",
      "if (643.4s - 643.5s)\n",
      "we (643.5s - 643.7s)\n",
      "have (643.7s - 643.8s)\n",
      "maybe (643.8s - 644.0s)\n",
      "three (644.0s - 644.3s)\n",
      "lines (644.3s - 644.6s)\n",
      "meeting (644.6s - 644.9s)\n",
      "this (644.9s - 645.0s)\n",
      "way (645.0s - 645.2s)\n",
      "and (645.2s - 645.3s)\n",
      "then (645.3s - 645.4s)\n",
      "it (645.4s - 645.5s)\n",
      "might (645.5s - 645.6s)\n",
      "be (645.6s - 645.8s)\n",
      "a (645.8s - 645.8s)\n",
      "corner (645.8s - 646.2s)\n",
      "and (646.2s - 646.4s)\n",
      "an (646.4s - 646.4s)\n",
      "ear (646.4s - 646.7s)\n",
      "has (646.7s - 646.9s)\n",
      "one (646.9s - 647.2s)\n",
      "corner (647.2s - 647.5s)\n",
      "here (647.5s - 647.7s)\n",
      "in (647.7s - 647.7s)\n",
      "one (647.7s - 647.9s)\n",
      "corner (647.9s - 648.2s)\n",
      "there (648.2s - 648.3s)\n",
      "in (648.3s - 648.5s)\n",
      "one (648.5s - 648.6s)\n",
      "corner (648.6s - 648.9s)\n",
      "there (648.9s - 649.2s)\n",
      "and (649.2s - 649.8s)\n",
      "then (649.8s - 650.0s)\n",
      "kind (650.0s - 650.2s)\n",
      "of (650.2s - 650.2s)\n",
      "right (650.2s - 650.5s)\n",
      "down (650.5s - 650.6s)\n",
      "this (650.6s - 650.8s)\n",
      "explicit (650.8s - 651.2s)\n",
      "set (651.2s - 651.8s)\n",
      "of (651.8s - 651.9s)\n",
      "rules (651.9s - 652.2s)\n",
      "for (652.2s - 652.3s)\n",
      "recognizing (652.3s - 652.9s)\n",
      "cats (652.9s - 653.4s)\n",
      "but (653.4s - 655.0s)\n",
      "this (655.0s - 655.1s)\n",
      "turns (655.1s - 655.4s)\n",
      "out (655.4s - 655.5s)\n",
      "not (655.5s - 655.7s)\n",
      "to (655.7s - 655.9s)\n",
      "work (655.9s - 656.1s)\n",
      "very (656.1s - 656.2s)\n",
      "well (656.2s - 656.4s)\n",
      "one (656.4s - 657.8s)\n",
      "is (657.8s - 658.0s)\n",
      "super (658.0s - 658.2s)\n",
      "brittle (658.2s - 658.6s)\n",
      "and (658.6s - 659.3s)\n",
      "to (659.3s - 659.7s)\n",
      "say (659.7s - 660.3s)\n",
      "if (660.3s - 661.0s)\n",
      "you (661.0s - 661.1s)\n",
      "want (661.1s - 661.3s)\n",
      "to (661.3s - 661.3s)\n",
      "start (661.3s - 661.6s)\n",
      "over (661.6s - 661.7s)\n",
      "for (661.7s - 662.0s)\n",
      "another (662.0s - 662.1s)\n",
      "object (662.1s - 662.4s)\n",
      "category (662.4s - 663.1s)\n",
      "and (663.1s - 663.7s)\n",
      "maybe (663.7s - 663.9s)\n",
      "not (663.9s - 664.0s)\n",
      "worried (664.0s - 664.3s)\n",
      "about (664.3s - 664.4s)\n",
      "cats (664.4s - 665.0s)\n",
      "but (665.0s - 665.2s)\n",
      "talk (665.2s - 665.4s)\n",
      "about (665.4s - 665.4s)\n",
      "trucks (665.4s - 666.0s)\n",
      "or (666.0s - 666.2s)\n",
      "dogs (666.2s - 666.6s)\n",
      "are (666.6s - 666.7s)\n",
      "fishes (666.7s - 667.2s)\n",
      "or (667.2s - 667.3s)\n",
      "something (667.3s - 667.6s)\n",
      "else (667.6s - 667.9s)\n",
      "then (667.9s - 668.4s)\n",
      "you (668.4s - 668.6s)\n",
      "need (668.6s - 668.7s)\n",
      "to (668.7s - 668.8s)\n",
      "start (668.8s - 668.8s)\n",
      "all (668.8s - 669.2s)\n",
      "over (669.2s - 669.3s)\n",
      "again (669.3s - 669.5s)\n",
      "so (669.5s - 670.3s)\n",
      "this (670.3s - 670.5s)\n",
      "is (670.5s - 670.8s)\n",
      "really (670.8s - 670.9s)\n",
      "not (670.9s - 671.2s)\n",
      "very (671.3s - 671.6s)\n",
      "scalable (671.6s - 671.9s)\n",
      "approach (671.9s - 672.1s)\n",
      "we (672.1s - 673.0s)\n",
      "want (673.0s - 673.3s)\n",
      "to (673.3s - 673.4s)\n",
      "come (673.4s - 673.6s)\n",
      "up (673.6s - 673.6s)\n",
      "with (673.6s - 673.8s)\n",
      "some (673.8s - 674.0s)\n",
      "algorithm (674.0s - 674.7s)\n",
      "or (674.7s - 674.9s)\n",
      "some (674.9s - 675.1s)\n",
      "method (675.1s - 675.7s)\n",
      "for (675.7s - 676.0s)\n",
      "these (676.0s - 676.9s)\n",
      "recognition (676.9s - 677.6s)\n",
      "tasks (677.6s - 678.0s)\n",
      "which (678.0s - 678.5s)\n",
      "scales (678.5s - 678.9s)\n",
      "much (678.9s - 679.1s)\n",
      "more (679.1s - 679.2s)\n",
      "naturally (679.2s - 679.7s)\n",
      "to (679.7s - 679.8s)\n",
      "all (679.8s - 680.0s)\n",
      "the (680.0s - 680.2s)\n",
      "variety (680.2s - 680.7s)\n",
      "of (680.7s - 680.8s)\n",
      "objects (680.8s - 681.2s)\n",
      "in (681.2s - 681.3s)\n",
      "the (681.3s - 681.4s)\n",
      "world (681.4s - 681.4s)\n",
      "so (684.2s - 684.6s)\n",
      "the (684.6s - 684.9s)\n",
      "Insight (684.9s - 685.3s)\n",
      "that (685.3s - 685.6s)\n",
      "sort (685.6s - 686.6s)\n",
      "of (686.6s - 686.6s)\n",
      "makes (686.6s - 686.8s)\n",
      "this (686.8s - 687.0s)\n",
      "all (687.0s - 687.2s)\n",
      "work (687.2s - 687.6s)\n",
      "is (687.6s - 688.1s)\n",
      "this (688.1s - 688.2s)\n",
      "idea (688.2s - 688.6s)\n",
      "of (688.6s - 688.7s)\n",
      "the (688.7s - 688.8s)\n",
      "data (688.8s - 689.0s)\n",
      "driven (689.0s - 689.2s)\n",
      "approach (689.2s - 689.8s)\n",
      "is (689.8s - 690.7s)\n",
      "that (690.7s - 690.8s)\n",
      "rather (690.8s - 691.6s)\n",
      "than (691.6s - 691.8s)\n",
      "sitting (691.8s - 692.4s)\n",
      "down (692.4s - 692.7s)\n",
      "and (692.7s - 692.9s)\n",
      "writing (692.9s - 693.1s)\n",
      "these (693.1s - 693.4s)\n",
      "hand (693.4s - 693.8s)\n",
      "specified (693.8s - 694.3s)\n",
      "rules (694.3s - 694.8s)\n",
      "to (694.8s - 695.0s)\n",
      "try (695.0s - 695.3s)\n",
      "to (695.3s - 695.4s)\n",
      "craft (695.4s - 695.9s)\n",
      "exactly (695.9s - 696.4s)\n",
      "what (696.4s - 696.6s)\n",
      "is (696.6s - 696.7s)\n",
      "a (696.7s - 696.7s)\n",
      "cat (696.7s - 697.1s)\n",
      "or (697.1s - 697.3s)\n",
      "a (697.3s - 697.3s)\n",
      "fish (697.3s - 697.6s)\n",
      "or (697.6s - 697.7s)\n",
      "what-have-you (697.7s - 698.0s)\n",
      "instead (698.0s - 699.6s)\n",
      "will (699.6s - 699.8s)\n",
      "go (699.8s - 700.0s)\n",
      "out (700.0s - 700.2s)\n",
      "onto (700.2s - 700.4s)\n",
      "the (700.4s - 700.5s)\n",
      "internet (700.5s - 700.8s)\n",
      "and (700.8s - 701.3s)\n",
      "collect (701.3s - 701.7s)\n",
      "a (701.7s - 701.8s)\n",
      "large (701.8s - 702.1s)\n",
      "dataset (702.1s - 702.5s)\n",
      "of (702.5s - 703.4s)\n",
      "many (703.4s - 703.7s)\n",
      "many (703.7s - 703.9s)\n",
      "cats (703.9s - 704.4s)\n",
      "and (704.4s - 704.5s)\n",
      "many (704.5s - 704.8s)\n",
      "many (704.8s - 705.0s)\n",
      "airplanes (705.0s - 705.5s)\n",
      "and (705.5s - 705.7s)\n",
      "many (705.7s - 705.9s)\n",
      "many (705.9s - 706.1s)\n",
      "deer (706.1s - 706.4s)\n",
      "and (706.4s - 707.0s)\n",
      "and (707.0s - 707.2s)\n",
      "different (707.2s - 707.4s)\n",
      "things (707.4s - 707.5s)\n",
      "like (707.5s - 707.7s)\n",
      "this (707.7s - 707.9s)\n",
      "and (707.9s - 708.5s)\n",
      "we (708.5s - 708.8s)\n",
      "can (708.8s - 709.0s)\n",
      "actually (709.0s - 709.0s)\n",
      "use (709.0s - 709.5s)\n",
      "tools (709.5s - 710.2s)\n",
      "like (710.2s - 710.4s)\n",
      "Google (710.4s - 710.6s)\n",
      "image (710.6s - 710.8s)\n",
      "search (710.8s - 711.0s)\n",
      "or (711.0s - 711.3s)\n",
      "something (711.3s - 711.6s)\n",
      "like (711.6s - 711.7s)\n",
      "that (711.7s - 711.8s)\n",
      "to (711.8s - 712.3s)\n",
      "go (712.3s - 712.4s)\n",
      "out (712.4s - 712.5s)\n",
      "and (712.5s - 712.7s)\n",
      "collect (712.7s - 712.9s)\n",
      "a (712.9s - 713.1s)\n",
      "very (713.1s - 713.3s)\n",
      "large (713.3s - 713.7s)\n",
      "number (713.7s - 713.7s)\n",
      "of (713.7s - 714.0s)\n",
      "examples (714.0s - 714.3s)\n",
      "of (714.3s - 714.7s)\n",
      "these (714.7s - 714.9s)\n",
      "different (714.9s - 715.1s)\n",
      "categories (715.1s - 715.7s)\n",
      "by (715.7s - 717.2s)\n",
      "the (717.2s - 717.3s)\n",
      "way (717.3s - 717.4s)\n",
      "this (717.4s - 717.6s)\n",
      "actually (717.6s - 717.9s)\n",
      "takes (717.9s - 718.1s)\n",
      "quite (718.1s - 718.3s)\n",
      "a (718.3s - 718.4s)\n",
      "lot (718.4s - 718.5s)\n",
      "of (718.5s - 718.6s)\n",
      "effort (718.6s - 719.0s)\n",
      "to (719.0s - 719.4s)\n",
      "go (719.4s - 719.6s)\n",
      "out (719.6s - 719.7s)\n",
      "and (719.7s - 719.9s)\n",
      "actually (719.9s - 720.0s)\n",
      "but (720.0s - 721.6s)\n",
      "people (721.6s - 722.0s)\n",
      "luckily (722.0s - 722.5s)\n",
      "there's (722.5s - 722.7s)\n",
      "a (722.7s - 722.7s)\n",
      "lot (722.7s - 722.9s)\n",
      "of (722.9s - 722.9s)\n",
      "really (722.9s - 723.3s)\n",
      "good (723.3s - 723.5s)\n",
      "high-quality (723.5s - 723.8s)\n",
      "data (723.8s - 724.2s)\n",
      "sets (724.2s - 724.6s)\n",
      "out (724.6s - 725.1s)\n",
      "there (725.1s - 725.3s)\n",
      "already (725.3s - 725.4s)\n",
      "for (725.4s - 725.8s)\n",
      "the (725.8s - 725.9s)\n",
      "use (725.9s - 726.2s)\n",
      "then (726.2s - 727.3s)\n",
      "once (727.3s - 727.6s)\n",
      "we (727.6s - 727.7s)\n",
      "get (727.7s - 728.0s)\n",
      "this (728.0s - 728.2s)\n",
      "data (728.2s - 728.5s)\n",
      "set (728.5s - 728.5s)\n",
      "we (728.5s - 729.2s)\n",
      "trained (729.2s - 729.4s)\n",
      "this (729.4s - 729.6s)\n",
      "machine (729.6s - 730.0s)\n",
      "learning (730.0s - 730.2s)\n",
      "classifier (730.2s - 730.8s)\n",
      "that (730.8s - 731.9s)\n",
      "is (731.9s - 732.1s)\n",
      "going (732.1s - 732.3s)\n",
      "to (732.3s - 732.4s)\n",
      "ingest (732.4s - 733.0s)\n",
      "all (733.0s - 733.2s)\n",
      "of (733.2s - 733.3s)\n",
      "the (733.3s - 733.4s)\n",
      "data (733.4s - 733.6s)\n",
      "summarized (733.6s - 734.4s)\n",
      "in (734.4s - 734.5s)\n",
      "some (734.5s - 734.8s)\n",
      "way (734.8s - 735.0s)\n",
      "and (735.0s - 735.5s)\n",
      "then (735.5s - 735.7s)\n",
      "spit (735.7s - 736.1s)\n",
      "out (736.1s - 736.2s)\n",
      "a (736.2s - 736.3s)\n",
      "model (736.3s - 736.7s)\n",
      "that (736.7s - 736.9s)\n",
      "summarizes (736.9s - 737.6s)\n",
      "the (737.6s - 738.0s)\n",
      "knowledge (738.0s - 738.6s)\n",
      "of (738.6s - 738.7s)\n",
      "how (738.7s - 738.8s)\n",
      "to (738.8s - 738.9s)\n",
      "recognize (738.9s - 739.4s)\n",
      "these (739.4s - 739.6s)\n",
      "different (739.6s - 740.0s)\n",
      "subject (740.0s - 740.3s)\n",
      "categories (740.3s - 740.8s)\n",
      "then (740.8s - 741.7s)\n",
      "finally (741.7s - 742.1s)\n",
      "will (742.1s - 742.2s)\n",
      "use (742.2s - 742.5s)\n",
      "this (742.5s - 742.6s)\n",
      "train (742.6s - 742.9s)\n",
      "model (742.9s - 743.2s)\n",
      "and (743.2s - 743.3s)\n",
      "apply (743.3s - 743.6s)\n",
      "it (743.6s - 743.8s)\n",
      "on (743.8s - 743.9s)\n",
      "new (743.9s - 744.1s)\n",
      "that (744.2s - 745.1s)\n",
      "will (745.1s - 745.2s)\n",
      "then (745.2s - 745.5s)\n",
      "be (745.5s - 746.2s)\n",
      "able (746.2s - 746.3s)\n",
      "to (746.3s - 746.4s)\n",
      "recognize (746.4s - 746.9s)\n",
      "cats (746.9s - 747.2s)\n",
      "and (747.2s - 747.3s)\n",
      "dogs (747.3s - 747.5s)\n",
      "and (747.5s - 747.6s)\n",
      "whatnot (747.6s - 747.9s)\n",
      "so (747.9s - 749.2s)\n",
      "here (749.2s - 749.5s)\n",
      "at (749.5s - 749.6s)\n",
      "our (749.6s - 749.8s)\n",
      "API (749.8s - 750.2s)\n",
      "has (750.2s - 750.4s)\n",
      "changed (750.4s - 750.7s)\n",
      "a (750.7s - 750.8s)\n",
      "little (750.8s - 750.9s)\n",
      "bit (750.9s - 751.0s)\n",
      "rather (751.0s - 751.7s)\n",
      "than (751.7s - 751.8s)\n",
      "a (751.8s - 752.0s)\n",
      "single (752.0s - 752.2s)\n",
      "function (752.2s - 752.6s)\n",
      "that (752.6s - 752.9s)\n",
      "just (752.9s - 753.1s)\n",
      "inputs (753.1s - 753.5s)\n",
      "an (753.5s - 753.6s)\n",
      "image (753.6s - 753.8s)\n",
      "and (753.8s - 753.9s)\n",
      "recognize (753.9s - 754.3s)\n",
      "the (754.3s - 754.4s)\n",
      "cat (754.4s - 754.8s)\n",
      "we (754.8s - 755.2s)\n",
      "have (755.2s - 755.3s)\n",
      "these (755.3s - 755.4s)\n",
      "two (755.4s - 755.7s)\n",
      "functions (755.7s - 756.2s)\n",
      "one (756.2s - 757.0s)\n",
      "is (757.0s - 757.1s)\n",
      "going (757.1s - 757.2s)\n",
      "to (757.2s - 757.3s)\n",
      "call (757.3s - 757.8s)\n",
      "train (757.8s - 758.3s)\n",
      "that's (758.3s - 758.7s)\n",
      "going (758.7s - 758.8s)\n",
      "to (758.8s - 758.9s)\n",
      "input (758.9s - 759.2s)\n",
      "images (759.2s - 759.5s)\n",
      "at (759.5s - 760.7s)\n",
      "labels (760.7s - 760.8s)\n",
      "and (760.8s - 761.2s)\n",
      "then (761.2s - 761.4s)\n",
      "I'll (761.4s - 761.5s)\n",
      "put (761.5s - 761.7s)\n",
      "a (761.7s - 761.7s)\n",
      "model (761.7s - 762.1s)\n",
      "and (762.1s - 762.6s)\n",
      "then (762.6s - 762.7s)\n",
      "separately (762.7s - 763.4s)\n",
      "another (763.4s - 763.8s)\n",
      "function (763.8s - 764.2s)\n",
      "called (764.2s - 764.4s)\n",
      "predict (764.4s - 764.9s)\n",
      "which (764.9s - 765.3s)\n",
      "will (765.3s - 765.4s)\n",
      "input (765.4s - 765.7s)\n",
      "the (765.7s - 765.8s)\n",
      "model (765.8s - 766.1s)\n",
      "and (766.1s - 766.3s)\n",
      "then (766.3s - 766.5s)\n",
      "they (766.5s - 766.7s)\n",
      "predictions (766.7s - 767.2s)\n",
      "for (767.2s - 767.4s)\n",
      "images (767.4s - 767.8s)\n",
      "and (767.8s - 768.6s)\n",
      "this (768.6s - 768.8s)\n",
      "is (768.8s - 769.0s)\n",
      "kind (769.0s - 769.2s)\n",
      "of (769.2s - 769.2s)\n",
      "a (769.2s - 769.3s)\n",
      "key (769.3s - 769.5s)\n",
      "Insight (769.5s - 769.9s)\n",
      "that (769.9s - 770.0s)\n",
      "allowed (770.0s - 770.4s)\n",
      "all (770.4s - 770.6s)\n",
      "these (770.6s - 770.7s)\n",
      "things (770.7s - 770.8s)\n",
      "to (770.8s - 771.0s)\n",
      "start (771.0s - 771.3s)\n",
      "working (771.3s - 771.6s)\n",
      "really (771.6s - 771.8s)\n",
      "well (771.8s - 772.0s)\n",
      "in (772.0s - 772.3s)\n",
      "the (772.3s - 772.3s)\n",
      "last (772.3s - 772.6s)\n",
      "over (772.6s - 772.9s)\n",
      "the (772.9s - 773.0s)\n",
      "last (773.0s - 773.2s)\n",
      "10 (773.2s - 773.5s)\n",
      "20 (773.5s - 773.8s)\n",
      "years (773.8s - 774.0s)\n",
      "or (774.0s - 774.2s)\n",
      "so (774.2s - 774.2s)\n",
      "so (777.7s - 778.1s)\n",
      "the (778.1s - 778.2s)\n",
      "first (778.2s - 778.4s)\n",
      "thing (778.4s - 778.6s)\n",
      "so (778.6s - 779.0s)\n",
      "this (779.0s - 779.6s)\n",
      "class (779.6s - 779.8s)\n",
      "is (779.8s - 780.0s)\n",
      "primarily (780.0s - 780.4s)\n",
      "about (780.4s - 780.5s)\n",
      "neural (780.5s - 780.9s)\n",
      "networks (780.9s - 780.9s)\n",
      "and (780.9s - 781.6s)\n",
      "convolutional (781.6s - 782.2s)\n",
      "neural (782.2s - 782.3s)\n",
      "networks (782.3s - 782.8s)\n",
      "and (782.8s - 782.9s)\n",
      "deep (782.9s - 783.2s)\n",
      "learning (783.2s - 783.3s)\n",
      "and (783.3s - 783.6s)\n",
      "all (783.6s - 783.7s)\n",
      "that (783.7s - 784.0s)\n",
      "but (784.0s - 784.4s)\n",
      "there's (784.4s - 784.9s)\n",
      "this (784.9s - 785.4s)\n",
      "idea (785.4s - 785.6s)\n",
      "of (785.6s - 785.9s)\n",
      "a (785.9s - 785.9s)\n",
      "data-driven (785.9s - 786.2s)\n",
      "approach (786.2s - 786.5s)\n",
      "is (786.5s - 786.9s)\n",
      "much (786.9s - 787.1s)\n",
      "more (787.1s - 787.3s)\n",
      "General (787.3s - 787.7s)\n",
      "than (787.7s - 787.7s)\n",
      "just (787.7s - 788.1s)\n",
      "deep (788.1s - 788.2s)\n",
      "learning (788.2s - 788.4s)\n",
      "and (788.4s - 789.1s)\n",
      "I (789.1s - 789.2s)\n",
      "think (789.2s - 789.5s)\n",
      "it's (789.5s - 789.8s)\n",
      "useful (789.8s - 790.2s)\n",
      "to (790.2s - 790.6s)\n",
      "sort (790.6s - 790.9s)\n",
      "of (790.9s - 791.0s)\n",
      "stepped (791.0s - 791.4s)\n",
      "through (791.4s - 791.5s)\n",
      "this (791.5s - 791.6s)\n",
      "process (791.6s - 791.9s)\n",
      "for (791.9s - 792.3s)\n",
      "a (792.3s - 792.4s)\n",
      "very (792.4s - 792.6s)\n",
      "simple (792.6s - 792.7s)\n",
      "classifier (792.7s - 793.5s)\n",
      "first (793.5s - 794.0s)\n",
      "before (794.0s - 794.1s)\n",
      "we (794.1s - 794.4s)\n",
      "get (794.4s - 794.5s)\n",
      "to (794.5s - 794.6s)\n",
      "these (794.6s - 794.7s)\n",
      "big (794.7s - 794.9s)\n",
      "complex (794.9s - 795.2s)\n",
      "ones (795.2s - 795.6s)\n",
      "so (795.6s - 796.4s)\n",
      "probably (796.4s - 797.7s)\n",
      "the (797.7s - 797.8s)\n",
      "simplest (797.8s - 798.4s)\n",
      "classify (798.4s - 799.1s)\n",
      "are (799.1s - 799.1s)\n",
      "you (799.1s - 799.3s)\n",
      "can (799.3s - 799.4s)\n",
      "imagine (799.4s - 799.6s)\n",
      "it's (799.6s - 800.2s)\n",
      "something (800.2s - 800.5s)\n",
      "we (800.5s - 800.6s)\n",
      "call (800.6s - 800.8s)\n",
      "nearest (800.8s - 801.1s)\n",
      "neighbor (801.1s - 801.4s)\n",
      "algorithm (801.4s - 802.3s)\n",
      "is (802.3s - 802.5s)\n",
      "is (802.5s - 802.6s)\n",
      "pretty (802.6s - 802.9s)\n",
      "pretty. (802.9s - 803.5s)\n",
      "I'm (803.5s - 803.6s)\n",
      "honestly (803.6s - 804.0s)\n",
      "so (804.0s - 804.7s)\n",
      "work (804.7s - 805.2s)\n",
      "during (805.2s - 805.4s)\n",
      "the (805.4s - 805.6s)\n",
      "training (805.6s - 805.8s)\n",
      "stuff (805.8s - 806.1s)\n",
      "we (806.1s - 806.5s)\n",
      "won't (806.5s - 806.6s)\n",
      "do (806.6s - 806.8s)\n",
      "anything (806.8s - 806.8s)\n",
      "we'll (806.8s - 807.5s)\n",
      "just (807.5s - 807.6s)\n",
      "memorize (807.6s - 808.2s)\n",
      "all (808.2s - 808.4s)\n",
      "of (808.4s - 808.5s)\n",
      "the (808.5s - 808.6s)\n",
      "training (808.6s - 808.8s)\n",
      "data (808.8s - 809.1s)\n",
      "so (809.1s - 810.1s)\n",
      "this (810.1s - 810.2s)\n",
      "is (810.2s - 810.4s)\n",
      "very (810.4s - 811.1s)\n",
      "simple (811.1s - 811.6s)\n",
      "and (811.6s - 812.2s)\n",
      "now (812.2s - 812.4s)\n",
      "it's (812.4s - 812.6s)\n",
      "during (812.6s - 812.7s)\n",
      "the (812.7s - 812.8s)\n",
      "prediction (812.8s - 813.2s)\n",
      "stuff (813.2s - 813.6s)\n",
      "we're (813.6s - 814.4s)\n",
      "going (814.4s - 814.5s)\n",
      "to (814.5s - 814.6s)\n",
      "take (814.6s - 814.8s)\n",
      "some (814.8s - 815.4s)\n",
      "new (815.4s - 815.6s)\n",
      "image (815.6s - 815.9s)\n",
      "and (815.9s - 816.4s)\n",
      "go (816.4s - 816.6s)\n",
      "and (816.6s - 816.9s)\n",
      "try (816.9s - 817.1s)\n",
      "to (817.1s - 817.1s)\n",
      "find (817.1s - 817.2s)\n",
      "the (817.2s - 817.7s)\n",
      "most (817.7s - 818.0s)\n",
      "similar (818.0s - 818.4s)\n",
      "image (818.4s - 818.8s)\n",
      "in (818.8s - 818.9s)\n",
      "the (818.9s - 818.9s)\n",
      "training (818.9s - 819.3s)\n",
      "data (819.3s - 819.5s)\n",
      "to (819.5s - 820.0s)\n",
      "that (820.0s - 820.1s)\n",
      "new (820.1s - 820.2s)\n",
      "image (820.2s - 820.6s)\n",
      "and (820.6s - 821.2s)\n",
      "now (821.2s - 821.3s)\n",
      "predict (821.3s - 821.8s)\n",
      "the (821.8s - 822.0s)\n",
      "label (822.0s - 822.4s)\n",
      "of (822.4s - 822.6s)\n",
      "that (822.6s - 822.8s)\n",
      "most (822.8s - 823.1s)\n",
      "similar (823.1s - 823.4s)\n",
      "image (823.4s - 823.7s)\n",
      "very (823.7s - 824.8s)\n",
      "simple (824.8s - 825.1s)\n",
      "algorithm (825.1s - 825.3s)\n",
      "but (825.3s - 826.4s)\n",
      "it (826.4s - 826.6s)\n",
      "sort (826.6s - 826.8s)\n",
      "of (826.8s - 826.8s)\n",
      "has (826.8s - 827.1s)\n",
      "a (827.1s - 827.2s)\n",
      "lot (827.2s - 827.3s)\n",
      "of (827.3s - 827.4s)\n",
      "these (827.4s - 827.5s)\n",
      "nice (827.5s - 827.8s)\n",
      "properties (827.8s - 828.2s)\n",
      "with (828.2s - 828.5s)\n",
      "respect (828.5s - 828.8s)\n",
      "to (828.8s - 828.8s)\n",
      "data (828.8s - 829.1s)\n",
      "driven (829.1s - 829.3s)\n",
      "this (829.3s - 829.6s)\n",
      "and (829.6s - 829.7s)\n",
      "and (829.7s - 830.0s)\n",
      "whatnot (830.0s - 830.3s)\n",
      "so (832.8s - 833.2s)\n",
      "to (833.2s - 833.4s)\n",
      "be (833.4s - 833.5s)\n",
      "a (833.5s - 833.6s)\n",
      "little (833.6s - 833.7s)\n",
      "bit (833.7s - 833.8s)\n",
      "more (833.8s - 833.9s)\n",
      "concrete (833.9s - 834.6s)\n",
      "you (834.6s - 834.9s)\n",
      "might (834.9s - 835.1s)\n",
      "imagine (835.1s - 835.2s)\n",
      "working (835.2s - 835.6s)\n",
      "on (835.6s - 835.9s)\n",
      "this (835.9s - 836.4s)\n",
      "date (836.4s - 836.6s)\n",
      "of (836.6s - 836.7s)\n",
      "set (836.7s - 836.9s)\n",
      "call (836.9s - 837.1s)\n",
      "c-par (837.1s - 837.4s)\n",
      "10 (837.4s - 837.7s)\n",
      "which (837.7s - 838.2s)\n",
      "is (838.2s - 838.4s)\n",
      "very (838.4s - 838.6s)\n",
      "commonly (838.6s - 839.0s)\n",
      "used (839.0s - 839.4s)\n",
      "in (839.4s - 839.5s)\n",
      "machine (839.5s - 839.8s)\n",
      "learning (839.8s - 839.9s)\n",
      "as (839.9s - 840.8s)\n",
      "kind (840.8s - 841.0s)\n",
      "of (841.0s - 841.0s)\n",
      "a (841.0s - 841.1s)\n",
      "small (841.1s - 841.4s)\n",
      "test (841.4s - 841.7s)\n",
      "case (841.7s - 842.0s)\n",
      "and (842.0s - 842.5s)\n",
      "you'll (842.5s - 842.6s)\n",
      "be (842.6s - 842.6s)\n",
      "working (842.6s - 842.9s)\n",
      "with (842.9s - 843.0s)\n",
      "the (843.0s - 843.1s)\n",
      "state (843.1s - 843.2s)\n",
      "of (843.2s - 843.4s)\n",
      "snot (843.4s - 843.6s)\n",
      "on (843.6s - 843.7s)\n",
      "your (843.7s - 843.8s)\n",
      "homework (843.8s - 844.2s)\n",
      "so (844.2s - 844.8s)\n",
      "the (844.8s - 845.1s)\n",
      "cr-10 (845.1s - 845.6s)\n",
      "dataset (845.6s - 846.2s)\n",
      "gives (846.2s - 846.7s)\n",
      "you (846.7s - 846.8s)\n",
      "ten (846.8s - 847.0s)\n",
      "different (847.0s - 847.1s)\n",
      "classes (847.1s - 847.8s)\n",
      "airplanes (847.8s - 848.7s)\n",
      "and (848.7s - 848.8s)\n",
      "Automobiles (848.8s - 848.9s)\n",
      "and (848.9s - 849.7s)\n",
      "birds (849.7s - 850.1s)\n",
      "and (850.1s - 850.2s)\n",
      "cats (850.2s - 850.7s)\n",
      "and (850.7s - 850.9s)\n",
      "different (850.9s - 851.2s)\n",
      "things (851.2s - 851.2s)\n",
      "like (851.2s - 851.5s)\n",
      "that (851.5s - 851.6s)\n",
      "and (851.6s - 853.1s)\n",
      "we (853.1s - 853.5s)\n",
      "for (853.5s - 853.9s)\n",
      "each (853.9s - 854.1s)\n",
      "of (854.1s - 854.2s)\n",
      "those (854.2s - 854.3s)\n",
      "10 (854.3s - 854.6s)\n",
      "categories (854.6s - 854.9s)\n",
      "it (854.9s - 855.4s)\n",
      "provides (855.4s - 855.9s)\n",
      "10000 (855.9s - 856.7s)\n",
      "sorry (856.7s - 858.0s)\n",
      "provides (858.0s - 858.5s)\n",
      "50,000 (858.5s - 859.4s)\n",
      "training (859.4s - 859.7s)\n",
      "images (859.7s - 860.1s)\n",
      "roughly (860.1s - 860.6s)\n",
      "evenly (860.6s - 861.1s)\n",
      "distributed (861.1s - 861.7s)\n",
      "across (861.7s - 862.0s)\n",
      "these (862.0s - 862.2s)\n",
      "10 (862.2s - 862.5s)\n",
      "categories (862.5s - 862.8s)\n",
      "and (862.8s - 863.6s)\n",
      "then (863.6s - 863.9s)\n",
      "ten (863.9s - 864.3s)\n",
      "thousand (864.3s - 864.8s)\n",
      "additional (864.8s - 864.9s)\n",
      "testing (864.9s - 865.6s)\n",
      "images (865.6s - 866.0s)\n",
      "that (866.0s - 866.9s)\n",
      "you (866.9s - 867.6s)\n",
      "are (867.6s - 867.8s)\n",
      "supposed (867.8s - 868.1s)\n",
      "to (868.1s - 868.2s)\n",
      "testerone (868.2s - 868.7s)\n",
      "so (870.4s - 870.8s)\n",
      "now (870.8s - 871.0s)\n",
      "if (871.0s - 871.2s)\n",
      "you (871.2s - 871.2s)\n",
      "think (871.2s - 871.5s)\n",
      "about (871.5s - 871.5s)\n",
      "so (871.5s - 872.1s)\n",
      "here's (872.1s - 872.3s)\n",
      "an (872.3s - 872.4s)\n",
      "example (872.4s - 872.5s)\n",
      "of (872.5s - 873.0s)\n",
      "applying (873.0s - 873.6s)\n",
      "this (873.6s - 873.9s)\n",
      "this (873.9s - 874.1s)\n",
      "simple (874.1s - 874.5s)\n",
      "nearest (874.5s - 874.8s)\n",
      "neighbor (874.8s - 875.1s)\n",
      "classifier (875.1s - 875.7s)\n",
      "to (875.7s - 875.9s)\n",
      "some (875.9s - 876.1s)\n",
      "of (876.1s - 876.3s)\n",
      "these (876.3s - 876.4s)\n",
      "tests (876.4s - 876.8s)\n",
      "images (876.8s - 876.9s)\n",
      "on (876.9s - 877.4s)\n",
      "on (877.4s - 877.6s)\n",
      "cr-10 (877.6s - 878.3s)\n",
      "so (878.3s - 879.1s)\n",
      "on (879.1s - 879.4s)\n",
      "this (879.4s - 879.6s)\n",
      "on (879.6s - 879.9s)\n",
      "this (879.9s - 879.9s)\n",
      "grid (879.9s - 880.3s)\n",
      "on (880.3s - 880.4s)\n",
      "the (880.4s - 880.5s)\n",
      "right (880.5s - 880.8s)\n",
      "each (880.8s - 881.3s)\n",
      "for (881.3s - 882.1s)\n",
      "the (882.1s - 882.2s)\n",
      "loss (882.2s - 882.6s)\n",
      "column (882.6s - 882.9s)\n",
      "gives (882.9s - 884.2s)\n",
      "a (884.2s - 884.3s)\n",
      "test (884.3s - 884.7s)\n",
      "image (884.7s - 884.8s)\n",
      "in (884.8s - 885.2s)\n",
      "the (885.2s - 885.2s)\n",
      "cr-10 (885.2s - 885.8s)\n",
      "data (885.8s - 886.1s)\n",
      "set (886.1s - 886.3s)\n",
      "and (886.3s - 886.9s)\n",
      "now (886.9s - 887.0s)\n",
      "on (887.0s - 887.3s)\n",
      "the (887.3s - 887.6s)\n",
      "right (887.6s - 888.0s)\n",
      "we (888.0s - 888.4s)\n",
      "see (888.4s - 888.7s)\n",
      "the (888.7s - 889.6s)\n",
      "we (889.6s - 890.1s)\n",
      "sorted (890.1s - 890.5s)\n",
      "the (890.5s - 890.6s)\n",
      "training (890.6s - 891.0s)\n",
      "images (891.0s - 891.4s)\n",
      "and (891.4s - 891.7s)\n",
      "get (891.7s - 891.9s)\n",
      "and (891.9s - 892.2s)\n",
      "show (892.2s - 892.3s)\n",
      "the (892.3s - 892.5s)\n",
      "most (892.5s - 892.9s)\n",
      "similar (892.9s - 893.4s)\n",
      "training (893.4s - 893.8s)\n",
      "images (893.8s - 894.4s)\n",
      "each (894.4s - 894.7s)\n",
      "of (894.7s - 894.8s)\n",
      "these (894.8s - 894.9s)\n",
      "test (894.9s - 895.3s)\n",
      "examples (895.3s - 895.9s)\n",
      "and (895.9s - 896.7s)\n",
      "you (896.7s - 896.9s)\n",
      "can (896.9s - 897.0s)\n",
      "see (897.0s - 897.2s)\n",
      "that (897.2s - 897.2s)\n",
      "they (897.2s - 897.5s)\n",
      "look (897.5s - 897.7s)\n",
      "kind (897.7s - 897.9s)\n",
      "of (897.9s - 898.0s)\n",
      "visually (898.0s - 898.4s)\n",
      "similar (898.4s - 898.6s)\n",
      "to (898.6s - 899.1s)\n",
      "the (899.1s - 899.2s)\n",
      "to (899.2s - 899.4s)\n",
      "the (899.4s - 899.5s)\n",
      "training (899.5s - 899.8s)\n",
      "images (899.8s - 900.2s)\n",
      "although (900.2s - 901.5s)\n",
      "they (901.5s - 901.6s)\n",
      "are (901.6s - 901.9s)\n",
      "not (901.9s - 902.1s)\n",
      "always (902.1s - 902.4s)\n",
      "correct (902.4s - 902.7s)\n",
      "right (902.7s - 904.0s)\n",
      "so (904.0s - 904.3s)\n",
      "maybe (904.3s - 904.6s)\n",
      "on (904.6s - 904.7s)\n",
      "the (904.7s - 904.9s)\n",
      "second (904.9s - 905.3s)\n",
      "row (905.3s - 905.4s)\n",
      "we (905.4s - 905.8s)\n",
      "see (905.8s - 905.9s)\n",
      "that (905.9s - 906.0s)\n",
      "the (906.0s - 906.1s)\n",
      "test (906.1s - 906.4s)\n",
      "this (906.4s - 906.9s)\n",
      "is (906.9s - 907.1s)\n",
      "kind (907.1s - 907.3s)\n",
      "of (907.3s - 907.3s)\n",
      "hard (907.3s - 907.4s)\n",
      "to (907.4s - 907.5s)\n",
      "see (907.5s - 907.7s)\n",
      "cuz (907.7s - 907.9s)\n",
      "these (907.9s - 908.0s)\n",
      "images (908.0s - 908.4s)\n",
      "are (908.4s - 908.7s)\n",
      "32 (908.7s - 909.5s)\n",
      "by (909.5s - 909.6s)\n",
      "32 (909.6s - 909.9s)\n",
      "pixels (909.9s - 910.5s)\n",
      "you (910.5s - 910.6s)\n",
      "need (910.6s - 910.7s)\n",
      "to (910.7s - 910.8s)\n",
      "really (910.8s - 911.0s)\n",
      "dive (911.0s - 911.6s)\n",
      "in (911.6s - 911.7s)\n",
      "there (911.7s - 911.9s)\n",
      "and (911.9s - 912.0s)\n",
      "then (912.0s - 912.2s)\n",
      "try (912.2s - 912.4s)\n",
      "to (912.4s - 912.4s)\n",
      "make (912.4s - 912.5s)\n",
      "your (912.5s - 912.7s)\n",
      "best (912.7s - 912.9s)\n",
      "guess (912.9s - 913.2s)\n",
      "but (913.2s - 914.5s)\n",
      "this (914.5s - 914.8s)\n",
      "image (914.8s - 915.1s)\n",
      "is (915.1s - 915.2s)\n",
      "a (915.2s - 915.3s)\n",
      "dog (915.3s - 915.6s)\n",
      "and (915.6s - 916.0s)\n",
      "its (916.0s - 916.2s)\n",
      "nearest (916.2s - 916.5s)\n",
      "neighbor (916.5s - 916.8s)\n",
      "is (916.8s - 916.8s)\n",
      "also (916.8s - 917.2s)\n",
      "a (917.2s - 917.2s)\n",
      "dog (917.2s - 917.6s)\n",
      "but (917.6s - 918.8s)\n",
      "this (918.8s - 919.0s)\n",
      "next (919.0s - 919.2s)\n",
      "one (919.2s - 919.6s)\n",
      "is (919.6s - 919.7s)\n",
      "I (919.7s - 919.9s)\n",
      "think (919.9s - 920.1s)\n",
      "it's (920.1s - 920.4s)\n",
      "actually (920.4s - 920.5s)\n",
      "a (920.5s - 920.9s)\n",
      "deer (920.9s - 921.2s)\n",
      "or (921.2s - 921.3s)\n",
      "a (921.3s - 921.4s)\n",
      "horse (921.4s - 921.8s)\n",
      "or (921.8s - 921.9s)\n",
      "something (921.9s - 922.2s)\n",
      "else (922.2s - 922.5s)\n",
      "but (922.5s - 923.3s)\n",
      "you (923.3s - 923.4s)\n",
      "can (923.4s - 923.5s)\n",
      "see (923.5s - 923.7s)\n",
      "that (923.7s - 923.8s)\n",
      "it (923.8s - 923.9s)\n",
      "looks (923.9s - 924.0s)\n",
      "quite (924.0s - 924.3s)\n",
      "visually (924.3s - 924.8s)\n",
      "similar (924.8s - 924.9s)\n",
      "because (924.9s - 926.4s)\n",
      "there's (926.4s - 926.8s)\n",
      "kind (926.8s - 927.0s)\n",
      "of (927.0s - 927.0s)\n",
      "a (927.0s - 927.0s)\n",
      "white (927.0s - 927.2s)\n",
      "blob (927.2s - 927.6s)\n",
      "in (927.6s - 927.7s)\n",
      "the (927.7s - 927.7s)\n",
      "middle (927.7s - 928.0s)\n",
      "and (928.0s - 928.1s)\n",
      "then (928.1s - 928.3s)\n",
      "what (928.3s - 928.5s)\n",
      "not (928.5s - 928.6s)\n",
      "so (929.3s - 929.7s)\n",
      "if (929.7s - 930.3s)\n",
      "we're (930.3s - 930.4s)\n",
      "applying (930.4s - 930.7s)\n",
      "the (930.7s - 930.9s)\n",
      "nearest (930.9s - 931.2s)\n",
      "neighbor (931.2s - 931.5s)\n",
      "algorithm (931.5s - 931.7s)\n",
      "to (931.7s - 932.0s)\n",
      "the (932.0s - 932.1s)\n",
      "centage (932.1s - 932.5s)\n",
      "will (932.5s - 932.9s)\n",
      "find (932.9s - 933.4s)\n",
      "the (933.4s - 933.6s)\n",
      "the (933.6s - 933.8s)\n",
      "closest (933.8s - 934.2s)\n",
      "example (934.2s - 934.6s)\n",
      "in (934.6s - 934.7s)\n",
      "the (934.7s - 934.8s)\n",
      "training (934.8s - 935.1s)\n",
      "set (935.1s - 935.4s)\n",
      "and (935.4s - 935.9s)\n",
      "now (935.9s - 936.0s)\n",
      "the (936.0s - 936.3s)\n",
      "closest (936.3s - 936.7s)\n",
      "example (936.7s - 937.1s)\n",
      "we (937.1s - 937.3s)\n",
      "know (937.3s - 937.5s)\n",
      "it's (937.5s - 937.7s)\n",
      "label (937.7s - 938.2s)\n",
      "because (938.2s - 938.5s)\n",
      "it (938.5s - 938.6s)\n",
      "comes (938.6s - 939.0s)\n",
      "from (939.0s - 939.0s)\n",
      "the (939.0s - 939.1s)\n",
      "training (939.1s - 939.4s)\n",
      "set (939.4s - 939.8s)\n",
      "and (939.8s - 940.3s)\n",
      "now (940.3s - 940.4s)\n",
      "it (940.4s - 940.6s)\n",
      "will (940.6s - 940.6s)\n",
      "simply (940.6s - 941.0s)\n",
      "say (941.0s - 941.3s)\n",
      "that (941.3s - 941.5s)\n",
      "this (941.5s - 941.6s)\n",
      "testing (941.6s - 942.1s)\n",
      "image (942.1s - 942.3s)\n",
      "is (942.3s - 942.5s)\n",
      "also (942.5s - 943.0s)\n",
      "a (943.0s - 943.0s)\n",
      "dog (943.0s - 943.4s)\n",
      "you (943.4s - 944.3s)\n",
      "can (944.3s - 944.5s)\n",
      "see (944.5s - 944.8s)\n",
      "kind (944.8s - 945.2s)\n",
      "of (945.2s - 945.3s)\n",
      "from (945.3s - 945.6s)\n",
      "these (945.6s - 945.8s)\n",
      "examples (945.8s - 946.2s)\n",
      "that (946.2s - 946.4s)\n",
      "this (946.4s - 946.5s)\n",
      "is (946.5s - 946.6s)\n",
      "probably (946.6s - 946.8s)\n",
      "not (946.8s - 947.0s)\n",
      "going (947.0s - 947.3s)\n",
      "to (947.3s - 947.4s)\n",
      "work (947.4s - 947.5s)\n",
      "very (947.5s - 947.8s)\n",
      "well (947.8s - 947.9s)\n",
      "but (947.9s - 949.1s)\n",
      "it's (949.1s - 949.3s)\n",
      "still (949.3s - 949.5s)\n",
      "kind (949.5s - 949.6s)\n",
      "of (949.6s - 949.8s)\n",
      "a (949.8s - 949.9s)\n",
      "nice (949.9s - 950.1s)\n",
      "nice (950.1s - 950.3s)\n",
      "example (950.3s - 950.6s)\n",
      "to (950.6s - 951.0s)\n",
      "work (951.0s - 951.2s)\n",
      "through (951.2s - 951.4s)\n",
      "but (953.7s - 954.1s)\n",
      "then (954.1s - 954.4s)\n",
      "one (954.4s - 954.7s)\n",
      "one (954.7s - 955.0s)\n",
      "detail (955.0s - 955.4s)\n",
      "that (955.4s - 955.5s)\n",
      "we (955.5s - 955.7s)\n",
      "need (955.7s - 955.9s)\n",
      "to (955.9s - 955.9s)\n",
      "know (955.9s - 956.1s)\n",
      "is (956.1s - 956.5s)\n",
      "given (956.5s - 957.1s)\n",
      "a (957.1s - 957.1s)\n",
      "pair (957.1s - 957.3s)\n",
      "of (957.3s - 957.4s)\n",
      "images (957.4s - 957.8s)\n",
      "how (957.8s - 958.2s)\n",
      "can (958.2s - 958.4s)\n",
      "we (958.4s - 958.5s)\n",
      "actually (958.5s - 958.6s)\n",
      "compare (958.6s - 959.1s)\n",
      "them (959.1s - 959.4s)\n",
      "because (959.4s - 960.3s)\n",
      "we (960.3s - 960.4s)\n",
      "if (960.4s - 960.6s)\n",
      "we're (960.6s - 960.7s)\n",
      "going (960.7s - 960.9s)\n",
      "to (960.9s - 960.9s)\n",
      "take (960.9s - 961.1s)\n",
      "our (961.1s - 961.2s)\n",
      "test (961.2s - 961.5s)\n",
      "image (961.5s - 961.8s)\n",
      "and (961.8s - 961.9s)\n",
      "compare (961.9s - 962.3s)\n",
      "this (962.3s - 962.4s)\n",
      "all (962.4s - 962.6s)\n",
      "the (962.6s - 962.7s)\n",
      "training (962.7s - 963.0s)\n",
      "images (963.0s - 963.3s)\n",
      "where (963.3s - 963.8s)\n",
      "she (963.8s - 964.0s)\n",
      "have (964.0s - 964.2s)\n",
      "many (964.2s - 964.4s)\n",
      "different (964.4s - 964.5s)\n",
      "choices (964.5s - 965.0s)\n",
      "for (965.0s - 965.3s)\n",
      "exactly (965.3s - 965.8s)\n",
      "what (965.8s - 966.0s)\n",
      "that (966.0s - 966.0s)\n",
      "comparison (966.0s - 966.6s)\n",
      "function (966.6s - 967.2s)\n",
      "should (967.2s - 967.4s)\n",
      "look (967.4s - 967.6s)\n",
      "like (967.6s - 967.7s)\n",
      "so (967.7s - 968.9s)\n",
      "any (968.9s - 969.1s)\n",
      "example (969.1s - 969.5s)\n",
      "in (969.5s - 969.7s)\n",
      "the (969.7s - 969.8s)\n",
      "previous (969.8s - 970.1s)\n",
      "slide (970.1s - 970.4s)\n",
      "we've (970.4s - 971.0s)\n",
      "used (971.0s - 971.2s)\n",
      "was (971.2s - 971.4s)\n",
      "called (971.4s - 971.5s)\n",
      "the (971.5s - 971.7s)\n",
      "L1 (971.7s - 972.1s)\n",
      "distance (972.1s - 972.5s)\n",
      "is (972.5s - 973.3s)\n",
      "also (973.3s - 973.6s)\n",
      "sometimes (973.6s - 973.8s)\n",
      "called (973.8s - 974.0s)\n",
      "the (974.0s - 974.2s)\n",
      "Manhattan (974.2s - 974.7s)\n",
      "distance (974.7s - 975.1s)\n",
      "so (975.1s - 975.7s)\n",
      "this (975.7s - 975.9s)\n",
      "is (975.9s - 976.0s)\n",
      "a (976.0s - 976.1s)\n",
      "really (976.1s - 976.3s)\n",
      "really (976.3s - 976.8s)\n",
      "sweet (976.8s - 976.9s)\n",
      "of (976.9s - 977.0s)\n",
      "simple (977.0s - 977.5s)\n",
      "UTI (977.5s - 977.9s)\n",
      "idea (977.9s - 978.9s)\n",
      "for (978.9s - 979.1s)\n",
      "comparing (979.1s - 979.5s)\n",
      "images (979.5s - 979.9s)\n",
      "and (979.9s - 980.6s)\n",
      "that's (980.6s - 980.8s)\n",
      "that (980.8s - 981.1s)\n",
      "we're (981.1s - 982.0s)\n",
      "going (982.0s - 982.0s)\n",
      "to (982.0s - 982.1s)\n",
      "take (982.1s - 982.3s)\n",
      "the (982.3s - 982.6s)\n",
      "just (982.6s - 983.2s)\n",
      "compare (983.2s - 983.7s)\n",
      "individual (983.7s - 984.4s)\n",
      "pixels (984.4s - 984.7s)\n",
      "enemies (984.7s - 985.0s)\n",
      "images (985.0s - 985.2s)\n",
      "so (985.2s - 986.2s)\n",
      "supposing (986.2s - 987.1s)\n",
      "that (987.1s - 987.2s)\n",
      "are (987.2s - 987.6s)\n",
      "test (987.6s - 987.9s)\n",
      "images (987.9s - 988.3s)\n",
      "maybe (988.3s - 988.7s)\n",
      "just (988.7s - 989.0s)\n",
      "a (989.0s - 989.0s)\n",
      "tiny (989.0s - 989.5s)\n",
      "4 (989.5s - 989.9s)\n",
      "by (989.9s - 990.0s)\n",
      "4 (990.0s - 990.3s)\n",
      "image (990.3s - 990.6s)\n",
      "of (990.6s - 990.9s)\n",
      "a (990.9s - 991.0s)\n",
      "pixel (991.0s - 991.4s)\n",
      "values (991.4s - 991.8s)\n",
      "then (991.8s - 992.5s)\n",
      "we're (992.5s - 992.6s)\n",
      "going (992.6s - 992.7s)\n",
      "to (992.7s - 992.8s)\n",
      "take (992.8s - 993.2s)\n",
      "this (993.2s - 993.7s)\n",
      "upper (993.7s - 994.4s)\n",
      "left-hand (994.4s - 994.6s)\n",
      "pixel (994.6s - 995.1s)\n",
      "of (995.1s - 995.2s)\n",
      "a (995.2s - 995.3s)\n",
      "test (995.3s - 995.6s)\n",
      "image (995.6s - 995.7s)\n",
      "subtract (995.7s - 996.7s)\n",
      "off (996.7s - 996.9s)\n",
      "the (996.9s - 997.0s)\n",
      "value (997.0s - 997.3s)\n",
      "in (997.3s - 997.4s)\n",
      "the (997.4s - 997.4s)\n",
      "training (997.4s - 997.7s)\n",
      "image (997.7s - 998.0s)\n",
      "take (998.0s - 998.3s)\n",
      "the (998.3s - 998.4s)\n",
      "absolute (998.4s - 998.8s)\n",
      "value (998.8s - 998.8s)\n",
      "and (998.8s - 999.6s)\n",
      "get (999.6s - 999.7s)\n",
      "the (999.7s - 999.8s)\n",
      "difference (999.8s - 1000.2s)\n",
      "in (1000.2s - 1000.4s)\n",
      "that (1000.4s - 1000.5s)\n",
      "pixel (1000.5s - 1000.8s)\n",
      "between (1000.8s - 1001.2s)\n",
      "the (1001.2s - 1001.2s)\n",
      "two (1001.2s - 1001.3s)\n",
      "images (1001.3s - 1001.6s)\n",
      "and (1001.6s - 1002.2s)\n",
      "then (1002.2s - 1002.4s)\n",
      "some (1002.4s - 1002.6s)\n",
      "of (1002.6s - 1002.7s)\n",
      "these (1002.7s - 1002.9s)\n",
      "off (1002.9s - 1003.1s)\n",
      "across (1003.1s - 1003.4s)\n",
      "all (1003.4s - 1003.6s)\n",
      "the (1003.6s - 1003.7s)\n",
      "pixels (1003.7s - 1004.0s)\n",
      "in (1004.0s - 1004.1s)\n",
      "the (1004.1s - 1004.2s)\n",
      "image (1004.2s - 1004.5s)\n",
      "so (1004.5s - 1005.1s)\n",
      "this (1005.1s - 1005.3s)\n",
      "is (1005.3s - 1005.4s)\n",
      "kind (1005.4s - 1005.5s)\n",
      "of (1005.5s - 1005.6s)\n",
      "a (1005.6s - 1005.7s)\n",
      "stupid (1005.7s - 1005.9s)\n",
      "way (1005.9s - 1006.3s)\n",
      "to (1006.3s - 1006.4s)\n",
      "compare (1006.4s - 1006.8s)\n",
      "images (1006.8s - 1007.0s)\n",
      "but (1007.0s - 1007.6s)\n",
      "it (1007.6s - 1008.4s)\n",
      "does (1008.4s - 1008.6s)\n",
      "some (1008.6s - 1009.1s)\n",
      "reasonable (1009.1s - 1009.7s)\n",
      "things (1009.7s - 1009.7s)\n",
      "sometimes (1009.7s - 1010.6s)\n",
      "but (1010.6s - 1011.1s)\n",
      "this (1011.1s - 1011.3s)\n",
      "gives (1011.3s - 1011.5s)\n",
      "us (1011.5s - 1011.6s)\n",
      "of (1011.6s - 1011.8s)\n",
      "a (1011.8s - 1011.9s)\n",
      "concrete (1011.9s - 1012.3s)\n",
      "way (1012.3s - 1012.6s)\n",
      "to (1012.6s - 1012.8s)\n",
      "measure (1012.8s - 1013.0s)\n",
      "the (1013.0s - 1013.3s)\n",
      "train (1013.7s - 1014.0s)\n",
      "to (1014.0s - 1014.2s)\n",
      "images (1014.2s - 1014.6s)\n",
      "and (1014.6s - 1015.3s)\n",
      "in (1015.3s - 1015.4s)\n",
      "this (1015.4s - 1015.4s)\n",
      "case (1015.4s - 1015.7s)\n",
      "we (1015.7s - 1016.1s)\n",
      "have (1016.1s - 1016.2s)\n",
      "this (1016.2s - 1016.3s)\n",
      "this (1016.3s - 1016.5s)\n",
      "this (1016.5s - 1016.7s)\n",
      "difference (1016.7s - 1017.2s)\n",
      "of (1017.2s - 1017.3s)\n",
      "456 (1017.3s - 1018.1s)\n",
      "between (1018.1s - 1018.3s)\n",
      "these (1018.3s - 1018.5s)\n",
      "two (1018.5s - 1018.7s)\n",
      "images (1018.7s - 1018.9s)\n",
      "so (1021.2s - 1021.7s)\n",
      "here's (1021.7s - 1022.2s)\n",
      "some (1022.2s - 1022.5s)\n",
      "full (1022.5s - 1023.0s)\n",
      "python (1023.0s - 1023.5s)\n",
      "code (1023.5s - 1024.0s)\n",
      "for (1024.0s - 1024.2s)\n",
      "implementing (1024.2s - 1024.6s)\n",
      "this (1024.6s - 1024.9s)\n",
      "nearest (1024.9s - 1025.2s)\n",
      "a (1025.2s - 1025.4s)\n",
      "classifier (1025.4s - 1026.0s)\n",
      "and (1026.0s - 1026.5s)\n",
      "you (1026.5s - 1026.6s)\n",
      "can (1026.6s - 1026.7s)\n",
      "see (1026.7s - 1026.9s)\n",
      "it's (1026.9s - 1027.1s)\n",
      "actually (1027.1s - 1027.4s)\n",
      "pretty (1027.4s - 1027.7s)\n",
      "pretty (1027.7s - 1028.2s)\n",
      "short (1028.2s - 1028.4s)\n",
      "and (1028.4s - 1028.6s)\n",
      "pretty (1028.6s - 1028.7s)\n",
      "concise (1028.7s - 1029.2s)\n",
      "because (1029.2s - 1029.7s)\n",
      "we (1029.7s - 1030.1s)\n",
      "made (1030.1s - 1030.3s)\n",
      "use (1030.3s - 1030.5s)\n",
      "of (1030.5s - 1030.7s)\n",
      "many (1030.7s - 1030.9s)\n",
      "of (1030.9s - 1031.2s)\n",
      "these (1031.2s - 1031.3s)\n",
      "of (1031.3s - 1031.6s)\n",
      "exercise (1031.6s - 1032.0s)\n",
      "operations (1032.0s - 1032.6s)\n",
      "offered (1032.6s - 1033.0s)\n",
      "by (1033.0s - 1033.2s)\n",
      "so (1033.2s - 1034.6s)\n",
      "here (1034.6s - 1034.9s)\n",
      "we (1034.9s - 1035.0s)\n",
      "can (1035.0s - 1035.1s)\n",
      "see (1035.1s - 1035.3s)\n",
      "that (1035.3s - 1035.7s)\n",
      "the (1035.7s - 1035.9s)\n",
      "training (1035.9s - 1036.4s)\n",
      "date (1036.4s - 1036.6s)\n",
      "the (1036.6s - 1036.7s)\n",
      "vistas (1036.7s - 1037.4s)\n",
      "train (1037.4s - 1037.7s)\n",
      "function (1037.7s - 1038.2s)\n",
      "that (1038.2s - 1038.2s)\n",
      "we (1038.2s - 1038.4s)\n",
      "talked (1038.4s - 1038.6s)\n",
      "about (1038.6s - 1038.6s)\n",
      "earlier (1038.6s - 1038.9s)\n",
      "is (1038.9s - 1039.6s)\n",
      "again (1039.6s - 1040.0s)\n",
      "very (1040.0s - 1040.2s)\n",
      "simple (1040.2s - 1040.7s)\n",
      "and (1040.7s - 1040.7s)\n",
      "the (1040.7s - 1040.9s)\n",
      "case (1040.9s - 1041.1s)\n",
      "of (1041.1s - 1041.2s)\n",
      "nearest (1041.2s - 1041.5s)\n",
      "neighbor (1041.5s - 1041.8s)\n",
      "you (1041.8s - 1042.1s)\n",
      "just (1042.1s - 1042.4s)\n",
      "memorize (1042.4s - 1042.9s)\n",
      "the (1042.9s - 1043.0s)\n",
      "training (1043.0s - 1043.3s)\n",
      "day. (1043.3s - 1043.6s)\n",
      "There's (1043.6s - 1043.8s)\n",
      "not (1043.8s - 1044.0s)\n",
      "really (1044.0s - 1044.0s)\n",
      "much (1044.0s - 1044.2s)\n",
      "to (1044.2s - 1044.5s)\n",
      "do (1044.5s - 1044.6s)\n",
      "here (1044.6s - 1044.7s)\n",
      "and (1046.2s - 1046.6s)\n",
      "now (1046.6s - 1046.7s)\n",
      "it's (1046.7s - 1046.9s)\n",
      "just (1046.9s - 1047.0s)\n",
      "time (1047.0s - 1047.4s)\n",
      "we're (1047.4s - 1047.6s)\n",
      "going (1047.6s - 1047.7s)\n",
      "to (1047.7s - 1047.7s)\n",
      "take (1047.7s - 1048.0s)\n",
      "in (1048.0s - 1048.3s)\n",
      "our (1048.3s - 1048.5s)\n",
      "image (1048.5s - 1048.7s)\n",
      "and (1048.7s - 1049.1s)\n",
      "then (1049.1s - 1049.3s)\n",
      "go (1049.3s - 1049.4s)\n",
      "in (1049.4s - 1049.6s)\n",
      "and (1049.6s - 1049.8s)\n",
      "compare (1049.8s - 1050.4s)\n",
      "using (1050.4s - 1050.9s)\n",
      "the (1050.9s - 1051.0s)\n",
      "cell (1051.0s - 1051.2s)\n",
      "one (1051.2s - 1051.4s)\n",
      "distance (1051.4s - 1051.8s)\n",
      "function (1051.8s - 1052.2s)\n",
      "our (1052.2s - 1052.4s)\n",
      "test (1052.4s - 1052.8s)\n",
      "image (1052.8s - 1053.1s)\n",
      "to (1053.1s - 1053.5s)\n",
      "each (1053.5s - 1053.7s)\n",
      "of (1053.7s - 1053.8s)\n",
      "these (1053.8s - 1053.9s)\n",
      "training (1053.9s - 1054.3s)\n",
      "examples (1054.3s - 1054.6s)\n",
      "and (1054.6s - 1055.3s)\n",
      "find (1055.3s - 1055.7s)\n",
      "the (1055.7s - 1055.8s)\n",
      "most (1055.8s - 1056.1s)\n",
      "similar (1056.1s - 1056.5s)\n",
      "example (1056.5s - 1056.9s)\n",
      "in (1056.9s - 1057.0s)\n",
      "the (1057.0s - 1057.0s)\n",
      "training (1057.0s - 1057.3s)\n",
      "set (1057.3s - 1057.6s)\n",
      "and (1057.6s - 1058.6s)\n",
      "you (1058.6s - 1058.7s)\n",
      "can (1058.7s - 1058.9s)\n",
      "see (1058.9s - 1059.1s)\n",
      "that (1059.1s - 1059.2s)\n",
      "we're (1059.2s - 1059.7s)\n",
      "actually (1059.7s - 1059.9s)\n",
      "able (1059.9s - 1060.1s)\n",
      "to (1060.1s - 1060.2s)\n",
      "do (1060.2s - 1060.4s)\n",
      "this (1060.4s - 1060.6s)\n",
      "in (1060.6s - 1060.7s)\n",
      "just (1060.7s - 1061.0s)\n",
      "one (1061.0s - 1061.6s)\n",
      "or (1061.6s - 1061.7s)\n",
      "two (1061.7s - 1061.8s)\n",
      "lines (1061.8s - 1062.2s)\n",
      "of (1062.2s - 1062.4s)\n",
      "python (1062.4s - 1062.8s)\n",
      "code (1062.8s - 1063.2s)\n",
      "using (1063.2s - 1063.6s)\n",
      "by (1063.6s - 1063.9s)\n",
      "utilizing (1063.9s - 1064.5s)\n",
      "these (1064.5s - 1064.5s)\n",
      "vectorize (1064.5s - 1065.0s)\n",
      "operations (1065.0s - 1065.6s)\n",
      "in (1065.6s - 1065.7s)\n",
      "numpy (1065.7s - 1066.0s)\n",
      "so (1066.0s - 1067.2s)\n",
      "this (1067.2s - 1067.5s)\n",
      "is (1067.5s - 1067.6s)\n",
      "something (1067.6s - 1067.8s)\n",
      "that (1067.8s - 1067.9s)\n",
      "you'll (1067.9s - 1068.1s)\n",
      "get (1068.1s - 1068.3s)\n",
      "practice (1068.3s - 1068.7s)\n",
      "with (1068.7s - 1068.9s)\n",
      "on (1068.9s - 1069.2s)\n",
      "the (1069.2s - 1069.3s)\n",
      "first (1069.3s - 1069.5s)\n",
      "assignment (1069.5s - 1069.7s)\n",
      "so (1070.9s - 1071.9s)\n",
      "now (1071.9s - 1072.0s)\n",
      "I (1072.0s - 1072.2s)\n",
      "have (1072.2s - 1072.3s)\n",
      "a (1072.3s - 1072.5s)\n",
      "couple (1072.5s - 1072.7s)\n",
      "questions (1072.7s - 1072.8s)\n",
      "about (1072.8s - 1073.3s)\n",
      "the (1073.3s - 1073.5s)\n",
      "simple (1073.5s - 1073.8s)\n",
      "classifier (1073.8s - 1074.3s)\n",
      "first (1074.3s - 1075.7s)\n",
      "if (1075.7s - 1075.7s)\n",
      "we (1075.7s - 1075.9s)\n",
      "have (1075.9s - 1076.1s)\n",
      "an (1076.1s - 1076.4s)\n",
      "examples (1076.4s - 1077.0s)\n",
      "in (1077.0s - 1077.1s)\n",
      "our (1077.1s - 1077.2s)\n",
      "training (1077.2s - 1077.3s)\n",
      "set (1077.3s - 1077.7s)\n",
      "then (1077.7s - 1078.2s)\n",
      "how (1078.2s - 1078.4s)\n",
      "fast (1078.4s - 1078.7s)\n",
      "can (1078.7s - 1079.0s)\n",
      "we (1079.0s - 1079.1s)\n",
      "expect (1079.1s - 1079.5s)\n",
      "training (1079.5s - 1079.9s)\n",
      "and (1079.9s - 1080.0s)\n",
      "testing (1080.0s - 1080.2s)\n",
      "to (1080.2s - 1080.4s)\n",
      "be (1080.4s - 1080.5s)\n",
      "well (1085.0s - 1085.4s)\n",
      "training (1085.4s - 1085.7s)\n",
      "is (1085.7s - 1085.9s)\n",
      "probably (1085.9s - 1086.2s)\n",
      "constant (1086.2s - 1087.0s)\n",
      "because (1087.0s - 1087.4s)\n",
      "we (1087.4s - 1087.6s)\n",
      "don't (1087.6s - 1087.7s)\n",
      "really (1087.7s - 1087.8s)\n",
      "need (1087.8s - 1088.0s)\n",
      "to (1088.0s - 1088.1s)\n",
      "do (1088.1s - 1088.3s)\n",
      "anything (1088.3s - 1088.6s)\n",
      "we (1088.6s - 1088.9s)\n",
      "just (1088.9s - 1088.9s)\n",
      "need (1088.9s - 1089.2s)\n",
      "to (1089.2s - 1089.2s)\n",
      "memorize (1089.2s - 1089.4s)\n",
      "the (1089.4s - 1089.8s)\n",
      "data (1089.8s - 1090.0s)\n",
      "and (1090.0s - 1091.3s)\n",
      "if (1091.3s - 1091.4s)\n",
      "you're (1091.4s - 1091.5s)\n",
      "just (1091.5s - 1091.6s)\n",
      "copying (1091.6s - 1091.9s)\n",
      "a (1091.9s - 1092.0s)\n",
      "pointer (1092.0s - 1092.4s)\n",
      "that's (1092.4s - 1092.6s)\n",
      "going (1092.6s - 1092.8s)\n",
      "to (1092.8s - 1092.8s)\n",
      "be (1092.8s - 1092.9s)\n",
      "constant (1092.9s - 1093.5s)\n",
      "time (1093.5s - 1093.7s)\n",
      "no (1093.7s - 1093.9s)\n",
      "matter (1093.9s - 1094.0s)\n",
      "how (1094.0s - 1094.2s)\n",
      "big (1094.2s - 1094.3s)\n",
      "your (1094.3s - 1094.6s)\n",
      "data (1094.6s - 1094.8s)\n",
      "set (1094.8s - 1094.9s)\n",
      "is (1094.9s - 1095.2s)\n",
      "but (1095.2s - 1095.9s)\n",
      "now (1095.9s - 1096.1s)\n",
      "I'm (1096.1s - 1096.3s)\n",
      "at (1096.3s - 1096.6s)\n",
      "the (1096.6s - 1096.8s)\n",
      "time (1096.8s - 1097.3s)\n",
      "we (1097.3s - 1097.7s)\n",
      "need (1097.7s - 1097.7s)\n",
      "to (1097.7s - 1097.9s)\n",
      "do (1097.9s - 1098.0s)\n",
      "the (1098.0s - 1098.1s)\n",
      "stats (1098.1s - 1098.5s)\n",
      "comparison (1098.5s - 1098.9s)\n",
      "stop (1098.9s - 1099.4s)\n",
      "and (1099.4s - 1099.7s)\n",
      "compare (1099.7s - 1100.1s)\n",
      "our (1100.1s - 1100.3s)\n",
      "test (1100.3s - 1100.6s)\n",
      "image (1100.6s - 1100.9s)\n",
      "to (1100.9s - 1101.0s)\n",
      "each (1101.0s - 1101.2s)\n",
      "other (1101.2s - 1101.5s)\n",
      "and (1101.5s - 1102.0s)\n",
      "training (1102.0s - 1102.4s)\n",
      "examples (1102.4s - 1102.7s)\n",
      "in (1102.7s - 1103.1s)\n",
      "the (1103.1s - 1103.1s)\n",
      "data (1103.1s - 1103.4s)\n",
      "set (1103.4s - 1103.6s)\n",
      "and (1103.6s - 1104.4s)\n",
      "this (1104.4s - 1104.6s)\n",
      "is (1104.6s - 1104.8s)\n",
      "actually (1104.8s - 1104.9s)\n",
      "quite (1104.9s - 1105.6s)\n",
      "slow (1105.6s - 1106.1s)\n",
      "so (1107.7s - 1108.3s)\n",
      "this (1108.3s - 1108.8s)\n",
      "is (1108.8s - 1109.0s)\n",
      "actually (1109.0s - 1109.5s)\n",
      "somewhat (1109.5s - 1110.0s)\n",
      "backwards (1110.0s - 1110.6s)\n",
      "if (1110.6s - 1110.7s)\n",
      "you (1110.7s - 1110.8s)\n",
      "think (1110.8s - 1111.0s)\n",
      "about (1111.0s - 1111.0s)\n",
      "it (1111.0s - 1111.3s)\n",
      "because (1111.3s - 1111.8s)\n",
      "it (1111.8s - 1112.7s)\n",
      "in (1112.7s - 1112.9s)\n",
      "practice (1112.9s - 1113.0s)\n",
      "we (1113.0s - 1113.6s)\n",
      "we (1113.6s - 1113.7s)\n",
      "want (1113.7s - 1114.2s)\n",
      "our (1114.2s - 1114.3s)\n",
      "classifiers (1114.3s - 1114.9s)\n",
      "to (1114.9s - 1115.0s)\n",
      "be (1115.0s - 1115.1s)\n",
      "slow (1115.1s - 1115.6s)\n",
      "at (1115.6s - 1115.7s)\n",
      "training (1115.7s - 1116.0s)\n",
      "time (1116.0s - 1116.4s)\n",
      "and (1116.4s - 1116.8s)\n",
      "then (1116.8s - 1116.9s)\n",
      "fast (1116.9s - 1117.2s)\n",
      "a (1117.2s - 1117.3s)\n",
      "testing (1117.3s - 1117.8s)\n",
      "time (1117.8s - 1117.8s)\n",
      "because (1117.8s - 1118.7s)\n",
      "you (1118.7s - 1118.8s)\n",
      "might (1118.8s - 1119.0s)\n",
      "imagine (1119.0s - 1119.1s)\n",
      "that (1119.1s - 1119.7s)\n",
      "the (1119.7s - 1119.8s)\n",
      "classifier (1119.8s - 1120.3s)\n",
      "might (1120.3s - 1120.7s)\n",
      "go (1120.7s - 1120.9s)\n",
      "and (1120.9s - 1121.1s)\n",
      "be (1121.1s - 1121.3s)\n",
      "trained (1121.3s - 1121.6s)\n",
      "in (1121.6s - 1121.7s)\n",
      "a (1121.7s - 1121.8s)\n",
      "data (1121.8s - 1122.1s)\n",
      "center (1122.1s - 1122.3s)\n",
      "somewhere (1122.3s - 1122.6s)\n",
      "and (1122.6s - 1123.2s)\n",
      "you (1123.2s - 1123.3s)\n",
      "can (1123.3s - 1123.4s)\n",
      "afford (1123.4s - 1123.5s)\n",
      "to (1123.5s - 1123.8s)\n",
      "spend (1123.8s - 1124.1s)\n",
      "a (1124.1s - 1124.3s)\n",
      "lot (1124.3s - 1124.3s)\n",
      "of (1124.3s - 1124.6s)\n",
      "competition (1124.6s - 1125.2s)\n",
      "at (1125.2s - 1125.3s)\n",
      "training (1125.3s - 1125.5s)\n",
      "time (1125.5s - 1125.9s)\n",
      "to (1125.9s - 1126.2s)\n",
      "make (1126.2s - 1126.4s)\n",
      "the (1126.4s - 1126.5s)\n",
      "classifier (1126.5s - 1126.9s)\n",
      "really (1126.9s - 1127.2s)\n",
      "good (1127.2s - 1127.6s)\n",
      "but (1127.6s - 1128.1s)\n",
      "then (1128.1s - 1128.3s)\n",
      "when (1128.3s - 1128.5s)\n",
      "you (1128.5s - 1128.5s)\n",
      "go (1128.5s - 1128.7s)\n",
      "into (1128.7s - 1128.8s)\n",
      "Ploy (1128.8s - 1129.1s)\n",
      "the (1129.1s - 1129.2s)\n",
      "classifier (1129.2s - 1129.8s)\n",
      "a (1129.8s - 1129.9s)\n",
      "test (1129.9s - 1130.2s)\n",
      "time (1130.2s - 1130.5s)\n",
      "you (1130.5s - 1130.9s)\n",
      "want (1130.9s - 1131.1s)\n",
      "it (1131.1s - 1131.1s)\n",
      "to (1131.1s - 1131.3s)\n",
      "run (1131.3s - 1131.6s)\n",
      "on (1131.6s - 1131.7s)\n",
      "your (1131.7s - 1131.9s)\n",
      "mobile (1131.9s - 1132.2s)\n",
      "phone (1132.2s - 1132.3s)\n",
      "or (1132.3s - 1132.8s)\n",
      "in (1132.8s - 1132.9s)\n",
      "the (1132.9s - 1133.0s)\n",
      "browser (1133.0s - 1133.3s)\n",
      "or (1133.3s - 1133.7s)\n",
      "some (1133.7s - 1133.9s)\n",
      "other (1133.9s - 1134.1s)\n",
      "low-power (1134.1s - 1134.6s)\n",
      "device (1134.6s - 1135.0s)\n",
      "and (1135.0s - 1135.5s)\n",
      "you (1135.5s - 1135.6s)\n",
      "really (1135.6s - 1135.8s)\n",
      "want (1135.8s - 1136.1s)\n",
      "the (1136.1s - 1136.3s)\n",
      "testing (1136.3s - 1137.0s)\n",
      "time (1137.0s - 1137.3s)\n",
      "performance (1137.3s - 1137.7s)\n",
      "of (1137.7s - 1137.9s)\n",
      "your (1137.9s - 1138.0s)\n",
      "class (1138.0s - 1138.2s)\n",
      "fire (1138.2s - 1138.5s)\n",
      "to (1138.5s - 1139.0s)\n",
      "be (1139.0s - 1139.1s)\n",
      "quite (1139.1s - 1139.2s)\n",
      "fast (1139.2s - 1139.7s)\n",
      "so (1139.7s - 1140.3s)\n",
      "from (1140.3s - 1140.6s)\n",
      "this (1140.6s - 1140.7s)\n",
      "perspective (1140.7s - 1141.0s)\n",
      "this (1141.0s - 1142.1s)\n",
      "nearest (1142.1s - 1142.4s)\n",
      "neighbor (1142.4s - 1142.6s)\n",
      "algorithm (1142.6s - 1142.9s)\n",
      "is (1142.9s - 1143.2s)\n",
      "actually (1143.2s - 1143.5s)\n",
      "a (1143.5s - 1143.8s)\n",
      "little (1143.8s - 1143.9s)\n",
      "bit (1143.9s - 1144.1s)\n",
      "backwards (1144.1s - 1144.7s)\n",
      "and (1144.7s - 1145.1s)\n",
      "we'll (1145.1s - 1145.2s)\n",
      "see (1145.2s - 1145.4s)\n",
      "that (1145.4s - 1145.5s)\n",
      "once (1145.5s - 1145.9s)\n",
      "we (1145.9s - 1146.0s)\n",
      "moved (1146.0s - 1146.4s)\n",
      "to (1146.4s - 1146.4s)\n",
      "convolutional (1146.4s - 1147.2s)\n",
      "neural (1147.2s - 1147.4s)\n",
      "networks (1147.4s - 1147.8s)\n",
      "and (1147.8s - 1148.0s)\n",
      "other (1148.0s - 1148.0s)\n",
      "types (1148.0s - 1148.4s)\n",
      "of (1148.4s - 1148.4s)\n",
      "parametric (1148.4s - 1148.8s)\n",
      "models (1148.8s - 1149.3s)\n",
      "will (1149.3s - 1150.1s)\n",
      "be (1150.1s - 1150.2s)\n",
      "the (1150.2s - 1150.4s)\n",
      "reverse (1150.4s - 1150.6s)\n",
      "of (1150.6s - 1150.8s)\n",
      "this (1150.8s - 1151.0s)\n",
      "where (1151.0s - 1151.6s)\n",
      "you'll (1151.6s - 1151.8s)\n",
      "spend (1151.8s - 1151.9s)\n",
      "a (1151.9s - 1152.0s)\n",
      "lot (1152.0s - 1152.2s)\n",
      "of (1152.2s - 1152.3s)\n",
      "computer (1152.3s - 1152.6s)\n",
      "training (1152.6s - 1152.9s)\n",
      "time (1152.9s - 1153.2s)\n",
      "but (1153.2s - 1153.6s)\n",
      "then (1153.6s - 1153.8s)\n",
      "they'll (1153.8s - 1153.9s)\n",
      "be (1153.9s - 1154.0s)\n",
      "quite (1154.0s - 1154.3s)\n",
      "fast (1154.3s - 1154.7s)\n",
      "a (1154.7s - 1154.7s)\n",
      "testing (1154.7s - 1155.1s)\n",
      "time (1155.1s - 1155.2s)\n",
      "send (1157.7s - 1158.2s)\n",
      "a (1158.2s - 1158.2s)\n",
      "question (1158.2s - 1158.7s)\n",
      "is (1158.7s - 1158.9s)\n",
      "what (1158.9s - 1159.3s)\n",
      "exactly (1159.3s - 1159.4s)\n",
      "does (1159.4s - 1160.1s)\n",
      "this (1160.1s - 1160.3s)\n",
      "nearest (1160.3s - 1160.6s)\n",
      "neighbor (1160.6s - 1160.9s)\n",
      "algorithm (1160.9s - 1161.2s)\n",
      "look (1161.2s - 1161.6s)\n",
      "like (1161.6s - 1162.1s)\n",
      "when (1162.1s - 1162.2s)\n",
      "you (1162.2s - 1162.3s)\n",
      "apply (1162.3s - 1162.6s)\n",
      "it (1162.6s - 1162.7s)\n",
      "in (1162.7s - 1162.8s)\n",
      "practice (1162.8s - 1163.0s)\n",
      "so (1163.0s - 1164.0s)\n",
      "here (1164.0s - 1164.3s)\n",
      "we've (1164.3s - 1164.5s)\n",
      "drawn (1164.5s - 1164.7s)\n",
      "what (1164.7s - 1165.1s)\n",
      "we (1165.1s - 1165.3s)\n",
      "call (1165.3s - 1165.5s)\n",
      "the (1165.5s - 1165.7s)\n",
      "decision (1165.7s - 1166.3s)\n",
      "regions (1166.3s - 1167.0s)\n",
      "of (1167.0s - 1167.2s)\n",
      "our (1167.2s - 1167.7s)\n",
      "nearest (1167.7s - 1167.9s)\n",
      "neighbor (1167.9s - 1168.2s)\n",
      "classifier (1168.2s - 1168.7s)\n",
      "so (1168.7s - 1169.5s)\n",
      "here (1169.5s - 1169.8s)\n",
      "are (1169.8s - 1169.8s)\n",
      "training (1169.8s - 1170.5s)\n",
      "set (1170.5s - 1170.8s)\n",
      "is (1170.8s - 1171.1s)\n",
      "consist (1171.1s - 1171.6s)\n",
      "of (1171.6s - 1172.0s)\n",
      "these (1172.0s - 1172.2s)\n",
      "points (1172.2s - 1172.7s)\n",
      "in (1172.7s - 1172.8s)\n",
      "the (1172.8s - 1172.9s)\n",
      "in (1172.9s - 1173.1s)\n",
      "the (1173.1s - 1173.2s)\n",
      "two-dimensional (1173.2s - 1173.6s)\n",
      "plane (1173.6s - 1174.1s)\n",
      "where (1174.1s - 1175.3s)\n",
      "the (1175.3s - 1175.5s)\n",
      "the (1175.5s - 1175.6s)\n",
      "color (1175.6s - 1176.3s)\n",
      "of (1176.3s - 1176.4s)\n",
      "a (1176.4s - 1176.5s)\n",
      "point (1176.5s - 1176.8s)\n",
      "represents (1176.8s - 1177.0s)\n",
      "the (1177.0s - 1177.6s)\n",
      "category (1177.6s - 1178.5s)\n",
      "or (1178.5s - 1178.6s)\n",
      "the (1178.6s - 1178.7s)\n",
      "class (1178.7s - 1179.1s)\n",
      "label (1179.1s - 1179.6s)\n",
      "of (1179.6s - 1179.7s)\n",
      "that (1179.7s - 1179.9s)\n",
      "point (1179.9s - 1180.2s)\n",
      "so (1180.2s - 1180.8s)\n",
      "here (1180.8s - 1181.0s)\n",
      "we (1181.0s - 1181.2s)\n",
      "see (1181.2s - 1181.3s)\n",
      "if (1181.3s - 1181.4s)\n",
      "we (1181.4s - 1181.4s)\n",
      "have (1181.4s - 1181.6s)\n",
      "bypasses (1181.6s - 1182.3s)\n",
      "and (1182.3s - 1182.6s)\n",
      "some (1182.6s - 1183.0s)\n",
      "blue (1183.0s - 1183.2s)\n",
      "ones (1183.2s - 1183.4s)\n",
      "up (1183.4s - 1183.5s)\n",
      "in (1183.5s - 1183.6s)\n",
      "the (1183.6s - 1183.7s)\n",
      "corner (1183.7s - 1183.9s)\n",
      "here (1183.9s - 1184.3s)\n",
      "some (1184.3s - 1184.8s)\n",
      "purple (1184.8s - 1185.1s)\n",
      "ones (1185.1s - 1185.3s)\n",
      "in (1185.3s - 1185.4s)\n",
      "the (1185.4s - 1185.5s)\n",
      "upper (1185.5s - 1185.5s)\n",
      "right-hand (1185.5s - 1185.9s)\n",
      "corner (1185.9s - 1186.1s)\n",
      "and (1186.1s - 1187.0s)\n",
      "now (1187.0s - 1187.2s)\n",
      "for (1187.2s - 1187.4s)\n",
      "each (1187.4s - 1187.7s)\n",
      "pixel (1187.7s - 1188.1s)\n",
      "in (1188.1s - 1188.5s)\n",
      "this (1188.5s - 1188.6s)\n",
      "entire (1188.6s - 1189.0s)\n",
      "plane (1189.0s - 1189.5s)\n",
      "we've (1189.5s - 1189.9s)\n",
      "gone (1189.9s - 1190.2s)\n",
      "and (1190.2s - 1190.3s)\n",
      "computed (1190.3s - 1190.9s)\n",
      "what (1190.9s - 1191.3s)\n",
      "is (1191.3s - 1191.4s)\n",
      "the (1191.4s - 1191.5s)\n",
      "nearest (1191.5s - 1191.9s)\n",
      "Training (1191.9s - 1192.3s)\n",
      "what (1192.3s - 1193.1s)\n",
      "is (1193.1s - 1193.2s)\n",
      "the (1193.2s - 1193.3s)\n",
      "nearest (1193.3s - 1193.6s)\n",
      "example (1193.6s - 1194.1s)\n",
      "in (1194.1s - 1194.3s)\n",
      "the (1194.3s - 1194.4s)\n",
      "Indies (1194.4s - 1194.8s)\n",
      "trading (1194.8s - 1195.2s)\n",
      "data (1195.2s - 1195.4s)\n",
      "and (1195.4s - 1195.9s)\n",
      "then (1195.9s - 1196.0s)\n",
      "colored (1196.0s - 1196.5s)\n",
      "the (1196.5s - 1196.6s)\n",
      "point (1196.6s - 1196.9s)\n",
      "of (1196.9s - 1196.9s)\n",
      "the (1196.9s - 1197.0s)\n",
      "background (1197.0s - 1197.3s)\n",
      "corresponding (1197.3s - 1198.2s)\n",
      "to (1198.2s - 1198.3s)\n",
      "what (1198.3s - 1198.5s)\n",
      "is (1198.5s - 1198.7s)\n",
      "the (1198.7s - 1198.8s)\n",
      "class (1198.8s - 1199.1s)\n",
      "label (1199.1s - 1199.4s)\n",
      "so (1199.4s - 1200.2s)\n",
      "you (1200.2s - 1200.2s)\n",
      "can (1200.2s - 1200.4s)\n",
      "see (1200.4s - 1200.6s)\n",
      "that (1200.6s - 1200.7s)\n",
      "this (1200.7s - 1200.8s)\n",
      "nearest (1200.8s - 1201.1s)\n",
      "neighbor (1201.1s - 1201.5s)\n",
      "classifier (1201.5s - 1202.1s)\n",
      "is (1202.1s - 1202.3s)\n",
      "just (1202.3s - 1202.6s)\n",
      "one (1202.6s - 1202.7s)\n",
      "of (1202.7s - 1202.7s)\n",
      "carving (1202.7s - 1203.1s)\n",
      "up (1203.1s - 1203.2s)\n",
      "the (1203.2s - 1203.3s)\n",
      "space (1203.3s - 1203.7s)\n",
      "and (1203.7s - 1204.4s)\n",
      "coloring (1204.4s - 1205.2s)\n",
      "the (1205.2s - 1205.3s)\n",
      "space (1205.3s - 1205.6s)\n",
      "according (1205.6s - 1206.1s)\n",
      "to (1206.1s - 1206.2s)\n",
      "the (1206.2s - 1206.3s)\n",
      "nearby (1206.3s - 1206.5s)\n",
      "points (1206.5s - 1207.0s)\n",
      "but (1207.0s - 1208.7s)\n",
      "this (1208.7s - 1208.9s)\n",
      "discussed (1208.9s - 1209.5s)\n",
      "prior (1209.5s - 1209.7s)\n",
      "is (1209.7s - 1209.9s)\n",
      "maybe (1209.9s - 1210.2s)\n",
      "not (1210.2s - 1210.7s)\n",
      "so (1210.7s - 1210.9s)\n",
      "great (1210.9s - 1211.4s)\n",
      "and (1211.4s - 1211.7s)\n",
      "by (1211.7s - 1211.9s)\n",
      "looking (1211.9s - 1212.1s)\n",
      "at (1212.1s - 1212.3s)\n",
      "this (1212.3s - 1212.4s)\n",
      "picture (1212.4s - 1212.7s)\n",
      "we (1212.7s - 1213.2s)\n",
      "can (1213.2s - 1213.4s)\n",
      "start (1213.4s - 1213.7s)\n",
      "to (1213.7s - 1213.8s)\n",
      "see (1213.8s - 1213.8s)\n",
      "some (1213.8s - 1214.2s)\n",
      "of (1214.2s - 1214.3s)\n",
      "the (1214.3s - 1214.3s)\n",
      "problems (1214.3s - 1214.8s)\n",
      "that (1214.8s - 1214.9s)\n",
      "might (1214.9s - 1215.2s)\n",
      "come (1215.2s - 1215.5s)\n",
      "out (1215.5s - 1215.7s)\n",
      "with (1215.7s - 1215.8s)\n",
      "the (1215.8s - 1215.9s)\n",
      "nearest (1215.9s - 1216.1s)\n",
      "neighbor (1216.1s - 1216.4s)\n",
      "classifier (1216.4s - 1217.0s)\n",
      "for (1217.5s - 1217.9s)\n",
      "one (1217.9s - 1218.3s)\n",
      "this (1218.3s - 1219.0s)\n",
      "central (1219.0s - 1219.5s)\n",
      "region (1219.5s - 1219.9s)\n",
      "actually (1219.9s - 1220.2s)\n",
      "contains (1220.2s - 1220.7s)\n",
      "mostly (1220.7s - 1221.2s)\n",
      "green (1221.2s - 1221.5s)\n",
      "points (1221.5s - 1222.0s)\n",
      "but (1222.0s - 1222.5s)\n",
      "one (1222.5s - 1222.7s)\n",
      "little (1222.7s - 1223.0s)\n",
      "yellow (1223.0s - 1223.3s)\n",
      "point (1223.3s - 1223.6s)\n",
      "in (1223.6s - 1223.7s)\n",
      "the (1223.7s - 1223.8s)\n",
      "middle (1223.8s - 1224.0s)\n",
      "but (1224.0s - 1225.2s)\n",
      "because (1225.2s - 1225.7s)\n",
      "we're (1225.7s - 1225.9s)\n",
      "just (1225.9s - 1226.0s)\n",
      "looking (1226.0s - 1226.3s)\n",
      "at (1226.3s - 1226.4s)\n",
      "the (1226.4s - 1226.5s)\n",
      "nearest (1226.5s - 1226.8s)\n",
      "neighbor (1226.8s - 1227.2s)\n",
      "this (1227.2s - 1227.7s)\n",
      "causes (1227.7s - 1228.0s)\n",
      "a (1228.0s - 1228.1s)\n",
      "little (1228.1s - 1228.2s)\n",
      "yellow (1228.2s - 1228.6s)\n",
      "Island (1228.6s - 1229.0s)\n",
      "to (1229.0s - 1229.2s)\n",
      "appear (1229.2s - 1229.5s)\n",
      "in (1229.5s - 1229.7s)\n",
      "the (1229.7s - 1229.8s)\n",
      "middle (1229.8s - 1230.5s)\n",
      "of (1230.5s - 1230.6s)\n",
      "the (1230.6s - 1230.7s)\n",
      "green (1230.7s - 1230.9s)\n",
      "cluster (1230.9s - 1231.3s)\n",
      "and (1231.3s - 1231.8s)\n",
      "that's (1231.8s - 1232.0s)\n",
      "maybe (1232.0s - 1232.3s)\n",
      "not (1232.3s - 1232.3s)\n",
      "so (1232.3s - 1232.6s)\n",
      "great (1232.6s - 1232.9s)\n",
      "maybe (1232.9s - 1233.8s)\n",
      "those (1233.8s - 1234.1s)\n",
      "Regional (1234.1s - 1234.6s)\n",
      "maybe (1234.6s - 1234.9s)\n",
      "those (1234.9s - 1235.0s)\n",
      "points (1235.0s - 1235.4s)\n",
      "actually (1235.4s - 1235.5s)\n",
      "should (1235.5s - 1235.8s)\n",
      "have (1235.8s - 1235.9s)\n",
      "been (1235.9s - 1236.0s)\n",
      "clean (1236.0s - 1236.3s)\n",
      "and (1236.3s - 1237.2s)\n",
      "then (1237.2s - 1237.4s)\n",
      "similarly (1237.4s - 1237.9s)\n",
      "we (1237.9s - 1238.0s)\n",
      "also (1238.0s - 1238.2s)\n",
      "see (1238.2s - 1238.5s)\n",
      "these (1238.5s - 1238.8s)\n",
      "sort (1238.8s - 1239.0s)\n",
      "of (1239.0s - 1239.0s)\n",
      "fingers (1239.0s - 1239.7s)\n",
      "of (1239.7s - 1240.0s)\n",
      "the (1240.0s - 1240.1s)\n",
      "the (1240.1s - 1240.6s)\n",
      "different (1240.6s - 1241.0s)\n",
      "like (1241.0s - 1241.2s)\n",
      "the (1241.2s - 1241.3s)\n",
      "Greeks (1241.3s - 1241.8s)\n",
      "and (1241.8s - 1241.8s)\n",
      "pushing (1241.8s - 1242.3s)\n",
      "into (1242.3s - 1242.4s)\n",
      "the (1242.4s - 1242.5s)\n",
      "blue (1242.5s - 1242.7s)\n",
      "region (1242.7s - 1243.1s)\n",
      "again (1243.1s - 1243.6s)\n",
      "to (1243.6s - 1243.7s)\n",
      "the (1243.7s - 1243.8s)\n",
      "present (1243.8s - 1244.1s)\n",
      "due (1244.1s - 1244.2s)\n",
      "to (1244.2s - 1244.3s)\n",
      "the (1244.3s - 1244.4s)\n",
      "presence (1244.4s - 1244.8s)\n",
      "of (1244.8s - 1244.8s)\n",
      "one (1244.8s - 1245.1s)\n",
      "point (1245.1s - 1245.3s)\n",
      "which (1245.3s - 1245.9s)\n",
      "may (1245.9s - 1246.0s)\n",
      "have (1246.0s - 1246.1s)\n",
      "been (1246.1s - 1246.2s)\n",
      "noisy (1246.2s - 1246.5s)\n",
      "or (1246.5s - 1246.9s)\n",
      "spurious (1246.9s - 1247.4s)\n",
      "so (1247.4s - 1248.5s)\n",
      "this (1248.5s - 1248.8s)\n",
      "kind (1248.8s - 1249.0s)\n",
      "of (1249.0s - 1249.1s)\n",
      "motivates (1249.1s - 1249.6s)\n",
      "a (1249.6s - 1250.3s)\n",
      "slight (1250.3s - 1250.6s)\n",
      "generalization (1250.6s - 1251.4s)\n",
      "of (1251.4s - 1251.5s)\n",
      "this (1251.5s - 1251.7s)\n",
      "algorithm (1251.7s - 1251.9s)\n",
      "called (1251.9s - 1252.7s)\n",
      "K (1252.7s - 1252.8s)\n",
      "nearest (1252.8s - 1253.1s)\n",
      "neighbor (1253.1s - 1253.2s)\n",
      "so (1254.4s - 1254.9s)\n",
      "rather (1254.9s - 1255.3s)\n",
      "than (1255.3s - 1255.5s)\n",
      "just (1255.5s - 1256.0s)\n",
      "looking (1256.0s - 1256.2s)\n",
      "for (1256.2s - 1256.6s)\n",
      "the (1256.6s - 1256.6s)\n",
      "single (1256.6s - 1257.2s)\n",
      "nearest (1257.2s - 1257.6s)\n",
      "neighbor (1257.6s - 1257.9s)\n",
      "and (1257.9s - 1258.3s)\n",
      "said (1258.3s - 1258.6s)\n",
      "we'll (1258.6s - 1258.8s)\n",
      "do (1258.8s - 1258.9s)\n",
      "something (1258.9s - 1259.2s)\n",
      "a (1259.2s - 1259.7s)\n",
      "little (1259.7s - 1259.7s)\n",
      "bit (1259.7s - 1260.0s)\n",
      "fancier (1260.0s - 1260.8s)\n",
      "and (1260.8s - 1261.3s)\n",
      "find (1261.3s - 1261.7s)\n",
      "K (1261.7s - 1262.5s)\n",
      "of (1262.5s - 1262.8s)\n",
      "our (1262.8s - 1262.9s)\n",
      "nearest (1262.9s - 1263.2s)\n",
      "neighbors (1263.2s - 1263.7s)\n",
      "according (1263.7s - 1264.1s)\n",
      "to (1264.1s - 1264.2s)\n",
      "our (1264.2s - 1264.2s)\n",
      "distance (1264.2s - 1264.7s)\n",
      "metric (1264.7s - 1265.1s)\n",
      "and (1265.1s - 1265.5s)\n",
      "then (1265.5s - 1265.8s)\n",
      "take (1265.8s - 1266.2s)\n",
      "a (1266.2s - 1266.3s)\n",
      "vote (1266.3s - 1266.7s)\n",
      "among (1266.7s - 1266.8s)\n",
      "each (1266.8s - 1267.2s)\n",
      "of (1267.2s - 1267.3s)\n",
      "our (1267.3s - 1267.4s)\n",
      "neighbors (1267.4s - 1267.8s)\n",
      "and (1267.8s - 1268.3s)\n",
      "then (1268.3s - 1268.5s)\n",
      "predict (1268.5s - 1268.9s)\n",
      "the (1268.9s - 1269.0s)\n",
      "majority (1269.0s - 1269.5s)\n",
      "vote (1269.5s - 1269.8s)\n",
      "among (1269.8s - 1270.2s)\n",
      "our (1270.2s - 1270.3s)\n",
      "neighbors (1270.3s - 1270.5s)\n",
      "you (1270.5s - 1271.9s)\n",
      "can (1271.9s - 1272.0s)\n",
      "imagine (1272.0s - 1272.3s)\n",
      "slightly (1272.3s - 1272.7s)\n",
      "more (1272.7s - 1272.8s)\n",
      "complex (1272.8s - 1273.3s)\n",
      "ways (1273.3s - 1273.5s)\n",
      "of (1273.5s - 1273.7s)\n",
      "doing (1273.7s - 1273.9s)\n",
      "this (1273.9s - 1274.0s)\n",
      "maybe (1274.0s - 1274.4s)\n",
      "you (1274.4s - 1274.5s)\n",
      "vote (1274.5s - 1274.9s)\n",
      "waited (1274.9s - 1275.3s)\n",
      "on (1275.3s - 1275.4s)\n",
      "the (1275.4s - 1275.5s)\n",
      "distance (1275.5s - 1275.9s)\n",
      "or (1275.9s - 1276.2s)\n",
      "something (1276.2s - 1276.5s)\n",
      "like (1276.5s - 1276.6s)\n",
      "that (1276.6s - 1276.7s)\n",
      "but (1276.7s - 1277.6s)\n",
      "that's (1277.6s - 1277.7s)\n",
      "the (1277.7s - 1278.0s)\n",
      "simplest (1278.0s - 1278.3s)\n",
      "simplest (1278.3s - 1279.1s)\n",
      "thing (1279.1s - 1279.4s)\n",
      "that (1279.4s - 1279.6s)\n",
      "tends (1279.6s - 1279.8s)\n",
      "to (1279.8s - 1279.9s)\n",
      "work (1279.9s - 1280.2s)\n",
      "pretty (1280.2s - 1280.6s)\n",
      "well (1280.6s - 1280.7s)\n",
      "is (1280.7s - 1281.1s)\n",
      "just (1281.1s - 1281.2s)\n",
      "taking (1281.2s - 1281.5s)\n",
      "a (1281.5s - 1281.6s)\n",
      "majority (1281.6s - 1282.0s)\n",
      "vote (1282.0s - 1282.0s)\n",
      "so (1282.0s - 1283.0s)\n",
      "here (1283.0s - 1283.4s)\n",
      "we've (1283.4s - 1283.6s)\n",
      "shown (1283.6s - 1284.0s)\n",
      "the (1284.0s - 1284.2s)\n",
      "exact (1284.2s - 1284.5s)\n",
      "same (1284.5s - 1284.8s)\n",
      "set (1284.8s - 1285.2s)\n",
      "of (1285.2s - 1285.3s)\n",
      "points (1285.3s - 1285.4s)\n",
      "using (1285.4s - 1286.1s)\n",
      "this (1286.1s - 1286.5s)\n",
      "k (1286.5s - 1286.8s)\n",
      "equals (1286.8s - 1287.2s)\n",
      "1 (1287.2s - 1287.6s)\n",
      "nearest (1287.6s - 1287.9s)\n",
      "neighbor (1287.9s - 1288.3s)\n",
      "classifier (1288.3s - 1288.8s)\n",
      "as (1288.8s - 1289.3s)\n",
      "well (1289.3s - 1289.5s)\n",
      "as (1289.5s - 1289.6s)\n",
      "two (1289.6s - 1289.8s)\n",
      "equals (1289.8s - 1290.1s)\n",
      "three (1290.1s - 1290.4s)\n",
      "and (1290.4s - 1290.6s)\n",
      "cables (1290.6s - 1291.0s)\n",
      "five (1291.0s - 1291.4s)\n",
      "in (1291.4s - 1291.6s)\n",
      "the (1291.6s - 1291.6s)\n",
      "middle (1291.6s - 1291.8s)\n",
      "and (1291.8s - 1292.0s)\n",
      "on (1292.0s - 1292.1s)\n",
      "the (1292.1s - 1292.2s)\n",
      "right (1292.2s - 1292.3s)\n",
      "and (1292.3s - 1293.2s)\n",
      "once (1293.2s - 1293.7s)\n",
      "we (1293.7s - 1293.8s)\n",
      "move (1293.8s - 1294.0s)\n",
      "to (1294.0s - 1294.1s)\n",
      "k (1294.1s - 1294.3s)\n",
      "equals (1294.3s - 1294.5s)\n",
      "3 (1294.5s - 1295.0s)\n",
      "you (1295.0s - 1295.9s)\n",
      "can (1295.9s - 1295.9s)\n",
      "see (1295.9s - 1296.2s)\n",
      "that (1296.2s - 1296.4s)\n",
      "that (1296.4s - 1296.5s)\n",
      "spurious (1296.5s - 1296.9s)\n",
      "yellow (1296.9s - 1297.4s)\n",
      "point (1297.4s - 1297.8s)\n",
      "in (1297.8s - 1297.9s)\n",
      "the (1297.9s - 1298.0s)\n",
      "middle (1298.0s - 1298.2s)\n",
      "of (1298.2s - 1298.3s)\n",
      "the (1298.3s - 1298.4s)\n",
      "green (1298.4s - 1298.6s)\n",
      "cluster (1298.6s - 1299.0s)\n",
      "is (1299.0s - 1299.5s)\n",
      "no (1299.5s - 1299.7s)\n",
      "longer (1299.7s - 1299.9s)\n",
      "causing (1299.9s - 1300.5s)\n",
      "about (1300.5s - 1300.8s)\n",
      "the (1300.8s - 1301.1s)\n",
      "points (1301.1s - 1301.7s)\n",
      "near (1301.7s - 1301.9s)\n",
      "that (1301.9s - 1302.1s)\n",
      "region (1302.1s - 1302.4s)\n",
      "to (1302.4s - 1302.6s)\n",
      "be (1302.6s - 1302.7s)\n",
      "classified (1302.7s - 1302.9s)\n",
      "as (1302.9s - 1303.5s)\n",
      "yellow (1303.5s - 1303.8s)\n",
      "now (1303.8s - 1304.2s)\n",
      "this (1304.2s - 1304.4s)\n",
      "entire (1304.4s - 1304.8s)\n",
      "green (1304.8s - 1305.7s)\n",
      "Porsche (1305.7s - 1306.1s)\n",
      "in (1306.1s - 1306.2s)\n",
      "the (1306.2s - 1306.3s)\n",
      "middle (1306.3s - 1306.6s)\n",
      "is (1306.6s - 1307.0s)\n",
      "all (1307.0s - 1307.1s)\n",
      "being (1307.1s - 1307.3s)\n",
      "classified (1307.3s - 1307.6s)\n",
      "as (1307.6s - 1307.9s)\n",
      "green (1307.9s - 1308.2s)\n",
      "you (1308.2s - 1309.1s)\n",
      "can (1309.1s - 1309.3s)\n",
      "also (1309.3s - 1309.4s)\n",
      "see (1309.4s - 1309.7s)\n",
      "that (1309.7s - 1309.8s)\n",
      "these (1309.8s - 1310.1s)\n",
      "fingers (1310.1s - 1310.7s)\n",
      "of (1310.7s - 1311.0s)\n",
      "the (1311.0s - 1311.1s)\n",
      "red (1311.1s - 1311.5s)\n",
      "and (1311.5s - 1311.6s)\n",
      "blue (1311.6s - 1311.9s)\n",
      "regions (1311.9s - 1312.4s)\n",
      "are (1312.4s - 1312.5s)\n",
      "starting (1312.5s - 1312.8s)\n",
      "to (1312.8s - 1312.9s)\n",
      "get (1312.9s - 1313.0s)\n",
      "smooth (1313.0s - 1313.4s)\n",
      "out (1313.4s - 1313.6s)\n",
      "due (1313.6s - 1314.1s)\n",
      "to (1314.1s - 1314.2s)\n",
      "this (1314.2s - 1314.3s)\n",
      "are (1314.4s - 1314.7s)\n",
      "you (1314.7s - 1314.7s)\n",
      "voting (1314.7s - 1314.8s)\n",
      "and (1314.8s - 1315.6s)\n",
      "then (1315.6s - 1315.8s)\n",
      "once (1315.8s - 1316.4s)\n",
      "we (1316.4s - 1316.5s)\n",
      "move (1316.5s - 1316.8s)\n",
      "to (1316.8s - 1316.9s)\n",
      "the (1316.9s - 1317.0s)\n",
      "Caples (1317.0s - 1317.5s)\n",
      "5 (1317.5s - 1317.8s)\n",
      "case (1317.8s - 1318.2s)\n",
      "then (1318.2s - 1318.8s)\n",
      "these (1318.8s - 1319.1s)\n",
      "decisions (1319.1s - 1319.4s)\n",
      "boundaries (1319.4s - 1320.0s)\n",
      "between (1320.0s - 1320.2s)\n",
      "the (1320.2s - 1320.4s)\n",
      "blue (1320.4s - 1320.7s)\n",
      "and (1320.7s - 1320.8s)\n",
      "red (1320.8s - 1321.0s)\n",
      "regions (1321.0s - 1321.5s)\n",
      "had (1321.5s - 1321.9s)\n",
      "become (1321.9s - 1322.1s)\n",
      "quite (1322.1s - 1322.3s)\n",
      "smooth (1322.3s - 1322.7s)\n",
      "and (1322.7s - 1322.8s)\n",
      "quite (1322.8s - 1323.0s)\n",
      "nice (1323.0s - 1323.3s)\n",
      "so (1323.3s - 1324.0s)\n",
      "this (1324.0s - 1324.2s)\n",
      "is (1324.2s - 1324.4s)\n",
      "generally (1324.4s - 1324.8s)\n",
      "something (1324.8s - 1325.1s)\n",
      "so (1325.1s - 1325.4s)\n",
      "generally (1325.4s - 1325.9s)\n",
      "when (1325.9s - 1326.0s)\n",
      "you (1326.0s - 1326.1s)\n",
      "use (1326.1s - 1326.3s)\n",
      "the (1326.3s - 1326.4s)\n",
      "nearest (1326.4s - 1326.8s)\n",
      "neighbor (1326.8s - 1327.0s)\n",
      "classifiers (1327.0s - 1327.6s)\n",
      "you (1327.6s - 1328.0s)\n",
      "almost (1328.0s - 1328.2s)\n",
      "always (1328.2s - 1328.5s)\n",
      "want (1328.5s - 1328.8s)\n",
      "to (1328.8s - 1329.0s)\n",
      "use (1329.0s - 1329.3s)\n",
      "some (1329.3s - 1329.6s)\n",
      "some (1329.6s - 1330.1s)\n",
      "value (1330.1s - 1330.5s)\n",
      "of (1330.5s - 1330.6s)\n",
      "K (1330.6s - 1330.8s)\n",
      "which (1330.8s - 1332.0s)\n",
      "is (1332.0s - 1332.1s)\n",
      "larger (1332.1s - 1332.4s)\n",
      "than (1332.4s - 1332.5s)\n",
      "one (1332.5s - 1332.8s)\n",
      "because (1332.8s - 1334.2s)\n",
      "this (1334.2s - 1334.3s)\n",
      "tends (1334.3s - 1334.6s)\n",
      "to (1334.6s - 1334.7s)\n",
      "smooth (1334.7s - 1335.0s)\n",
      "out (1335.0s - 1335.2s)\n",
      "your (1335.2s - 1335.3s)\n",
      "your (1335.3s - 1335.5s)\n",
      "decision (1335.5s - 1336.1s)\n",
      "down (1336.1s - 1336.2s)\n",
      "trees (1336.2s - 1336.5s)\n",
      "and (1336.5s - 1336.7s)\n",
      "beats (1336.7s - 1336.8s)\n",
      "better (1336.8s - 1337.1s)\n",
      "better (1337.1s - 1337.4s)\n",
      "results (1337.4s - 1337.8s)\n",
      "so (1339.7s - 1340.2s)\n",
      "if (1340.2s - 1340.5s)\n",
      "we (1340.5s - 1340.6s)\n",
      "if (1340.6s - 1340.8s)\n",
      "we (1340.8s - 1340.9s)\n",
      "have (1340.9s - 1341.0s)\n",
      "the (1341.0s - 1341.6s)\n",
      "kind (1341.6s - 1341.8s)\n",
      "of (1341.8s - 1341.8s)\n",
      "questions (1341.8s - 1342.9s)\n",
      "yeah (1347.1s - 1347.5s)\n",
      "so (1347.5s - 1347.7s)\n",
      "the (1347.7s - 1347.8s)\n",
      "question (1347.8s - 1347.9s)\n",
      "is (1347.9s - 1348.2s)\n",
      "what (1348.2s - 1348.6s)\n",
      "is (1348.6s - 1348.7s)\n",
      "the (1348.7s - 1348.9s)\n",
      "deal (1348.9s - 1349.1s)\n",
      "with (1349.1s - 1349.2s)\n",
      "these (1349.2s - 1349.3s)\n",
      "white (1349.3s - 1349.6s)\n",
      "regions (1349.6s - 1350.1s)\n",
      "and (1350.1s - 1350.6s)\n",
      "those (1350.6s - 1350.8s)\n",
      "are (1350.8s - 1351.0s)\n",
      "the (1351.0s - 1351.1s)\n",
      "white (1351.1s - 1351.7s)\n",
      "regions (1351.7s - 1352.0s)\n",
      "are (1352.0s - 1352.1s)\n",
      "where (1352.1s - 1352.3s)\n",
      "they (1352.3s - 1352.4s)\n",
      "are (1352.4s - 1352.5s)\n",
      "where (1352.5s - 1353.1s)\n",
      "there (1353.1s - 1353.6s)\n",
      "was (1353.6s - 1353.7s)\n",
      "no (1353.7s - 1353.8s)\n",
      "majority (1353.8s - 1354.1s)\n",
      "among (1354.1s - 1354.5s)\n",
      "the (1354.5s - 1354.7s)\n",
      "K (1354.7s - 1354.9s)\n",
      "nearest (1354.9s - 1355.1s)\n",
      "neighbors (1355.1s - 1355.5s)\n",
      "you (1355.5s - 1356.4s)\n",
      "could (1356.4s - 1356.6s)\n",
      "imagine (1356.6s - 1356.7s)\n",
      "they (1356.7s - 1357.1s)\n",
      "be (1357.1s - 1357.2s)\n",
      "doing (1357.2s - 1357.3s)\n",
      "something (1357.3s - 1357.7s)\n",
      "slightly (1357.7s - 1358.0s)\n",
      "fancier (1358.0s - 1358.5s)\n",
      "and (1358.5s - 1358.9s)\n",
      "make (1358.9s - 1359.2s)\n",
      "me (1359.2s - 1359.3s)\n",
      "taking (1359.3s - 1359.6s)\n",
      "a (1359.6s - 1359.7s)\n",
      "gas (1359.7s - 1360.0s)\n",
      "or (1360.0s - 1360.2s)\n",
      "randomly (1360.2s - 1360.6s)\n",
      "randomly (1360.6s - 1361.4s)\n",
      "selecting (1361.4s - 1361.9s)\n",
      "among (1361.9s - 1362.1s)\n",
      "the (1362.1s - 1362.4s)\n",
      "majority (1362.4s - 1362.9s)\n",
      "winners (1362.9s - 1363.3s)\n",
      "but (1363.3s - 1363.9s)\n",
      "for (1363.9s - 1364.1s)\n",
      "this (1364.1s - 1364.2s)\n",
      "simple (1364.2s - 1364.5s)\n",
      "example (1364.5s - 1364.9s)\n",
      "of (1364.9s - 1364.9s)\n",
      "were (1364.9s - 1365.0s)\n",
      "just (1365.0s - 1365.2s)\n",
      "calling (1365.2s - 1365.4s)\n",
      "it (1365.4s - 1365.5s)\n",
      "wiped (1365.5s - 1365.9s)\n",
      "indicate (1365.9s - 1366.3s)\n",
      "that (1366.3s - 1366.3s)\n",
      "there (1366.3s - 1366.5s)\n",
      "was (1366.5s - 1366.7s)\n",
      "no (1366.7s - 1366.9s)\n",
      "nearest (1366.9s - 1367.2s)\n",
      "neighbor (1367.2s - 1367.5s)\n",
      "in (1367.5s - 1367.6s)\n",
      "those (1367.6s - 1367.7s)\n",
      "points (1367.7s - 1367.9s)\n",
      "so (1370.4s - 1371.2s)\n",
      "we (1371.2s - 1371.3s)\n",
      "already (1371.3s - 1371.3s)\n",
      "kind (1371.3s - 1371.6s)\n",
      "of (1371.6s - 1371.7s)\n",
      "sauce (1371.7s - 1372.0s)\n",
      "so (1372.0s - 1372.4s)\n",
      "I (1372.4s - 1372.6s)\n",
      "like (1372.6s - 1372.9s)\n",
      "to (1372.9s - 1373.0s)\n",
      "whatever (1373.0s - 1373.5s)\n",
      "you (1373.5s - 1373.7s)\n",
      "whenever (1373.7s - 1374.0s)\n",
      "we're (1374.0s - 1374.4s)\n",
      "thinking (1374.4s - 1374.4s)\n",
      "about (1374.4s - 1374.8s)\n",
      "computer (1374.8s - 1375.2s)\n",
      "vision (1375.2s - 1375.3s)\n",
      "I (1375.3s - 1375.6s)\n",
      "think (1375.6s - 1375.8s)\n",
      "it's (1375.8s - 1375.9s)\n",
      "really (1375.9s - 1376.1s)\n",
      "useful (1376.1s - 1376.4s)\n",
      "to (1376.4s - 1376.6s)\n",
      "kind (1376.6s - 1376.8s)\n",
      "of (1376.8s - 1376.8s)\n",
      "flip (1376.8s - 1377.4s)\n",
      "back (1377.4s - 1377.6s)\n",
      "and (1377.6s - 1377.7s)\n",
      "forth (1377.7s - 1377.8s)\n",
      "between (1377.8s - 1378.2s)\n",
      "several (1378.2s - 1378.6s)\n",
      "different (1378.6s - 1378.9s)\n",
      "viewpoints (1378.9s - 1379.5s)\n",
      "one (1379.5s - 1380.1s)\n",
      "is (1380.1s - 1380.3s)\n",
      "this (1380.3s - 1380.3s)\n",
      "idea (1380.3s - 1380.7s)\n",
      "of (1380.7s - 1381.0s)\n",
      "high (1381.0s - 1381.2s)\n",
      "dimensional (1381.2s - 1381.7s)\n",
      "points (1381.7s - 1382.0s)\n",
      "in (1382.0s - 1382.1s)\n",
      "the (1382.1s - 1382.2s)\n",
      "plane (1382.2s - 1382.4s)\n",
      "and (1382.4s - 1382.9s)\n",
      "then (1382.9s - 1383.0s)\n",
      "the (1383.0s - 1383.1s)\n",
      "other (1383.1s - 1383.2s)\n",
      "is (1383.2s - 1383.5s)\n",
      "actually (1383.5s - 1383.8s)\n",
      "looking (1383.8s - 1384.1s)\n",
      "at (1384.1s - 1384.2s)\n",
      "Ed (1384.2s - 1384.5s)\n",
      "concrete (1384.5s - 1384.9s)\n",
      "images (1384.9s - 1385.4s)\n",
      "because (1385.4s - 1386.3s)\n",
      "the (1386.3s - 1386.5s)\n",
      "pixels (1386.5s - 1386.9s)\n",
      "of (1386.9s - 1387.0s)\n",
      "the (1387.0s - 1387.0s)\n",
      "image (1387.0s - 1387.4s)\n",
      "actually (1387.4s - 1387.8s)\n",
      "allow (1387.8s - 1388.4s)\n",
      "us (1388.4s - 1388.5s)\n",
      "to (1388.5s - 1388.6s)\n",
      "think (1388.6s - 1388.8s)\n",
      "of (1388.8s - 1389.1s)\n",
      "these (1389.1s - 1389.4s)\n",
      "these (1389.4s - 1390.4s)\n",
      "images (1390.4s - 1390.7s)\n",
      "of (1390.7s - 1390.8s)\n",
      "high (1390.8s - 1391.1s)\n",
      "dimensional (1391.1s - 1391.5s)\n",
      "vectors (1391.5s - 1391.9s)\n",
      "and (1391.9s - 1392.4s)\n",
      "it (1392.4s - 1392.5s)\n",
      "is (1392.5s - 1392.7s)\n",
      "sort (1392.7s - 1392.8s)\n",
      "of (1392.8s - 1392.9s)\n",
      "useful (1392.9s - 1393.3s)\n",
      "to (1393.3s - 1393.3s)\n",
      "ping-pong (1393.3s - 1393.7s)\n",
      "back (1393.7s - 1394.0s)\n",
      "and (1394.0s - 1394.0s)\n",
      "forth (1394.0s - 1394.4s)\n",
      "between (1394.4s - 1394.5s)\n",
      "these (1394.5s - 1394.7s)\n",
      "two (1394.7s - 1394.9s)\n",
      "different (1394.9s - 1395.1s)\n",
      "viewpoints (1395.1s - 1395.3s)\n",
      "so (1395.3s - 1396.6s)\n",
      "then (1396.6s - 1396.8s)\n",
      "when (1396.8s - 1397.8s)\n",
      "Amber (1397.8s - 1399.3s)\n",
      "and (1399.3s - 1399.4s)\n",
      "going (1399.4s - 1399.6s)\n",
      "back (1399.6s - 1399.8s)\n",
      "to (1399.8s - 1399.8s)\n",
      "the (1399.8s - 1399.9s)\n",
      "images (1399.9s - 1400.4s)\n",
      "you (1400.4s - 1401.0s)\n",
      "can (1401.0s - 1401.2s)\n",
      "see (1401.2s - 1401.3s)\n",
      "that (1401.3s - 1401.5s)\n",
      "it's (1401.5s - 1401.6s)\n",
      "actually (1401.6s - 1401.7s)\n",
      "not (1401.7s - 1401.9s)\n",
      "very (1401.9s - 1402.1s)\n",
      "good (1402.1s - 1402.4s)\n",
      "if (1402.4s - 1402.6s)\n",
      "you (1402.6s - 1402.6s)\n",
      "have (1402.6s - 1402.7s)\n",
      "colored (1402.7s - 1403.1s)\n",
      "in (1403.1s - 1403.4s)\n",
      "red (1403.4s - 1403.6s)\n",
      "and (1403.6s - 1403.7s)\n",
      "green (1403.7s - 1404.0s)\n",
      "witch (1404.0s - 1404.6s)\n",
      "images (1404.6s - 1405.0s)\n",
      "would (1405.0s - 1405.3s)\n",
      "actually (1405.3s - 1405.4s)\n",
      "be (1405.4s - 1405.6s)\n",
      "classified (1405.6s - 1406.1s)\n",
      "correctly (1406.1s - 1406.4s)\n",
      "or (1406.4s - 1406.6s)\n",
      "incorrectly (1406.6s - 1406.7s)\n",
      "according (1406.7s - 1407.5s)\n",
      "to (1407.5s - 1407.6s)\n",
      "their (1407.6s - 1407.7s)\n",
      "nearest (1407.7s - 1407.9s)\n",
      "neighbor (1407.9s - 1408.3s)\n",
      "and (1408.3s - 1408.9s)\n",
      "you (1408.9s - 1408.9s)\n",
      "can (1408.9s - 1409.1s)\n",
      "see (1409.1s - 1409.2s)\n",
      "that (1409.2s - 1409.3s)\n",
      "it's (1409.3s - 1409.5s)\n",
      "really (1409.5s - 1409.9s)\n",
      "not (1409.9s - 1410.0s)\n",
      "very (1410.0s - 1410.1s)\n",
      "good (1410.1s - 1410.4s)\n",
      "but (1410.4s - 1411.4s)\n",
      "maybe (1411.4s - 1411.8s)\n",
      "if (1411.8s - 1412.0s)\n",
      "we (1412.0s - 1412.1s)\n",
      "used (1412.1s - 1412.5s)\n",
      "a (1412.5s - 1413.1s)\n",
      "larger (1413.1s - 1413.5s)\n",
      "value (1413.5s - 1413.6s)\n",
      "of (1413.6s - 1413.8s)\n",
      "K (1413.8s - 1414.0s)\n",
      "then (1414.0s - 1414.8s)\n",
      "this (1414.8s - 1415.0s)\n",
      "would (1415.0s - 1415.2s)\n",
      "involve (1415.2s - 1415.4s)\n",
      "actually (1415.4s - 1416.0s)\n",
      "voting (1416.0s - 1416.4s)\n",
      "among (1416.4s - 1416.8s)\n",
      "maybe (1416.8s - 1417.0s)\n",
      "the (1417.0s - 1417.1s)\n",
      "top (1417.1s - 1417.4s)\n",
      "three (1417.4s - 1417.7s)\n",
      "or (1417.7s - 1417.8s)\n",
      "the (1417.8s - 1417.9s)\n",
      "top (1417.9s - 1418.1s)\n",
      "five (1418.1s - 1418.4s)\n",
      "maybe (1418.4s - 1419.1s)\n",
      "even (1419.1s - 1419.2s)\n",
      "the (1419.2s - 1419.4s)\n",
      "whole (1419.4s - 1419.4s)\n",
      "row (1419.4s - 1419.8s)\n",
      "and (1419.8s - 1420.4s)\n",
      "you (1420.4s - 1420.6s)\n",
      "can (1420.6s - 1420.6s)\n",
      "imagine (1420.6s - 1420.7s)\n",
      "that (1420.7s - 1421.1s)\n",
      "that (1421.1s - 1421.3s)\n",
      "would (1421.3s - 1421.5s)\n",
      "end (1421.5s - 1421.7s)\n",
      "up (1421.7s - 1421.7s)\n",
      "being (1421.7s - 1421.9s)\n",
      "a (1421.9s - 1422.0s)\n",
      "lot (1422.0s - 1422.1s)\n",
      "more (1422.1s - 1422.2s)\n",
      "robust (1422.2s - 1422.8s)\n",
      "to (1422.8s - 1423.1s)\n",
      "some (1423.1s - 1423.3s)\n",
      "of (1423.3s - 1423.4s)\n",
      "these (1423.4s - 1423.5s)\n",
      "some (1423.5s - 1424.0s)\n",
      "of (1424.0s - 1424.0s)\n",
      "this (1424.0s - 1424.2s)\n",
      "noise (1424.2s - 1424.4s)\n",
      "that (1424.4s - 1424.7s)\n",
      "we (1424.7s - 1424.8s)\n",
      "see (1424.8s - 1425.0s)\n",
      "it (1425.0s - 1425.4s)\n",
      "when (1425.4s - 1425.6s)\n",
      "retreating (1425.6s - 1426.1s)\n",
      "Neighbors (1426.1s - 1426.4s)\n",
      "in (1426.4s - 1426.5s)\n",
      "this (1426.5s - 1426.6s)\n",
      "way (1426.6s - 1426.8s)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# audio_path = os.path.join(RECORDING, f\"test.webm\")\n",
    "# transcriptions = run_stt(audio_path)\n",
    "\n",
    "# Example usage\n",
    "gcs_uri = 'gs://vividreview_bucket/test.flac'\n",
    "transcriptions = run_stt(gcs_uri)\n",
    "\n",
    "# Print transcriptions with timestamps\n",
    "for transcription in transcriptions:\n",
    "    print(f\"{transcription['word']} ({transcription['start_time']}s - {transcription['end_time']}s)\")\n",
    "\n",
    "# Save the result to a JSON file\n",
    "import json\n",
    "output_path = os.path.join(SCRIPT, f\"test_word.json\")\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(transcriptions, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.2 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "[aist#0:0/pcm_s16le @ 0x138104340] Guessed Channel Layout: stereo\n",
      "Input #0, wav, from '../recordings/test.webm':\n",
      "  Duration: 00:23:47.58, bitrate: 1411 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "File '../recordings/test1.mp3' already exists. Overwrite? [y/N] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m파일 변환 중 오류 발생: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mconvert_webm_to_mp3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../recordings/test.webm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../recordings/test1.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mconvert_webm_to_mp3\u001b[0;34m(webm_path, mp3_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_webm_to_mp3\u001b[39m(webm_path: \u001b[38;5;28mstr\u001b[39m, mp3_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m         \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebm_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp3_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ffmpeg\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/ffmpeg/_run.py:322\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ffmpeg for the supplied node graph.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mReturns: (out, err) tuple containing captured stdout and stderr data.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m process \u001b[38;5;241m=\u001b[39m run_async(\n\u001b[1;32m    314\u001b[0m     stream_spec,\n\u001b[1;32m    315\u001b[0m     cmd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     overwrite_output\u001b[38;5;241m=\u001b[39moverwrite_output,\n\u001b[1;32m    321\u001b[0m )\n\u001b[0;32m--> 322\u001b[0m out, err \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1126\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py:1875\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1875\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1879\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from pydub import AudioSegment\n",
    "import ffmpeg\n",
    "\n",
    "def convert_webm_to_mp3(webm_path: str, mp3_path: str):\n",
    "    try:\n",
    "        ffmpeg.input(webm_path).output(mp3_path).run()\n",
    "    except ffmpeg.Error as e:\n",
    "        print(f\"ffmpeg error: {e.stderr}\")\n",
    "        raise Exception(f\"파일 변환 중 오류 발생: {e}\")\n",
    "    \n",
    "\n",
    "convert_webm_to_mp3('../recordings/test.webm', '../recordings/test1.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai api stt\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "RECORDING = './recordings'  \n",
    "\n",
    "audio_file = open(os.path.join(RECORDING, \"test.mp3\"), \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "# print(transcription)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_path = os.path.join(SCRIPT, f\"test_transcription_gpt.json\")\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(transcription.text, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai api stt with timestamp\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "RECORDING = './recordings'  \n",
    "\n",
    "audio_file = open(os.path.join(RECORDING, \"test1.mp3\"), \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  file=audio_file,\n",
    "  model=\"whisper-1\",\n",
    "  response_format=\"verbose_json\",\n",
    "  timestamp_granularities=[\"word\"]\n",
    ")\n",
    "\n",
    "# print(transcript.words)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_path = os.path.join(SCRIPT, f\"test_text_word.json\")\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(transcript.words, output_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('/mnt/data/test_word_gpt.json', 'r') as file:\n",
    "    word_times = json.load(file)\n",
    "\n",
    "# Convert all \"word\" entries to lowercase\n",
    "for entry in word_times:\n",
    "    entry[\"word\"] = entry[\"word\"].lower()\n",
    "\n",
    "# Save the modified data back to the file\n",
    "with open('/mnt/data/test_word_gpt_lower.json', 'w') as file:\n",
    "    json.dump(word_times, file, indent=4)\n",
    "\n",
    "# Print the output (optional)\n",
    "print(json.dumps(word_times, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 40.0,\n",
      "        \"text\": \"okay, so welcome to lecture two of cs231n. on tuesday we, just recall, we, sort of, gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. and today, we're really going to dive in, for the first time, into the details. and we'll start to see, in much more depth, exactly how some of these learning algorithms actually work in practice. so, the first lecture of the class is probably, sort of, the largest big picture vision. and the majority of the lectures in this class will be much more detail orientated, much more focused on the specific mechanics, of these different algorithms. so, today we'll see our first learning algorithm and that'll be really exciting, i think. but, before we get to that, i wanted to talk about a couple of administrative issues.\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"start\": 40.47999954223633,\n",
      "        \"end\": 100.77999877929688,\n",
      "        \"text\": \"one, is piazza. so, i saw it when i checked yesterday, it seemed like we had maybe 500 students signed up on piazza. which means that there are several hundred of you who are not yet there. so, we really want piazza to be the main source of communication between the students and the core staff. so, we've gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance. and, any, sort of, questions like that should really go to piazza. you'll probably get answers to your questions faster on piazza, because all the tas are knowing to check that. and it's, sort of, easy for emails to get lost in the shuffle if you just send to the course list. it's also come to my attention that some scpd students are having a bit of a hard time signing up for piazza. scpd students are supposed to receive a @stanford.edu email address. so, once you get that email address, then you can use the stanford email to sign into piazza. probably that doesn't affect those of you who are sitting in the room right now, but, for those students listening on scpd.\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"start\": 100.77999877929688,\n",
      "        \"end\": 152.3000030517578,\n",
      "        \"text\": \"the next administrative issue is about assignment one. assignment one will be up later today, probably sometime this afternoon, but i promise, before i go to sleep tonight, it'll be up. but, if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. it'll be pretty much the same content. we're just reshuffling it a little bit to make it, like, for example, upgrading to work with python 3, rather than python 2.7. and some of these minor cosmetic changes, but the content of the assignment will still be the same as last year. so, in this assignment you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. you'll also implement several different linear classifiers, including the svm and softmax, as well as a simple two-layer neural network. and we'll cover all this content over the next couple of lectures. so,\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"start\": 152.3000030517578,\n",
      "        \"end\": 200.66000366210938,\n",
      "        \"text\": \"all of our assignments are using python and numpy. if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed. but, this is, actually, pretty important. numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. and you'll get a lot of practice with this on the first assignment. so, for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"start\": 200.66000366210938,\n",
      "        \"end\": 282.0400085449219,\n",
      "        \"text\": \"the other thing i wanted to talk about is that we're happy to announce that we're officially supported through google cloud for this class. so, google cloud is somewhat similar to amazon aws. you can go and start virtual machines up in the cloud. these virtual machines can have gpus. we're working on the tutorial for exactly how to use google cloud and get it to work for the assignments. but our intention is that you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. and because google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use google cloud credits for free for the class. so you can feel free to use these for the assignments and also for the course projects when you want to start using gpus and larger machines and whatnot. so, we'll post more details about that, probably, on piazza later today. but, i just wanted to mention, because i know there had been a couple of questions about, can i use my laptop? do i have to run on corn? do i have to, whatever? and the answer is that, you'll be able to run on google cloud and we'll provide you some coupons for that. yeah, so, those are, kind of, the major administrative issues i wanted to talk about today. and then, let's dive into the content.\"\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"start\": 282.0400085449219,\n",
      "        \"end\": 337.94000244140625,\n",
      "        \"text\": \"so, the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision. and this is something that we'll really focus on throughout the course of the class. is, exactly, how do we work on this image classification task? so, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware of some predetermined set of categories or labels. so, these might be, like, a dog or a cat or a truck or a plane, and there's some fixed set of category labels, and the job of the computer is to look at the picture and assign it one of these fixed category labels. this seems like a really easy problem, because so much of your own visual system in your brain is hardwired to doing these, sort of, visual recognition tasks. but this is actually a really, really hard problem for a machine.\"\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"start\": 337.94000244140625,\n",
      "        \"end\": 399.2200012207031,\n",
      "        \"text\": \"so, if you dig in and think about, actually, what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it. and the computer really is representing the image as this gigantic grid of numbers. so, the image might be something like 800 by 600 pixels. and each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. so, to the computer, this is just a gigantic grid of numbers. and it's very difficult to distill the cat-ness out of this, like, giant array of thousands, or whatever, very many different numbers. so, we refer to this problem as the semantic gap. this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image, and there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. and this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\"\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"start\": 399.2200012207031,\n",
      "        \"end\": 423.5799865722656,\n",
      "        \"text\": \"so, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid, every single pixel, in this giant grid of numbers would be completely different. but, somehow, it's still representing the same cat. and our algorithms need to be robust to this. but, not only viewpoint is one problem, another is illumination.\"\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"start\": 423.5799865722656,\n",
      "        \"end\": 437.3999938964844,\n",
      "        \"text\": \"there can be different lighting conditions going on in the scene. whether the cat is appearing in this very dark, moody scene, or like is this very bright, sunlit scene, it's still a cat, and our algorithms need to be robust to that.\"\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"start\": 437.3999938964844,\n",
      "        \"end\": 450.79998779296875,\n",
      "        \"text\": \"objects can also deform. i think cats are, maybe, among the more deformable of animals that you might see out there. and cats can really assume a lot of different, varied poses and positions. and our algorithms should be robust to these different kinds of transforms.\"\n",
      "    },\n",
      "    \"11\": {\n",
      "        \"start\": 450.79998779296875,\n",
      "        \"end\": 475.6600036621094,\n",
      "        \"text\": \"there can also be problems of occlusion, where you might only see part of a cat, like, just the face, or in this extreme example, just a tail peeking out from under the couch cushion. but, in these cases, it's pretty easy for you, as a person, to realize that this is probably a cat, and you still recognize these images as cats. and this is something that our algorithms also must be robust to, which is quite difficult, i think.\"\n",
      "    },\n",
      "    \"12\": {\n",
      "        \"start\": 475.6600036621094,\n",
      "        \"end\": 488.82000732421875,\n",
      "        \"text\": \"there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background. and this is another thing that we need to handle.\"\n",
      "    },\n",
      "    \"13\": {\n",
      "        \"start\": 488.82000732421875,\n",
      "        \"end\": 569.8599853515625,\n",
      "        \"text\": \"there's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances. and cats can come in different shapes and sizes and colors and ages. and our algorithm, again, needs to work and handle all these different variations. so, this is actually a really, really challenging problem. and it's sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things. but now if we want our computer programs to deal with all of these problems, all simultaneously, and not just for cats, by the way, but for just about any object category you can imagine, this is a fantastically challenging problem. and it's, actually, somewhat miraculous that this works at all, in my opinion. but, actually, not only does it work, but these things work very close to human accuracy in some limited situations. and take only hundreds of milliseconds to do so. so, this is some pretty amazing, incredible technology, in my opinion, and over the course of the rest of the class we will really see what kinds of advancements have made this possible. so now, if you, kind of, think about what is the api for writing an image classifier, you might sit down and try to write a method in python like this. where you want to take in an image and then do some crazy magic and then, eventually, spit out this class label to say cat or dog or whatnot. and there's really no obvious way to do this, right?\"\n",
      "    },\n",
      "    \"14\": {\n",
      "        \"start\": 569.8599853515625,\n",
      "        \"end\": 612.739990234375,\n",
      "        \"text\": \"if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something like rsa encryption, you, sort of, can write down an algorithm and enumerate all the steps that need to happen in order for this things to work. but, when we're trying to recognize objects, or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense, for how you might go about recognizing these objects. so, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down and write this function, i think most people would be in trouble. that being said, people have definitely made explicit attempts to try to write, sort of, high-end coded rules for recognizing different animals. so,\"\n",
      "    },\n",
      "    \"15\": {\n",
      "        \"start\": 612.9600219726562,\n",
      "        \"end\": 739.6599731445312,\n",
      "        \"text\": \"we touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses. and we know that edges, from hubel and wiesel, we know that edges are pretty important when it comes to visual recognition. so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats. but this turns out not to work very well. one, it's super brittle. and, two, say, if you want to start over for another object category, and maybe not worry about cats, but talk about trucks or dogs or fishes or something else, then you need to start all over again. so, this is really not a very scalable approach. we want to come up with some algorithm, or some method, for these recognition tasks which scales much more naturally to all the variety of objects in the world. so, the insight that, sort of, makes this all work is this idea of the data-driven approach. rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this. and we can actually use tools like google image search, or something like that, to go out and collect a very large number of examples of these different categories. by the way, this actually takes quite a lot of effort to go out and actually collect these datasets but, luckily, there's a lot of really good, high quality datasets out there already for you to use. then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. then finally, we'll use this training model and apply it on new images that will then be able to recognize cats and dogs and whatnot.\"\n",
      "    },\n",
      "    \"16\": {\n",
      "        \"start\": 739.6599731445312,\n",
      "        \"end\": 792.4600219726562,\n",
      "        \"text\": \"so here our api has changed a little bit. rather than a single function that just inputs an image and recognizes a cat, we have these two functions. one, called, train, that's going to input images and labels and then output a model, and then, separately, another function called, predict, which will input the model and than make predictions for images. and this is, kind of, the key insight that allowed all these things to start working really well over the last 10, 20 years or so. so, this class is primarily about neural networks and convolutional neural networks and deep learning and all that, but this idea of a data-driven approach is much more general than just deep learning. and i think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. so, probably, the simplest classifier you can imagine is something we call nearest neighbor.\"\n",
      "    },\n",
      "    \"17\": {\n",
      "        \"start\": 792.4600219726562,\n",
      "        \"end\": 826.6199951171875,\n",
      "        \"text\": \"the algorithm is pretty dumb, honestly. so, during the training step we won't do anything, we'll just memorize all of the training data. so this is very simple. and now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image. a very simple algorithm. but it, sort of, has a lot of these nice properties with respect to data-drivenness and whatnot. so, to be a little bit more concrete,\"\n",
      "    },\n",
      "    \"18\": {\n",
      "        \"start\": 826.6199951171875,\n",
      "        \"end\": 855.1400146484375,\n",
      "        \"text\": \"you might imagine working on this dataset called cifar-10, which is very commonly used in machine learning, as kind of a small test case. and you'll be working with this dataset on your homework. so, the cifar-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. and for each of those 10 categories it provides 50,000 training images, roughly evenly distributed across these 10 categories. and then 10,000 additional testing images that you're supposed to test your algorithm on.\"\n",
      "    },\n",
      "    \"19\": {\n",
      "        \"start\": 855.4000244140625,\n",
      "        \"end\": 957.3200073242188,\n",
      "        \"text\": \"so here's an example of applying this simple nearest neighbor classifier to some of these test images on cifar-10. so, on this grid on the right, for the left most column, gives a test image in the cifar-10 dataset. and now on the right, we've sorted the training images and show the most similar training images to each of these test examples. and you can see that they look kind of visually similar to the training images, although they are not always correct, right? so, maybe on the second row, we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. but, this image is a dog and it's nearest neighbor is also a dog, but this next one, i think is actually a deer or a horse or something else. but, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. so, if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set. and now, the closest example, we know it's label, because it comes from the training set. and now, we'll simply say that this testing image is also a dog. you can see from these examples that is probably not going to work very well, but it's still kind of a nice example to work through. but then, one detail that we need to know is, given a pair of images, how can we actually compare them? because, if we're going to take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\"\n",
      "    },\n",
      "    \"20\": {\n",
      "        \"start\": 957.3200073242188,\n",
      "        \"end\": 1005.1799926757812,\n",
      "        \"text\": \"so, in the example in the previous slide, we've used what's called the l1 distance, also sometimes called the manhattan distance. so, this is a really sort of simple, easy idea for comparing images. and that's that we're going to just compare individual pixels in these images. so, supposing that our test image is maybe just a tiny four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. and then, sum all these up across all the pixels in the image. so, this is kind of a stupid way to compare images, but it does some reasonable things sometimes. but, this gives us a very concrete way to measure the difference between two images. and in this case, we have this difference of 456 between these two images.\"\n",
      "    },\n",
      "    \"21\": {\n",
      "        \"start\": 1005.1799926757812,\n",
      "        \"end\": 1044.0,\n",
      "        \"text\": \"so, here's some full python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by numpy. so, here we can see that this training function, that we talked about earlier, is, again, very simple, in the case of nearest neighbor, you just memorize the training data, there's not really much to do here. and now, at test time, we're going to take in our image and then go in and compare using this l1 distance function, our test image to each of these training examples and find the most similar example in the training set.\"\n",
      "    },\n",
      "    \"22\": {\n",
      "        \"start\": 1044.0,\n",
      "        \"end\": 1055.8800048828125,\n",
      "        \"text\": \"and you can see that, we're actually able to do this in just one or two lines of python code by utilizing these vectorized operations in numpy. so, this is something that you'll get practice with on the first assignment.\"\n",
      "    },\n",
      "    \"23\": {\n",
      "        \"start\": 1055.8800048828125,\n",
      "        \"end\": 1064.6800537109375,\n",
      "        \"text\": \"so now, a couple questions about this simple classifier. first, if we have n examples in our training set, then how fast can we expect training and testing to be?\"\n",
      "    },\n",
      "    \"24\": {\n",
      "        \"start\": 1064.6800537109375,\n",
      "        \"end\": 1094.3800048828125,\n",
      "        \"text\": \"well, training is probably constant because we don't really need to do anything, we just need to memorize the data. and if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. but now, at test time we need to do this comparison stop and compare our test image to each of the n training examples in the dataset. and this is actually quite slow. so, this is actually somewhat backwards, if you think about it.\"\n",
      "    },\n",
      "    \"25\": {\n",
      "        \"start\": 1094.3800048828125,\n",
      "        \"end\": 1126.3800048828125,\n",
      "        \"text\": \"because, in practice, we want our classifiers to be slow at training time and then fast at testing time. because, you might imagine, that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good. but then, when you go and deploy the classifier at test time, you want it to run on your mobile phone or in a browser or some other low power device, and you really want the testing time performance of your classifier to be quite fast.\"\n",
      "    },\n",
      "    \"26\": {\n",
      "        \"start\": 1126.3800048828125,\n",
      "        \"end\": 1152.1800537109375,\n",
      "        \"text\": \"so, from this perspective, this nearest neighbor algorithm, is, actually, a little bit backwards. and we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. so then, the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? so, here we've drawn, what we call the decision regions of a nearest neighbor classifier.\"\n",
      "    },\n",
      "    \"27\": {\n",
      "        \"start\": 1152.1800537109375,\n",
      "        \"end\": 1232.9200439453125,\n",
      "        \"text\": \"so, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point. so, here we see we have five classes and some blue ones up in the corner here, some purple ones in the upper-right hand corner. and now for each pixel in this entire plane, we've gone and computed what is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. so, you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points. but this classifier is maybe not so great. and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier. for one, this central region actually contains mostly green points, but one little yellow point in the middle. but because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of this green cluster. and that's, maybe, not so great. maybe those points actually should have been green. and then, similarly we also see these, sort of, fingers, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. so, this kind of motivates a slight generalization of this algorithm called k-nearest neighbors.\"\n",
      "    },\n",
      "    \"28\": {\n",
      "        \"start\": 1234.219970703125,\n",
      "        \"end\": 1373.1600341796875,\n",
      "        \"text\": \"so rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find k of our nearest neighbors, according to our distance metric, and then take a vote among each of our neighbors. and then predict the majority vote among our neighbors. you can imagine slightly more complex ways of doing this. maybe you'd vote weighted on the distance, or something like that, but the simplest thing that tends to work pretty well is just taking a majority vote. so here we've shown the exact same set of points using this k=1 nearest neighbor classifier, as well as k=3 and k=5 in the middle and on the right. and once we move to k=3, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. now this entire green portion in the middle is all being classified as green. you can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. and then, once we move to the k=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. so, generally when you're using nearest neighbors classifiers, you almost always want to use some value of k, which is larger than one because this tends to smooth out your decision boundaries and lead to better results. question? [student asking a question] yes, so the question is, what is the deal with these white regions? the white regions are where there was no majority among the k-nearest neighbors. you could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners, but for this simple example we're just coloring it white to indicate there was no nearest neighbor in those points. whenever we're thinking about computer vision i think it's really useful to kind of flip back and forth between several different viewpoints. one, is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images. because the pixels of the image actually allow us to think of these images as high dimensional vectors. and it's sort of useful to ping pong back and forth between these two different viewpoints. so then, sort of taking this k-nearest neighbor and going back to the images you can see that it's actually not very good.\"\n",
      "    },\n",
      "    \"29\": {\n",
      "        \"start\": 1373.8800048828125,\n",
      "        \"end\": 1392.3800048828125,\n",
      "        \"text\": \"here i've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor. and you can see that it's really not very good. but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row.\"\n",
      "    },\n",
      "    \"30\": {\n",
      "        \"start\": 1392.3800048828125,\n",
      "        \"end\": 1399.739990234375,\n",
      "        \"text\": \"and you could imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way. \"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "SCRIPT = './scripts'\n",
    "SPM = './spms'\n",
    "\n",
    "# Load the JSON data from the files\n",
    "with open(os.path.join(SCRIPT, 'test_word_gpt.json'), 'r') as file:\n",
    "    word_times = json.load(file)\n",
    "\n",
    "with open(os.path.join(SPM, \"ftest_output.json\"), 'r') as file:\n",
    "    paragraphs = json.load(file)\n",
    "\n",
    "# Function to get start and end times of a paragraph based on word times\n",
    "def get_paragraph_times(paragraph_text, word_times):\n",
    "    \n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph_text.lower())\n",
    "    print(words)\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "\n",
    "    for word_time in word_times:\n",
    "        word = word_time[\"word\"].lower()  # Ensure case-insensitivity\n",
    "        if word in words:\n",
    "            if start_time is None:\n",
    "                start_time = word_time[\"start\"]\n",
    "            end_time = word_time[\"end\"]\n",
    "            words.remove(word)  # Remove the word from the list to handle duplicates\n",
    "    \n",
    "    return start_time, end_time\n",
    "\n",
    "# Create the structured output\n",
    "output = {}\n",
    "offset = 0\n",
    "for para_id, paragraph_text in paragraphs.items():\n",
    "    # start_time, end_time = get_paragraph_times(paragraph_text, word_times)\n",
    "    words = paragraph_text.split()\n",
    "    start_time = word_times[offset][\"start\"]\n",
    "    end_time = word_times[offset + len(words) - 1][\"end\"]\n",
    "    \n",
    "    output[para_id] = {\n",
    "        \"start\": start_time,\n",
    "        \"end\": end_time,\n",
    "        \"text\": paragraph_text\n",
    "    }\n",
    "    offset += len(words) \n",
    "\n",
    "# Save the output to a JSON file\n",
    "with open(os.path.join(SPM, \"ftest_output.json\"), 'w') as file:\n",
    "    json.dump(output, file, indent=4)\n",
    "\n",
    "# Print the output (optional)\n",
    "print(json.dumps(output, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay,\n",
      "so\n",
      "welcome\n",
      "to\n",
      "lecture\n",
      "two\n",
      "of\n",
      "cs231n.\n",
      "on\n",
      "tuesday\n",
      "we,\n",
      "just\n",
      "recall,\n",
      "we,\n",
      "sort\n",
      "of,\n",
      "gave\n",
      "you\n",
      "the\n",
      "big\n",
      "picture\n",
      "view\n",
      "of\n",
      "what\n",
      "is\n",
      "computer\n",
      "vision,\n",
      "what\n",
      "is\n",
      "the\n",
      "history,\n",
      "and\n",
      "a\n",
      "little\n",
      "bit\n",
      "of\n",
      "the\n",
      "overview\n",
      "of\n",
      "the\n",
      "class.\n",
      "and\n",
      "today,\n",
      "we're\n",
      "really\n",
      "going\n",
      "to\n",
      "dive\n",
      "in,\n",
      "for\n",
      "the\n",
      "first\n",
      "time,\n",
      "into\n",
      "the\n",
      "details.\n",
      "and\n",
      "we'll\n",
      "start\n",
      "to\n",
      "see,\n",
      "in\n",
      "much\n",
      "more\n",
      "depth,\n",
      "exactly\n",
      "how\n",
      "some\n",
      "of\n",
      "these\n",
      "learning\n",
      "algorithms\n",
      "actually\n",
      "work\n",
      "in\n",
      "practice.\n",
      "so,\n",
      "the\n",
      "first\n",
      "lecture\n",
      "of\n",
      "the\n",
      "class\n",
      "is\n",
      "probably,\n",
      "sort\n",
      "of,\n",
      "the\n",
      "largest\n",
      "big\n",
      "picture\n",
      "vision.\n",
      "and\n",
      "the\n",
      "majority\n",
      "of\n",
      "the\n",
      "lectures\n",
      "in\n",
      "this\n",
      "class\n",
      "will\n",
      "be\n",
      "much\n",
      "more\n",
      "detail\n",
      "orientated,\n",
      "much\n",
      "more\n",
      "focused\n",
      "on\n",
      "the\n",
      "specific\n",
      "mechanics,\n",
      "of\n",
      "these\n",
      "different\n",
      "algorithms.\n",
      "so,\n",
      "today\n",
      "we'll\n",
      "see\n",
      "our\n",
      "first\n",
      "learning\n",
      "algorithm\n",
      "and\n",
      "that'll\n",
      "be\n",
      "really\n",
      "exciting,\n",
      "i\n",
      "think.\n",
      "but,\n",
      "before\n",
      "we\n",
      "get\n",
      "to\n",
      "that,\n",
      "i\n",
      "wanted\n",
      "to\n",
      "talk\n",
      "about\n",
      "a\n",
      "couple\n",
      "of\n",
      "administrative\n",
      "issues.\n"
     ]
    }
   ],
   "source": [
    "strr = \"okay, so welcome to lecture two of cs231n. on tuesday we, just recall, we, sort of, gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. and today, we're really going to dive in, for the first time, into the details. and we'll start to see, in much more depth, exactly how some of these learning algorithms actually work in practice. so, the first lecture of the class is probably, sort of, the largest big picture vision. and the majority of the lectures in this class will be much more detail orientated, much more focused on the specific mechanics, of these different algorithms. so, today we'll see our first learning algorithm and that'll be really exciting, i think. but, before we get to that, i wanted to talk about a couple of administrative issues.\"\n",
    "\n",
    "for word in strr.split():\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722248729.623578 21106537 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: results {\n",
      "  alternatives {\n",
      "    transcript: \"hello everyone\"\n",
      "    confidence: 0.951208353\n",
      "  }\n",
      "  result_end_time {\n",
      "    seconds: 3\n",
      "    nanos: 380000000\n",
      "  }\n",
      "  language_code: \"en-us\"\n",
      "}\n",
      "results {\n",
      "  alternatives {\n",
      "    transcript: \" let\\'s started today\\'s class\"\n",
      "    confidence: 0.94850564\n",
      "  }\n",
      "  result_end_time {\n",
      "    seconds: 7\n",
      "    nanos: 160000000\n",
      "  }\n",
      "  language_code: \"en-us\"\n",
      "}\n",
      "total_billed_time {\n",
      "  seconds: 8\n",
      "}\n",
      "request_id: 2173716954119387849\n",
      "\n",
      "STT Result: hello everyone  let's started today's class\n"
     ]
    }
   ],
   "source": [
    "project_id = 68\n",
    "audio_path = os.path.join(RECORDING, f\"{project_id}_recording.webm\")\n",
    "text_output_path = os.path.join(SCRIPT, f\"{project_id}_transcription.txt\")\n",
    "\n",
    "run_stt(audio_path, text_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722962975.828745 29582425 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 44100 Hz\n",
      "Channels: 2\n",
      "Duration: 1427.577438 seconds\n",
      "Codec Name: pcm_s16le\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "def get_audio_info(file_path):\n",
    "    try:\n",
    "        # Extract audio information using ffmpeg\n",
    "        probe = ffmpeg.probe(file_path)\n",
    "        audio_info = next(stream for stream in probe['streams'] if stream['codec_type'] == 'audio')\n",
    "\n",
    "        # Extract relevant information\n",
    "        sample_rate = audio_info['sample_rate']\n",
    "        channels = audio_info['channels']\n",
    "        duration = audio_info['duration']\n",
    "        codec_name = audio_info['codec_name']\n",
    "\n",
    "        return {\n",
    "            'sample_rate': sample_rate,\n",
    "            'channels': channels,\n",
    "            'duration': duration,\n",
    "            'codec_name': codec_name\n",
    "        }\n",
    "\n",
    "    except ffmpeg.Error as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = os.path.join(RECORDING, f\"test.webm\")\n",
    "audio_info = get_audio_info(file_path)\n",
    "\n",
    "if audio_info:\n",
    "    print(f\"Sample Rate: {audio_info['sample_rate']} Hz\")\n",
    "    print(f\"Channels: {audio_info['channels']}\")\n",
    "    print(f\"Duration: {audio_info['duration']} seconds\")\n",
    "    print(f\"Codec Name: {audio_info['codec_name']}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve audio information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening './recordings/65_recording.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m project_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m\n\u001b[1;32m      7\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RECORDING, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_recording.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m data, samplerate \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(audio_path, data, samplerate)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wave\u001b[38;5;241m.\u001b[39mopen(audio_path) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/soundfile.py:285\u001b[0m, in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(file, frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, always_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m          fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m          \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide audio data from a sound file as NumPy array.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    By default, the whole file is read from the beginning, but the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    287\u001b[0m         frames \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m_prepare_read(start, stop, frames)\n\u001b[1;32m    288\u001b[0m         data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(frames, dtype, always_2d, fill_value, out)\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening './recordings/65_recording.wav': System error."
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "\n",
    "project_id = 65\n",
    "audio_path = os.path.join(RECORDING, f\"{project_id}_recording.wav\")\n",
    "data, samplerate = sf.read(audio_path)\n",
    "sf.write(audio_path, data, samplerate)\n",
    "with wave.open(audio_path) as file:\n",
    "    print('File opened!')\n",
    "\n",
    "def check_audio_file(audio_path):\n",
    "    with contextlib.closing(wave.open(audio_path, 'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "        print(f\"Duration: {duration} seconds\")\n",
    "        print(f\"Sample width: {f.getsampwidth()}\")\n",
    "        print(f\"Frame rate: {rate}\")\n",
    "        print(f\"Number of channels: {f.getnchannels()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 48000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722248678.993137 21106537 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n",
      "I0000 00:00:1722248679.034071 21106537 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def get_sample_rate(webm_file_path):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(webm_file_path, format=\"webm\")\n",
    "        sample_rate = audio.frame_rate\n",
    "        print(f\"Sample rate: {sample_rate} Hz\")\n",
    "        return sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 경로를 여기에 지정하세요\n",
    "project_id = 68\n",
    "webm_file_path = os.path.join(RECORDING, f\"{project_id}_recording.webm\")\n",
    "get_sample_rate(webm_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "file does not start with RIFF id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m project_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m68\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcheck_audio_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRECORDING\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_recording.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m, in \u001b[0;36mcheck_audio_file\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_audio_file\u001b[39m(audio_path):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43mwave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         frames \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mgetnframes()\n\u001b[1;32m      8\u001b[0m         rate \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mgetframerate()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/wave.py:509\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    507\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWave_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_write(f)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/wave.py:163\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# else, assume it is an open file object already\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/wave.py:130\u001b[0m, in \u001b[0;36mWave_read.initfp\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m Chunk(file, bigendian \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mgetname() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile does not start with RIFF id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWAVE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot a WAVE file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: file does not start with RIFF id"
     ]
    }
   ],
   "source": [
    "project_id = 68\n",
    "check_audio_file(RECORDING + f\"/{project_id}_recording.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Directory 'static/' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/fitz/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrontend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mop\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/events/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclipboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_mixins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhash_change\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/events/clipboard.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_mixins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClipboardDataMixin\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Event\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClipboardEvent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mClipboardEvent\u001b[39;00m(Event, ClipboardDataMixin):\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/dom.py:439\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatcher\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/dispatcher.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mendpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebSocketEndpoint\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebsockets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebSocket\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, server\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m later_await\n\u001b[1;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreact\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/frontend/server.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m app: Any \u001b[38;5;241m=\u001b[39m Starlette(debug\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[0;32m---> 24\u001b[0m app\u001b[38;5;241m.\u001b[39mmount(config\u001b[38;5;241m.\u001b[39mSTATIC_ROUTE, \u001b[43mStaticFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATIC_DIRECTORY\u001b[49m\u001b[43m)\u001b[49m, name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mSTATIC_NAME)\n\u001b[1;32m     25\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(GZipMiddleware)\n\u001b[1;32m     26\u001b[0m app\u001b[38;5;241m.\u001b[39madd_middleware(\n\u001b[1;32m     27\u001b[0m     CORSMiddleware,\n\u001b[1;32m     28\u001b[0m     allow_origins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     allow_headers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/starlette/staticfiles.py:59\u001b[0m, in \u001b[0;36mStaticFiles.__init__\u001b[0;34m(self, directory, packages, html, check_dir, follow_symlink)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_symlink \u001b[38;5;241m=\u001b[39m follow_symlink\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_dir \u001b[38;5;129;01mand\u001b[39;00m directory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(directory):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Directory 'static/' does not exist"
     ]
    }
   ],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "\n",
    "# OpenAI API Key 설정\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    주어진 PDF 파일에서 텍스트를 추출하는 함수입니다.\n",
    "    \n",
    "    :param pdf_path: PDF 파일의 경로입니다.\n",
    "    :return: 추출된 텍스트입니다.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def generate_toc_from_text(text):\n",
    "    \"\"\"\n",
    "    GPT API를 사용하여 주어진 텍스트에서 목차를 생성하는 함수입니다.\n",
    "    \n",
    "    :param text: 목차를 생성할 텍스트입니다.\n",
    "    :return: 생성된 목차입니다.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract a detailed table of contents with page numbers from the following text: {text}\"}\n",
    "        ]\n",
    "    )\n",
    "    toc = response.choices[0].message['content']\n",
    "    return toc\n",
    "\n",
    "# 테스트할 PDF 파일 경로를 설정합니다.\n",
    "# PDF 파일에서 텍스트를 추출합니다.\n",
    "project_id = 65\n",
    "pdf_path = os.path.join(SCRIPT, f\"{project_id}_transcription.pdf\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 추출된 텍스트를 GPT로 보내어 목차를 생성합니다.\n",
    "toc = generate_toc_from_text(text)\n",
    "\n",
    "# 생성된 목차를 출력합니다.\n",
    "print(\"Generated TOC:\")\n",
    "print(toc)\n",
    "\n",
    "# 생성된 목차를 JSON 파일로 저장합니다.\n",
    "# toc_data = {\"table_of_contents\": toc}\n",
    "# toc_json_path = f\"{os.path.splitext(pdf_path)[0]}_toc.json\"\n",
    "# with open(toc_json_path, \"w\") as json_file:\n",
    "#     json.dump(toc_data, json_file)\n",
    "\n",
    "# print(f\"TOC saved to {toc_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Invalid image.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_image'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-CToOZZDPbfraSxC93R7dT3BlbkFJIp0YHNEfyv14bkqduyvs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat’s in this image?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./cs231n_2017_lecture2_page-0003.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:646\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    645\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/git/VividReview/venv/lib/python3.9/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1045\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1049\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1050\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1054\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Invalid image.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_image'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"./cs231n_2017_lecture2_page-0003.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"table_of_contents\": [\n",
      "    {\n",
      "      \"title\": \"Lecture Overview\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"title\": \"Image Classification pipeline\",\n",
      "          \"page\": [1]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Administrative Information\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"title\": \"Piazza\",\n",
      "          \"page\": [2]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Assignment 1\",\n",
      "          \"page\": [3]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Python + Numpy\",\n",
      "          \"page\": [4]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Google Cloud\",\n",
      "          \"page\": [5]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Core Tasks in Computer Vision\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"title\": \"Image Classification\",\n",
      "          \"page\": [6]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"The Problem: Semantic Gap\",\n",
      "          \"page\": [7]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Challenges in Computer Vision\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"title\": \"Viewpoint Variation\",\n",
      "          \"page\": [8]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Illumination\",\n",
      "          \"page\": [9]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Deformation\",\n",
      "          \"page\": [10]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Occlusion\",\n",
      "          \"page\": [11]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Background Clutter\",\n",
      "          \"page\": [12]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Intraclass Variation\",\n",
      "          \"page\": [13]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Image Classification Strategy\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"title\": \"An image classifier\",\n",
      "          \"page\": [14]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Attempts\",\n",
      "          \"page\": [15]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Data-Driven Approach\",\n",
      "          \"page\": [16]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Nearest Neighbor Classifier\",\n",
      "          \"page\": [17]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Example Dataset: CIFAR10\",\n",
      "          \"page\": [18, 19]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Distance Metric to compare images\",\n",
      "          \"page\": [20]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Nearest Neighbor classifier code\",\n",
      "          \"page\": [21, 22, 23, 24, 25, 26]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"K-Nearest Neighbors\",\n",
      "          \"page\": [28]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"KNN Example\",\n",
      "          \"page\": [29, 30]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "TOC saved to ./tocs/10_toc.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "IMAGE = './images'  \n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "   \n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "project_id = 10\n",
    "image_directory = os.path.join(IMAGE, f\"{str(project_id)}\")\n",
    "image_paths = sorted([os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith('.png')])\n",
    "\n",
    "# images = convert_pdf_to_images(os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\"))\n",
    "\n",
    "# Getting the base64 string\n",
    "# Encode images\n",
    "encoded_images = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encode_image(image)}\"}} for image in image_paths]\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Creating the content for the messages\n",
    "content = [{\"type\": \"text\", \"text\": (\n",
    "            \"Extract a detailed table of contents with page numbers from the following images. \"\n",
    "            \"Each section should have a unique title, and the subsections should be grouped under these main sections. \"\n",
    "            \"The page numbers should start from 1 and each page number should be in an array. \"\n",
    "            \"Ensure there are at least 5 main sections, and each main section should have at most 5 subsections. \"\n",
    "            \"The output should be in JSON format with the structure: \"\n",
    "            \"{\\\"table_of_contents\\\": [{\\\"title\\\": \\\"string\\\", \\\"subsections\\\": [{\\\"title\\\": \\\"string\\\", \\\"page\\\": [\\\"number\\\"]}]}]} \"\n",
    "            \"Make sure that all pages are included in the table of contents.\"\n",
    "        )}] + encoded_images\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "    \"messages\": [\n",
    "        {\n",
    "           \"role\": \"system\", \n",
    "           \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "# Get the response\n",
    "response_data = response.json()\n",
    "\n",
    "# Check if 'choices' key exists in the response\n",
    "if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "    # Parse the table of contents from the response\n",
    "    toc_text = response_data['choices'][0]['message']['content']\n",
    "    print(toc_text)\n",
    "    # Convert the TOC text to JSON format\n",
    "    try:\n",
    "        toc_data = json.loads(toc_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        toc_data = {\"error\": \"Failed to decode JSON\"}\n",
    "else:\n",
    "    print(\"Error: 'choices' key not found in the response\")\n",
    "    print(response_data)\n",
    "    toc_data = {\"error\": \"Failed to retrieve TOC\"}\n",
    "\n",
    "# Save the TOC data as a JSON file\n",
    "toc_json_path = os.path.join(TOC, f\"{project_id}_toc.json\")\n",
    "with open(toc_json_path, \"w\") as json_file:\n",
    "    json.dump(toc_data, json_file, indent=4)\n",
    "\n",
    "print(f\"TOC saved to {toc_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# PDF 파일 경로\n",
    "IMAGE = './images'\n",
    "PDF = './pdfs'\n",
    "project_id = 60\n",
    "pdf_path = os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")\n",
    "\n",
    "# PDF를 이미지로 변환\n",
    "images = convert_from_path(pdf_path)\n",
    "os.makedirs(os.path.join(IMAGE, str(project_id)), exist_ok=True)\n",
    "\n",
    "# 이미지를 파일로 저장\n",
    "for i, image in enumerate(images):\n",
    "    image_path = os.path.join(IMAGE, str(project_id), f\"page_{i + 1:04}.png\")\n",
    "    image.save(image_path, \"PNG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw script text: {\n",
      "  \"1\": \"Okay, so welcome\",\n",
      "  \"2\": \"Lecture 2 of CS231N.\",\n",
      "  \"3\": \"On Tuesday, we, just\",\n",
      "  \"4\": \"we sort of gave\",\n",
      "  \"5\": \"big picture view of\",\n",
      "  \"6\": \"what is computer vision,\",\n",
      "  \"7\": \"is the history, and\",\n",
      "  \"8\": \"little bit of the\",\n",
      "  \"9\": \"overview of the class.\",\n",
      "  \"10\": \"And today, we're really\",\n",
      "  \"11\": \"going to dive in\",\n",
      "  \"12\": \"for the first time\",\n",
      "  \"13\": \"into the details, and\",\n",
      "  \"14\": \"start to see in\",\n",
      "  \"15\": \"much more depth exactly\",\n",
      "  \"16\": \"how some of these\",\n",
      "  \"17\": \"learning algorithms actually work\",\n",
      "  \"18\": \"in practice. So the\",\n",
      "  \"19\": \"first lecture of the\",\n",
      "  \"20\": \"class is probably the\",\n",
      "  \"21\": \"sort of the largest\",\n",
      "  \"22\": \"big picture vision, and\",\n",
      "  \"23\": \"the majority of the\",\n",
      "  \"24\": \"lectures in this class\",\n",
      "  \"25\": \"will be much more\",\n",
      "  \"26\": \"detail-oriented, and much more\",\n",
      "  \"27\": \"focused on the specific\",\n",
      "  \"28\": \"mechanics of these different\",\n",
      "  \"29\": \"algorithms. So today, we'll\",\n",
      "  \"30\": \"see our first learning\"\n",
      "}\n",
      "Script saved to ./spms/test_spm.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "IMAGE = './images'\n",
    "SCRIPT = './scripts'\n",
    "SPM = './spms'\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "   \n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "# Path to your image and script\n",
    "project_id = 10\n",
    "image_directory = os.path.join(IMAGE, f\"{str(project_id)}\")\n",
    "script_path = os.path.join(SCRIPT, \"10_transcription.txt\")\n",
    "\n",
    "image_paths = sorted([os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith('.png')])\n",
    "\n",
    "# print(image_paths)\n",
    "encoded_images = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encode_image(image)}\"}} for image in image_paths]\n",
    "\n",
    "# Read the script\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Creating the content for the messages\n",
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"Given the following lecture notes images and the corresponding lecture script, \"\n",
    "            \"please distribute the script content accurately to each page of the lecture notes. \"\n",
    "            \"The output should be in the format: {\\\"1\\\": \\\"script\\\", \\\"2\\\": \\\"script\\\", ...}. \"\n",
    "            \"Each key should correspond to the page number in the lecture notes where the script content appears, \"\n",
    "            \"and the value should be the first sentence of the script content for that page. \"\n",
    "            \"The value corresponding to a larger key must be a sentence that appears later in the script.\"\n",
    "            f\"The number of dictionary keys must be equal to {len(encoded_images)}. \"\n",
    "            \"The original format of the script, including uppercase and lowercase letters, punctuation marks such as periods and commas, must be preserved without any alterations. \"\n",
    "        )\n",
    "    },\n",
    "    {\"type\": \"text\", \"text\": script_content}\n",
    "] + encoded_images\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "# Get the response\n",
    "response_data = response.json()\n",
    "\n",
    "# Check if 'choices' key exists in the response\n",
    "if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "    # Parse the table of contents from the response\n",
    "    script_text = response_data['choices'][0]['message']['content']\n",
    "\n",
    "    print(\"Raw script text:\", script_text)\n",
    "    \n",
    "    # Convert the script text to JSON format\n",
    "    try:\n",
    "        script_data = json.loads(script_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "else:\n",
    "    print(\"Error: 'choices' key not found in the response\")\n",
    "    print(response_data)\n",
    "    script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "# Save the script data as a JSON file\n",
    "script_json_path = os.path.join(SPM, f\"test_spm.json\")\n",
    "with open(script_json_path, \"w\") as json_file:\n",
    "    json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Script saved to {script_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 210 656\n",
      "3 656 1022\n",
      "4 1022 1657\n",
      "5 1657 1999\n",
      "6 1999 2833\n",
      "7 2833 2900\n",
      "8 2900 -1\n",
      "9 -1 -1\n",
      "10 -1 7835\n",
      "11 7835 8103\n",
      "12 8103 8747\n",
      "13 8747 5865\n",
      "14 5865 9894\n",
      "15 9894 13462\n",
      "16 13462 12269\n",
      "17 12269 14730\n",
      "18 14730 14964\n",
      "19 14964 -1\n",
      "20 -1 19210\n",
      "21 19210 17752\n",
      "22 17752 17823\n",
      "23 17823 4060\n",
      "24 4060 23009\n",
      "25 23009 19307\n",
      "26 19307 -1\n",
      "27 -1 15461\n",
      "28 15461 20192\n",
      "29 20192 24383\n",
      "Matched paragraphs saved to ./spms/test_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SPM = './spms'\n",
    "SCRIPT = './scripts'\n",
    "# Path to your script\n",
    "project_id = 'test'\n",
    "\n",
    "# Read the script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        return script_file.read()\n",
    "        # script_content = script_file.read()\n",
    "        # return json.loads(script_content)\n",
    "    \n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    matched_paragraphs = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "\n",
    "    script_content_lower = script_content.lower()\n",
    "\n",
    "    for i, page in enumerate(page_numbers):\n",
    "        current_sentence = first_sentences[page].lower()\n",
    "        next_sentence = first_sentences[str(int(page) + 1)].lower() if i < len(page_numbers) - 1 else None\n",
    "        prev_sentence = first_sentences[str(int(page) - 1)].lower() if i > 0 else None\n",
    "        \n",
    "        if i == 0:\n",
    "            try:\n",
    "                start_index = script_content_lower.find(current_sentence)\n",
    "                end_index = script_content_lower.find(next_sentence)\n",
    "                matched_paragraphs[page] = script_content[start_index:end_index].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "        elif i == len(page_numbers) - 1:\n",
    "            try:\n",
    "                start_index = script_content_lower.find(current_sentence)\n",
    "                matched_paragraphs[page] = script_content[start_index:].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "        else:\n",
    "            try:\n",
    "                start_index = script_content_lower.find(current_sentence)\n",
    "                end_index = script_content_lower.find(next_sentence)\n",
    "                print(page, start_index, end_index)\n",
    "                matched_paragraphs[page] = script_content[start_index:end_index].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, \"10_transcription.txt\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, \"test_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred at page 9\n",
      "Error occurred at page 10\n",
      "Error occurred at page 20\n",
      "Error occurred at page 27\n",
      "Matched paragraphs saved to ./spms/test_matched_paragraphs.json\n",
      "Object `수정해야할까` not found.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SPM = './spms'\n",
    "SCRIPT = './scripts'\n",
    "# Path to your script\n",
    "project_id = 'test'\n",
    "\n",
    "# Read the script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        return script_file.read()\n",
    "        # script_content = script_file.read()\n",
    "        # return json.loads(script_content)\n",
    "    \n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    matched_paragraphs = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "\n",
    "    for i, page in enumerate(page_numbers):\n",
    "        current_sentence = first_sentences[page]\n",
    "        next_page_number = str(int(page) + 1)\n",
    "        \n",
    "        if i == 0:\n",
    "            try:\n",
    "                matched_paragraphs[page] = script_content.split(first_sentences[next_page_number].lower())[0].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "        elif i == len(page_numbers) - 1:\n",
    "            try:\n",
    "                matched_paragraphs[page] = first_sentences[page].lower() + ' ' + script_content.split(first_sentences[page].lower())[1].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "        else:\n",
    "            try:\n",
    "                matched_paragraphs[page] = first_sentences[page].lower() + ' ' + script_content.split(first_sentences[page].lower())[1].split(first_sentences[next_page_number].lower())[0].strip()\n",
    "            except:\n",
    "                print(f\"Error occurred at page {page}\")\n",
    "    return matched_paragraphs\n",
    "\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, \"10_transcription.txt\")\n",
    "script_content = read_script(script_path).lower()\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, \"test_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all of our assignments are using python and numpy. if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed. but, this is, actually, pretty important. numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. and you'll get a lot of practice with this on the first assignment. so, for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial. \n"
     ]
    }
   ],
   "source": [
    "# print(script_content.split(first_sentences['2'])[0])\n",
    "print(first_sentences['4'].lower() + script_content.split(first_sentences['4'].lower())[1].split(first_sentences['5'].lower())[0])\n",
    "# print(first_sentences['29'] + script_content.split(first_sentences['29'])[1].split(first_sentences['30'])[0])\n",
    "# print(first_sentences['30'] + script_content.split(first_sentences['30'])[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Okay, so welcome to lecture two of CS231N.', '2': 'One, is Piazza.', '3': 'The next administrative issue is about assignment one.', '4': 'All of our assignments are using Python and NumPy.', '5': \"The other thing I wanted to talk about is that we're happy to announce that we're officially supported through Google Cloud for this class.\", '6': 'So, the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision.', '7': \"So, if you dig in and think about, actually, what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it.\", '8': 'So, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid, every single pixel, in this giant grid of numbers would be completely different.', '9': 'There can be different lighting conditions going on in the scene.', '10': 'Objects can also deform.', '11': 'There can also be problems of occlusion, where you might only see part of a cat, like, just the face, or in this extreme example, just a tail peeking out from under the couch cushion.', '12': 'There can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.', '13': \"There's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances.\", '14': \"If you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something like RSA encryption, you, sort of, can write down an algorithm and enumerate all the steps that need to happen in order for this things to work.\", '15': 'We touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses.', '16': 'So here our API has changed a little bit.', '17': 'The algorithm is pretty dumb, honestly.', '18': 'You might imagine working on this dataset called CIFAR-10, which is very commonly used in machine learning, as kind of a small test case.', '19': \"So here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR-10.\", '20': \"So, in the example in the previous slide, we've used what's called the L1 distance, also sometimes called the Manhattan distance.\", '21': \"So, here's some full Python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by NumPy.\", '22': \"And you can see that, we're actually able to do this in just one or two lines of Python code by utilizing these vectorized operations in NumPy.\", '23': 'So now, a couple questions about this simple classifier.', '24': \"Well, training is probably constant because we don't really need to do anything, we just need to memorize the data.\", '25': 'Because, in practice, we want our classifiers to be slow at training time and then fast at testing time.', '26': 'So, from this perspective, this nearest neighbor algorithm, is, actually, a little bit backwards.', '27': 'So, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point.', '28': \"So rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find K of our nearest neighbors, according to our distance metric, and then take a vote among each of our neighbors.\", '29': \"Here I've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor.\", '30': 'And you could imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.'}\n"
     ]
    }
   ],
   "source": [
    "print(first_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': \"okay, so welcome to lecture two of cs231n. on tuesday we, just recall, we, sort of, gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. and today, we're really going to dive in, for the first time, into the details. and we'll start to see, in much more depth, exactly how some of these learning algorithms actually work in practice. so, the first lecture of the class is probably, sort of, the largest big picture vision. and the majority of the lectures in this class will be much more detail orientated, much more focused on the specific mechanics, of these different algorithms. so, today we'll see our first learning algorithm and that'll be really exciting, i think. but, before we get to that, i wanted to talk about a couple of administrative issues.\", '2': \"one, is piazza.so, i saw it when i checked yesterday, it seemed like we had maybe 500 students signed up on piazza. which means that there are several hundred of you who are not yet there. so, we really want piazza to be the main source of communication between the students and the core staff. so, we've gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance. and, any, sort of, questions like that should really go to piazza. you'll probably get answers to your questions faster on piazza, because all the tas are knowing to check that. and it's, sort of, easy for emails to get lost in the shuffle if you just send to the course list. it's also come to my attention that some scpd students are having a bit of a hard time signing up for piazza. scpd students are supposed to receive a @stanford.edu email address. so, once you get that email address, then you can use the stanford email to sign into piazza. probably that doesn't affect those of you who are sitting in the room right now, but, for those students listening on scpd.\", '3': \"the next administrative issue is about assignment one.assignment one will be up later today, probably sometime this afternoon, but i promise, before i go to sleep tonight, it'll be up. but, if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. it'll be pretty much the same content. we're just reshuffling it a little bit to make it, like, for example, upgrading to work with python 3, rather than python 2.7. and some of these minor cosmetic changes, but the content of the assignment will still be the same as last year. so, in this assignment you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. you'll also implement several different linear classifiers, including the svm and softmax, as well as a simple two-layer neural network. and we'll cover all this content over the next couple of lectures. so,\", '4': \"all of our assignments are using python and numpy.if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed. but, this is, actually, pretty important. numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. and you'll get a lot of practice with this on the first assignment. so, for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\", '5': \"the other thing i wanted to talk about is that we're happy to announce that we're officially supported through google cloud for this class.so, google cloud is somewhat similar to amazon aws. you can go and start virtual machines up in the cloud. these virtual machines can have gpus. we're working on the tutorial for exactly how to use google cloud and get it to work for the assignments. but our intention is that you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. and because google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use google cloud credits for free for the class. so you can feel free to use these for the assignments and also for the course projects when you want to start using gpus and larger machines and whatnot. so, we'll post more details about that, probably, on piazza later today. but, i just wanted to mention, because i know there had been a couple of questions about, can i use my laptop? do i have to run on corn? do i have to, whatever? and the answer is that, you'll be able to run on google cloud and we'll provide you some coupons for that. yeah, so, those are, kind of, the major administrative issues i wanted to talk about today. and then, let's dive into the content.\", '6': \"so, the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision.and this is something that we'll really focus on throughout the course of the class. is, exactly, how do we work on this image classification task? so, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware of some predetermined set of categories or labels. so, these might be, like, a dog or a cat or a truck or a plane, and there's some fixed set of category labels, and the job of the computer is to look at the picture and assign it one of these fixed category labels. this seems like a really easy problem, because so much of your own visual system in your brain is hardwired to doing these, sort of, visual recognition tasks. but this is actually a really, really hard problem for a machine.\", '7': \"so, if you dig in and think about, actually, what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it.and the computer really is representing the image as this gigantic grid of numbers. so, the image might be something like 800 by 600 pixels. and each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. so, to the computer, this is just a gigantic grid of numbers. and it's very difficult to distill the cat-ness out of this, like, giant array of thousands, or whatever, very many different numbers. so, we refer to this problem as the semantic gap. this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image, and there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. and this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\", '8': \"so, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid, every single pixel, in this giant grid of numbers would be completely different.but, somehow, it's still representing the same cat. and our algorithms need to be robust to this. but, not only viewpoint is one problem, another is illumination.\", '9': \"there can be different lighting conditions going on in the scene.whether the cat is appearing in this very dark, moody scene, or like is this very bright, sunlit scene, it's still a cat, and our algorithms need to be robust to that.\", '10': 'objects can also deform.i think cats are, maybe, among the more deformable of animals that you might see out there. and cats can really assume a lot of different, varied poses and positions. and our algorithms should be robust to these different kinds of transforms.', '11': \"there can also be problems of occlusion, where you might only see part of a cat, like, just the face, or in this extreme example, just a tail peeking out from under the couch cushion.but, in these cases, it's pretty easy for you, as a person, to realize that this is probably a cat, and you still recognize these images as cats. and this is something that our algorithms also must be robust to, which is quite difficult, i think.\", '12': 'there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.and this is another thing that we need to handle.', '13': \"there's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances.and cats can come in different shapes and sizes and colors and ages. and our algorithm, again, needs to work and handle all these different variations. so, this is actually a really, really challenging problem. and it's sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things. but now if we want our computer programs to deal with all of these problems, all simultaneously, and not just for cats, by the way, but for just about any object category you can imagine, this is a fantastically challenging problem. and it's, actually, somewhat miraculous that this works at all, in my opinion. but, actually, not only does it work, but these things work very close to human accuracy in some limited situations. and take only hundreds of milliseconds to do so. so, this is some pretty amazing, incredible technology, in my opinion, and over the course of the rest of the class we will really see what kinds of advancements have made this possible. so now, if you, kind of, think about what is the api for writing an image classifier, you might sit down and try to write a method in python like this. where you want to take in an image and then do some crazy magic and then, eventually, spit out this class label to say cat or dog or whatnot. and there's really no obvious way to do this, right?\", '14': \"if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something like rsa encryption, you, sort of, can write down an algorithm and enumerate all the steps that need to happen in order for this things to work.but, when we're trying to recognize objects, or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense, for how you might go about recognizing these objects. so, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down and write this function, i think most people would be in trouble. that being said, people have definitely made explicit attempts to try to write, sort of, high-end coded rules for recognizing different animals. so,\", '15': \"we touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses.and we know that edges, from hubel and wiesel, we know that edges are pretty important when it comes to visual recognition. so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats. but this turns out not to work very well. one, it's super brittle. and, two, say, if you want to start over for another object category, and maybe not worry about cats, but talk about trucks or dogs or fishes or something else, then you need to start all over again. so, this is really not a very scalable approach. we want to come up with some algorithm, or some method, for these recognition tasks which scales much more naturally to all the variety of objects in the world. so, the insight that, sort of, makes this all work is this idea of the data-driven approach. rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this. and we can actually use tools like google image search, or something like that, to go out and collect a very large number of examples of these different categories. by the way, this actually takes quite a lot of effort to go out and actually collect these datasets but, luckily, there's a lot of really good, high quality datasets out there already for you to use. then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. then finally, we'll use this training model and apply it on new images that will then be able to recognize cats and dogs and whatnot.\", '16': \"so here our api has changed a little bit.rather than a single function that just inputs an image and recognizes a cat, we have these two functions. one, called, train, that's going to input images and labels and then output a model, and then, separately, another function called, predict, which will input the model and than make predictions for images. and this is, kind of, the key insight that allowed all these things to start working really well over the last 10, 20 years or so. so, this class is primarily about neural networks and convolutional neural networks and deep learning and all that, but this idea of a data-driven approach is much more general than just deep learning. and i think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. so, probably, the simplest classifier you can imagine is something we call nearest neighbor.\", '17': \"the algorithm is pretty dumb, honestly.so, during the training step we won't do anything, we'll just memorize all of the training data. so this is very simple. and now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image. a very simple algorithm. but it, sort of, has a lot of these nice properties with respect to data-drivenness and whatnot. so, to be a little bit more concrete,\", '18': \"you might imagine working on this dataset called cifar-10, which is very commonly used in machine learning, as kind of a small test case.and you'll be working with this dataset on your homework. so, the cifar-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. and for each of those 10 categories it provides 50,000 training images, roughly evenly distributed across these 10 categories. and then 10,000 additional testing images that you're supposed to test your algorithm on.\", '19': \"so here's an example of applying this simple nearest neighbor classifier to some of these test images on cifar-10.so, on this grid on the right, for the left most column, gives a test image in the cifar-10 dataset. and now on the right, we've sorted the training images and show the most similar training images to each of these test examples. and you can see that they look kind of visually similar to the training images, although they are not always correct, right? so, maybe on the second row, we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. but, this image is a dog and it's nearest neighbor is also a dog, but this next one, i think is actually a deer or a horse or something else. but, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. so, if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set. and now, the closest example, we know it's label, because it comes from the training set. and now, we'll simply say that this testing image is also a dog. you can see from these examples that is probably not going to work very well, but it's still kind of a nice example to work through. but then, one detail that we need to know is, given a pair of images, how can we actually compare them? because, if we're going to take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\", '20': \"so, in the example in the previous slide, we've used what's called the l1 distance, also sometimes called the manhattan distance.so, this is a really sort of simple, easy idea for comparing images. and that's that we're going to just compare individual pixels in these images. so, supposing that our test image is maybe just a tiny four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. and then, sum all these up across all the pixels in the image. so, this is kind of a stupid way to compare images, but it does some reasonable things sometimes. but, this gives us a very concrete way to measure the difference between two images. and in this case, we have this difference of 456 between these two images.\", '21': \"so, here's some full python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by numpy.so, here we can see that this training function, that we talked about earlier, is, again, very simple, in the case of nearest neighbor, you just memorize the training data, there's not really much to do here. and now, at test time, we're going to take in our image and then go in and compare using this l1 distance function, our test image to each of these training examples and find the most similar example in the training set.\", '22': \"and you can see that, we're actually able to do this in just one or two lines of python code by utilizing these vectorized operations in numpy.so, this is something that you'll get practice with on the first assignment.\", '23': 'so now, a couple questions about this simple classifier.first, if we have n examples in our training set, then how fast can we expect training and testing to be?', '24': \"well, training is probably constant because we don't really need to do anything, we just need to memorize the data.and if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. but now, at test time we need to do this comparison stop and compare our test image to each of the n training examples in the dataset. and this is actually quite slow. so, this is actually somewhat backwards, if you think about it.\", '25': 'because, in practice, we want our classifiers to be slow at training time and then fast at testing time.because, you might imagine, that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good. but then, when you go and deploy the classifier at test time, you want it to run on your mobile phone or in a browser or some other low power device, and you really want the testing time performance of your classifier to be quite fast.', '26': \"so, from this perspective, this nearest neighbor algorithm, is, actually, a little bit backwards.and we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. so then, the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? so, here we've drawn, what we call the decision regions of a nearest neighbor classifier.\", '27': \"so, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point.so, here we see we have five classes and some blue ones up in the corner here, some purple ones in the upper-right hand corner. and now for each pixel in this entire plane, we've gone and computed what is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. so, you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points. but this classifier is maybe not so great. and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier. for one, this central region actually contains mostly green points, but one little yellow point in the middle. but because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of this green cluster. and that's, maybe, not so great. maybe those points actually should have been green. and then, similarly we also see these, sort of, fingers, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. so, this kind of motivates a slight generalization of this algorithm called k-nearest neighbors.\", '28': \"so rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find k of our nearest neighbors, according to our distance metric, and then take a vote among each of our neighbors.and then predict the majority vote among our neighbors. you can imagine slightly more complex ways of doing this. maybe you'd vote weighted on the distance, or something like that, but the simplest thing that tends to work pretty well is just taking a majority vote. so here we've shown the exact same set of points using this k=1 nearest neighbor classifier, as well as k=3 and k=5 in the middle and on the right. and once we move to k=3, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. now this entire green portion in the middle is all being classified as green. you can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. and then, once we move to the k=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. so, generally when you're using nearest neighbors classifiers, you almost always want to use some value of k, which is larger than one because this tends to smooth out your decision boundaries and lead to better results. question? [student asking a question] yes, so the question is, what is the deal with these white regions? the white regions are where there was no majority among the k-nearest neighbors. you could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners, but for this simple example we're just coloring it white to indicate there was no nearest neighbor in those points. whenever we're thinking about computer vision i think it's really useful to kind of flip back and forth between several different viewpoints. one, is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images. because the pixels of the image actually allow us to think of these images as high dimensional vectors. and it's sort of useful to ping pong back and forth between these two different viewpoints. so then, sort of taking this k-nearest neighbor and going back to the images you can see that it's actually not very good.\", '29': \"here i've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor.and you can see that it's really not very good. but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row.\", '30': 'and you could imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.'}\n"
     ]
    }
   ],
   "source": [
    "matched_paragraphs = {}\n",
    "page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "\n",
    "for i, page in enumerate(page_numbers):\n",
    "    current_sentence = first_sentences[page]\n",
    "    next_page_number = str(int(page) + 1)\n",
    "    \n",
    "    if i == 0:\n",
    "        matched_paragraphs[page] = script_content.split(first_sentences[next_page_number].lower())[0].strip()\n",
    "    elif i == len(page_numbers) - 1:\n",
    "        matched_paragraphs[page] = first_sentences[page].lower() + script_content.split(first_sentences[page].lower())[1].strip()\n",
    "    else:\n",
    "        matched_paragraphs[page] = first_sentences[page].lower() + script_content.split(first_sentences[page].lower())[1].split(first_sentences[next_page_number].lower())[0].strip()\n",
    "      \n",
    "    \n",
    "print(matched_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ./images/11/page_0001.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"okay, so welcome to lecture two of cs231n.\",\n",
      "      \"bbox\": [0, 0, 1824, 229]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"on tuesday we, just recall, we, sort of, gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and today, we're really going to dive in, for the first time, into the details.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and we'll start to see, in much more depth, exactly how some of these learning algorithms actually work in practice.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, the first lecture of the class is probably, sort of, the largest big picture vision.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and the majority of the lectures in this class will be much more detail orientated, much more focused on the specific mechanics, of these different algorithms.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, today we'll see our first learning algorithm and that'll be really exciting, i think.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, before we get to that, i wanted to talk about a couple of administrative issues.\",\n",
      "      \"bbox\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 1 saved to ./bboxs/11/1_spm.json\n",
      "2 ./images/11/page_0002.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"one, is piazza.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, i saw it when i checked yesterday, it seemed like we had maybe 500 students signed up on piazza.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"which means that there are several hundred of you who are not yet there.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, we really want piazza to be the main source of communication between the students and the core staff.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, we've gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and, any, sort of, questions like that should really go to piazza.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you'll probably get answers to your questions faster on piazza, because all the tas are knowing to check that.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and it's, sort of, easy for emails to get lost in the shuffle if you just send to the course list.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"it's also come to my attention that some scpd students are having a bit of a hard time signing up for piazza.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"scpd students are supposed to receive a @stanford.edu email address.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, once you get that email address, then you can use the stanford email to sign into piazza.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"probably that doesn't affect those of you who are sitting in the room right now, but, for those students listening on scpd.\",\n",
      "      \"bbox\": [0, 0, 0, 0]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"For questions about midterm, poster session, projects, use Piazza instead of staff list!\",\n",
      "      \"bbox\": [156, 762, 1052, 87]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"SCPD students: Use your @stanford.edu address to register for Piazza; contact scpd-customerservice@stanford.edu for help.\",\n",
      "      \"bbox\": [97, 977, 1164, 45]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 2 saved to ./bboxs/11/2_spm.json\n",
      "3 ./images/11/page_0003.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"the next administrative issue is about assignment one.\",\n",
      "      \"bbox\": [0, 0, 600, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"assignment one will be up later today, probably sometime this afternoon, but i promise, before i go to sleep tonight, it'll be up.\",\n",
      "      \"bbox\": [0, 0, 600, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"it'll be pretty much the same content. we're just reshuffling it a little bit to make it, like, for example, upgrading to work with python 3, rather than python 2.7.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and some of these minor cosmetic changes, but the content of the assignment will still be the same as last year.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, in this assignment you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture.\",\n",
      "      \"bbox\": [0, 100, 600, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you'll also implement several different linear classifiers, including the svm and softmax, as well as a simple two-layer neural network.\",\n",
      "      \"bbox\": [0, 100, 600, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and we'll cover all this content over the next couple of lectures.\",\n",
      "      \"bbox\": [0, 100, 600, 200]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 3 saved to ./bboxs/11/3_spm.json\n",
      "4 ./images/11/page_0004.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"all of our assignments are using python and numpy.\",\n",
      "      \"bbox\": [0, 92, 960, 422]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed.\",\n",
      "      \"bbox\": [0, 92, 960, 422]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and you'll get a lot of practice with this on the first assignment.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\",\n",
      "      \"bbox\": [0, 92, 960, 422]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 4 saved to ./bboxs/11/4_spm.json\n",
      "5 ./images/11/page_0005.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"we're happy to announce that we're officially supported through google cloud for this class.\",\n",
      "      \"bbox\": [0, 0, 160, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, google cloud is somewhat similar to amazon aws.\",\n",
      "      \"bbox\": [0, 40, 200, 80]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"these virtual machines can have gpus.\",\n",
      "      \"bbox\": [0, 80, 200, 120]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but our intention is that you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud.\",\n",
      "      \"bbox\": [0, 120, 300, 160]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and because google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use google cloud credits for free for the class.\",\n",
      "      \"bbox\": [0, 160, 320, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"we'll post more details about that, probably, on piazza later today.\",\n",
      "      \"bbox\": [0, 200, 280, 240]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"i just wanted to mention, because i know there had been a couple of questions about, can i use my laptop? do i have to run on corn? do i have to, whatever?\",\n",
      "      \"bbox\": [0, 240, 350, 280]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and the answer is that, you'll be able to run on google cloud and we'll provide you some coupons for that.\",\n",
      "      \"bbox\": [0, 280, 320, 320]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 5 saved to ./bboxs/11/5_spm.json\n",
      "6 ./images/11/page_0006.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so, the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and this is something that we'll really focus on throughout the course of the class.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"is, exactly, how do we work on this image classification task?\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example,\",\n",
      "      \"bbox\": [61, 160, 418, 518]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and the system is aware of some predetermined set of categories or labels.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, these might be, like, a dog or a cat or a truck or a plane, and there's some fixed set of category labels,\",\n",
      "      \"bbox\": [664, 169, 236, 67]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and the job of the computer is to look at the picture and assign it one of these fixed category labels.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"this seems like a really easy problem, because so much of your own visual system in your brain is hardwired to doing these, sort of, visual recognition tasks.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but this is actually a really, really hard problem for a machine.\",\n",
      "      \"bbox\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 6 saved to ./bboxs/11/6_spm.json\n",
      "7 ./images/11/page_0007.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"actually, what does a computer see when it looks at this image,\",\n",
      "      \"bbox\": [435, 87, 410, 62]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"it definitely doesn't get this holistic idea of a cat that you see when you look at it.\",\n",
      "      \"bbox\": [9, 57, 462, 533]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and the computer really is representing the image as this gigantic grid of numbers.\",\n",
      "      \"bbox\": [910, 57, 421, 189]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, the image might be something like 800 by 600 pixels.\",\n",
      "      \"bbox\": [730, 369, 215, 29]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and each pixel is represented by three numbers, giving the red, green, and blue values for that pixel.\",\n",
      "      \"bbox\": [920, 555, 417, 24]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"to the computer, this is just a gigantic grid of numbers.\"\n",
      "      ,\n",
      "      \"bbox\": [910, 57, 421, 189]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and it's very difficult to distill the cat-ness out of this, like, giant array of thousands,\",\n",
      "      \"bbox\": [9, 57, 462, 533]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\",\n",
      "      \"bbox\": [9, 57, 462, 533]\n",
      "      }\n",
      "   ]\n",
      "}\n",
      "Script for page 7 saved to ./bboxs/11/7_spm.json\n",
      "8 ./images/11/page_0008.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so, for example, if we took this same cat,\",\n",
      "      \"bbox\": [366, 174, 372, 478]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen,\",\n",
      "      \"bbox\": [366, 174, 372, 478]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but we moved the camera to the other side, then every single grid,\",\n",
      "      \"bbox\": [42, 108, 122, 82]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"every single pixel, in this giant grid of numbers would be completely different.\",\n",
      "      \"bbox\": [484, 172, 378, 110]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, somehow, it's still representing the same cat.\",\n",
      "      \"bbox\": [366, 174, 372, 478]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and our algorithms need to be robust to this.\",\n",
      "      \"bbox\": [0, 1000, 0, 0] \n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, not only viewpoint is one problem, another is illumination.\",\n",
      "      \"bbox\": [0, 1000, 0, 0]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 8 saved to ./bboxs/11/8_spm.json\n",
      "9 ./images/11/page_0009.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"there can be different lighting conditions going on in the scene.\",\n",
      "      \"bbox\": [10, 250, 330, 440]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"whether the cat is appearing in this very dark, moody scene, or like is this very bright, sunlit scene, it's still a cat,\",\n",
      "      \"bbox\": [350, 250, 980, 440]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and our algorithms need to be robust to that.\",\n",
      "      \"bbox\": [10, 250, 1320, 450]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 9 saved to ./bboxs/11/9_spm.json\n",
      "10 ./images/11/page_0010.png\n",
      "\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"objects can also deform.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"i think cats are, maybe, among the more deformable of animals that you might see out there.\",\n",
      "      \"bbox\": [0, 70, 1365, 512]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and cats can really assume a lot of different, varied poses and positions.\",\n",
      "      \"bbox\": [0, 70, 1365, 512]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and our algorithms should be robust to these different kinds of transforms.\",\n",
      "      \"bbox\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 10 saved to ./bboxs/11/10_spm.json\n",
      "11 ./images/11/page_0011.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"or in this extreme example, just a tail peeking out from under the couch cushion.\",\n",
      "      \"bbox\": [1064, 480, 150, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"like, just the face,\",\n",
      "      \"bbox\": [116, 252, 300, 300]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"where you might only see part of a cat,\",\n",
      "      \"bbox\": [530, 300, 300, 300]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 11 saved to ./bboxs/11/11_spm.json\n",
      "12 ./images/11/page_0012.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.\",\n",
      "      \"bbox\": [40, 150, 550, 405]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.\",\n",
      "      \"bbox\": [730, 150, 550, 405]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 12 saved to ./bboxs/11/12_spm.json\n",
      "13 ./images/11/page_0013.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"there's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances.\",\n",
      "      \"bbox\": [0, 0, 1365, 60]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and cats can come in different shapes and sizes and colors and ages.\",\n",
      "      \"bbox\": [0, 280, 1365, 720]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and our algorithm, again, needs to work and handle all these different variations.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is actually a really, really challenging problem.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and it's sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but now if we want our computer programs to deal with all of these problems, all simultaneously, and not just for cats, by the way, but for just about any object category you can imagine, this is a fantastically challenging problem.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and it's, actually, somewhat miraculous that this works at all, in my opinion.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, actually, not only does it work, but these things work very close to human accuracy in some limited situations.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and take only hundreds of milliseconds to do so.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is some pretty amazing, incredible technology, in my opinion, and over the course of the rest of the class we will really see what kinds of advancements have made this possible.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so now, if you, kind of, think about what is the api for writing an image classifier, you might sit down and try to write a method in python like this.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"where you want to take in an image and then do some crazy magic and then, eventually, spit out this class label to say cat or dog or whatnot.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and there's really no obvious way to do this, right?\",\n",
      "      \"bbox\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 13 saved to ./bboxs/11/13_spm.json\n",
      "14 ./images/11/page_0014.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something like rsa encryption, you, sort of, can write down an algorithm and enumerate all the steps that need to happen in order for this things to work.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, when we're trying to recognize objects, or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense, for how you might go about recognizing these objects.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down and write this function, i think most people would be in trouble.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"that being said, people have definitely made explicit attempts to try to write, sort of, high-end coded rules for recognizing different animals.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"An image classifier\",\n",
      "      \"bbox\": [210, 40, 406, 60]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"def classify_image(image):\",\n",
      "      \"bbox\": [367, 165, 172, 22]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"# Some magic here?\",\n",
      "      \"bbox\": [367, 187, 148, 19]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"return class_label\",\n",
      "      \"bbox\": [367, 206, 129, 19]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Unlike e.g. sorting a list of numbers,\",\n",
      "      \"bbox\": [274, 290, 315, 22]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"no obvious way to hard-code the algorithm for recognizing a cat, or other classes.\",\n",
      "      \"bbox\": [174, 330, 515, 46]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Fei-Fei Li & Justin Johnson & Serena Yeung\",\n",
      "      \"bbox\": [15, 579, 485, 28]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Lecture 2 - 14\",\n",
      "      \"bbox\": [636, 579, 130, 28]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"April 6, 2017\",\n",
      "      \"bbox\": [1262, 579, 85, 28]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 14 saved to ./bboxs/11/14_spm.json\n",
      "15 ./images/11/page_0015.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"we touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses.\",\n",
      "      \"bbox\": [36, 92, 255, 358]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and we know that edges, from hubel and wiesel, we know that edges are pretty important when it comes to visual recognition.\",\n",
      "      \"bbox\": [412, 87, 228, 368]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats.\",\n",
      "      \"bbox\": [36, 92, 255, 358]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats.\",\n",
      "      \"bbox\": [412, 87, 228, 368]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but this turns out not to work very well. one, it's super brittle.\",\n",
      "      \"bbox\": [412, 87, 228, 368]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and, two, say, if you want to start over for another object category, and maybe not worry about cats, but talk about trucks or dogs or fishes or something else, then you need to start all over again. so, this is really not a very scalable approach.\",\n",
      "      \"bbox\": [412, 87, 228, 368]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 15 saved to ./bboxs/11/15_spm.json\n",
      "16 ./images/11/page_0016.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so here our api has changed a little bit.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"rather than a single function that just inputs an image and recognizes a cat, we have these two functions.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"one, called, train, that's going to input images and labels and then output a model,\",\n",
      "      \"bbox\": [35, 210, 298, 68]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then, separately, another function called, predict, which will input the model and than make predictions for images.\",\n",
      "      \"bbox\": [35, 311, 354, 57]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and this is, kind of, the key insight that allowed all these things to start working really well over the last 10, 20 years or so.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this class is primarily about neural networks and convolutional neural networks and deep learning and all that,\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but this idea of a data-driven approach is much more general than just deep learning.\",\n",
      "      \"bbox\": [35, 5, 381, 52]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and i think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, probably, the simplest classifier you can imagine is something we call nearest neighbor.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Collect a dataset of images and labels\",\n",
      "      \"bbox\": [35, 92, 340, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Use Machine Learning to train a classifier\",\n",
      "      \"bbox\": [35, 132, 392, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Evaluate the classifier on new images\",\n",
      "      \"bbox\": [35, 172, 340, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Example training set\",\n",
      "      \"bbox\": [620, 92, 246, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"airplane\",\n",
      "      \"bbox\": [598, 146, 92, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"automobile\",\n",
      "      \"bbox\": [598, 174, 102, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"bird\",\n",
      "      \"bbox\": [598, 202, 52, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"cat\",\n",
      "      \"bbox\": [598, 230, 44, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"deer\",\n",
      "      \"bbox\": [598, 258, 52, 30]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 16 saved to ./bboxs/11/16_spm.json\n",
      "17 ./images/11/page_0017.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"the algorithm is pretty dumb, honestly.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, during the training step we won't do anything, we'll just memorize all of the training data.\",\n",
      "      \"bbox\": [400, 200, 250, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so this is very simple.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image.\",\n",
      "      \"bbox\": [400, 340, 260, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"a very simple algorithm.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but it, sort of, has a lot of these nice properties with respect to data-drivenness and whatnot.\",\n",
      "      \"bbox\": []\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, to be a little bit more concrete,\",\n",
      "      \"bbox\": []\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 17 saved to ./bboxs/11/17_spm.json\n",
      "18 ./images/11/page_0018.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"you might imagine working on this dataset called cifar-10, which is very commonly used in machine learning, as kind of a small test case.\",\n",
      "      \"bbox\": [20, 20, 150, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and you'll be working with this dataset on your homework.\",\n",
      "      \"bbox\": [20, 20, 150, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, the cifar-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that.\",\n",
      "      \"bbox\": [20, 70, 70, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and for each of those 10 categories it provides 50,000 training images, roughly evenly distributed across these 10 categories.\",\n",
      "      \"bbox\": [20, 70, 70, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then 10,000 additional testing images that you're supposed to test your algorithm on.\",\n",
      "      \"bbox\": [20, 120, 70, 80]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 18 saved to ./bboxs/11/18_spm.json\n",
      "19 ./images/11/page_0019.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so here's an example of applying this simple nearest neighbor classifier to some of these test images on cifar-10.\",\n",
      "      \"bbox\": [10, 10, 400, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, on this grid on the right, for the left most column, gives a test image in the cifar-10 dataset.\",\n",
      "      \"bbox\": [740, 20, 600, 900]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and now on the right, we've sorted the training images and show the most similar training images to each of these test examples.\",\n",
      "      \"bbox\": [760, 30, 580, 880]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and you can see that they look kind of visually similar to the training images, although they are not always correct, right?\",\n",
      "      \"bbox\": [760, 30, 580, 880]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, maybe on the second row, we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess.\",\n",
      "      \"bbox\": [758, 252, 174, 72]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, this image is a dog and it's nearest neighbor is also a dog, but this next one, i think is actually a deer or a horse or something else.\",\n",
      "      \"bbox\": [758, 252, 174, 72]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot.\",\n",
      "      \"bbox\": [758, 252, 174, 72]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set.\",\n",
      "      \"bbox\": [10, 40, 720, 924]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and now, the closest example, we know it's label, because it comes from the training set.\",\n",
      "      \"bbox\": [10, 40, 720, 924]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and now, we'll simply say that this testing image is also a dog.\",\n",
      "      \"bbox\": [758, 252, 174, 72]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you can see from these examples that is probably not going to work very well, but it's still kind of a nice example to work through.\",\n",
      "      \"bbox\": [10, 40, 720, 924]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but then, one detail that we need to know is, given a pair of images, how can we actually compare them?\",\n",
      "      \"bbox\": [10, 40, 720, 924]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"because, if we're going to take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\",\n",
      "      \"bbox\": [10, 40, 720, 924]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 19 saved to ./bboxs/11/19_spm.json\n",
      "20 ./images/11/page_0020.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so, in the example in the previous slide, we've used what's called the l1 distance, also sometimes called the manhattan distance.\",\n",
      "      \"bbox\": [20, 20, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is a really sort of simple, easy idea for comparing images.\",\n",
      "      \"bbox\": [20, 60, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and that's that we're going to just compare individual pixels in these images.\",\n",
      "      \"bbox\": [20, 100, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, supposing that our test image is maybe just a tiny four by four image of pixel values,\",\n",
      "      \"bbox\": [20, 140, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"then we're take this upper-left hand pixel of the test image, subtract off the value in the training image, take the absolute value,\",\n",
      "      \"bbox\": [20, 180, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and get the difference in that pixel between the two images.\",\n",
      "      \"bbox\": [20, 220, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then, sum all these up across all the pixels in the image.\",\n",
      "      \"bbox\": [20, 260, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is kind of a stupid way to compare images, but it does some reasonable things sometimes.\",\n",
      "      \"bbox\": [20, 300, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but, this gives us a very concrete way to measure the difference between two images.\",\n",
      "      \"bbox\": [20, 340, 250, 40]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and in this case, we have this difference of 456 between these two images.\",\n",
      "      \"bbox\": [20, 380, 250, 40]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 20 saved to ./bboxs/11/20_spm.json\n",
      "21 ./images/11/page_0021.png\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, here's some full python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by numpy.\",\n",
      "            \"bbox\": [10, 10, 250, 40]\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, here we can see that this training function, that we talked about earlier, is, again, very simple, in the case of nearest neighbor, you just memorize the training data, there's not really much to do here.\",\n",
      "            \"bbox\": [10, 117, 640, 120]\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now, at test time, we're going to take in our image and then go in and compare using this l1 distance function, our test image to each of these training examples and find the most similar example in the training set.\",\n",
      "            \"bbox\": [10, 258, 640, 240]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Script for page 21 saved to ./bboxs/11/21_spm.json\n",
      "22 ./images/11/page_0022.png\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"and you can see that, we're actually able to do this in just one or two lines of python code by utilizing these vectorized operations in numpy.\",\n",
      "            \"bbox\": [14, 10, 535, 871]\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, this is something that you'll get practice with on the first assignment.\",\n",
      "            \"bbox\": []\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Script for page 22 saved to ./bboxs/11/22_spm.json\n",
      "23 ./images/11/page_0023.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"Nearest Neighbor classifier\",\n",
      "      \"bbox\": [850, 50, 250, 30]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"For each test image: Find closest train image Predict label of nearest image\",\n",
      "      \"bbox\": [850, 350, 300, 130]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 23 saved to ./bboxs/11/23_spm.json\n",
      "24 ./images/11/page_0024.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"well, training is probably constant because we don't really need to do anything, we just need to memorize the data.\",\n",
      "      \"bbox\": [853, 256, 400, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and if you're just copying a pointer, that's going to be constant time no matter how big your dataset is.\",\n",
      "      \"bbox\": [853, 256, 400, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but now, at test time we need to do this comparison stop and compare our test image to each of the n training examples in the dataset.\",\n",
      "      \"bbox\": [853, 256, 400, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and this is actually quite slow.\",\n",
      "      \"bbox\": [853, 256, 400, 200]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this is actually somewhat backwards, if you think about it.\",\n",
      "      \"bbox\": [853, 256, 400, 200]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 24 saved to ./bboxs/11/24_spm.json\n",
      "25 ./images/11/page_0025.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"because, in practice, we want our classifiers to be slow at training time and then fast at testing time.\",\n",
      "      \"bbox\": [800, 30, 500, 60]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"because, you might imagine, that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good.\",\n",
      "      \"bbox\": [800, 90, 500, 180]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but then, when you go and deploy the classifier at test time, you want it to run on your mobile phone or in a browser or some other low power device, and you really want the testing time performance of your classifier to be quite fast.\",\n",
      "      \"bbox\": [800, 270, 500, 210]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 25 saved to ./bboxs/11/25_spm.json\n",
      "26 ./images/11/page_0026.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"Nearest Neighbor classifier\",\n",
      "      \"bbox\": [900, 100, 300, 50]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Q: With N examples, how fast are training and prediction?\",\n",
      "      \"bbox\": [900, 150, 300, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"A: Train O(1), predict O(N)\",\n",
      "      \"bbox\": [900, 250, 300, 50]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"This is bad: we want classifiers that are fast at prediction; slow for training is ok\",\n",
      "      \"bbox\": [900, 300, 300, 100]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Fei-Fei Li & Justin Johnson & Serena Yeung\",\n",
      "      \"bbox\": [100, 650, 500, 50]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"Lecture 2 - 26\",\n",
      "      \"bbox\": [600, 650, 150, 50]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"April 6, 2017\",\n",
      "      \"bbox\": [1200, 650, 150, 50]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 26 saved to ./bboxs/11/26_spm.json\n",
      "27 ./images/11/page_0027.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, here we see we have five classes and some blue ones up in the corner here, some purple ones in the upper-right hand corner.\",\n",
      "      \"bbox\": [256, 0, 256, 128]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and now for each pixel in this entire plane, we've gone and computed what is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but this classifier is maybe not so great.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"for one, this central region actually contains mostly green points, but one little yellow point in the middle.\",\n",
      "      \"bbox\": [224, 160, 64, 64]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"but because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of this green cluster.\",\n",
      "      \"bbox\": [224, 160, 64, 64]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and that's, maybe, not so great. maybe those points actually should have been green.\",\n",
      "      \"bbox\": [224, 160, 64, 64]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then, similarly we also see these, sort of, fingers, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious.\",\n",
      "      \"bbox\": [64, 128, 128, 64]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, this kind of motivates a slight generalization of this algorithm called k-nearest neighbors.\",\n",
      "      \"bbox\": [0, 0, 512, 384]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 27 saved to ./bboxs/11/27_spm.json\n",
      "28 ./images/11/page_0028.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"so rather than just looking for the single nearest neighbor, instead we'll do something a little bit fancier and find k of our nearest neighbors, according to our distance metric, and then take a vote among each of our neighbors.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then predict the majority vote among our neighbors.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you can imagine slightly more complex ways of doing this.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"maybe you'd vote weighted on the distance, or something like that, but the simplest thing that tends to work pretty well is just taking a majority vote.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so here we've shown the exact same set of points using this k=1 nearest neighbor classifier, as well as k=3 and k=5 in the middle and on the right.\",\n",
      "      \"bbox\": [73, 190, 1175, 448]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and once we move to k=3, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow.\",\n",
      "      \"bbox\": [465, 256, 225, 192]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"now this entire green portion in the middle is all being classified as green.\",\n",
      "      \"bbox\": [632, 293, 93, 115]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting.\",\n",
      "      \"bbox\": [270, 330, 250, 160]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and then, once we move to the k=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice.\",\n",
      "      \"bbox\": [860, 256, 275, 192]\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so, generally when you're using nearest neighbors classifiers, you almost always want to use some value of k, which is larger than one because this tends to smooth out your decision boundaries and lead to better results.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"question?\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"[student asking a question]\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"yes, so the question is, what is the deal with these white regions?\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"the white regions are where there was no majority among the k-nearest neighbors.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"you could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners, but for this simple example we're just coloring it white to indicate there was no nearest neighbor in those points.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"whenever we're thinking about computer vision i think it's really useful to kind of flip back and forth between several different viewpoints.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"one, is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"because the pixels of the image actually allow us to think of these images as high dimensional vectors.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"and it's sort of useful to ping pong back and forth between these two different viewpoints.\",\n",
      "      \"bbox\": null\n",
      "    },\n",
      "    {\n",
      "      \"script\": \"so then, sort of taking this k-nearest neighbor and going back to the images you can see that it's actually not very good.\",\n",
      "      \"bbox\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 28 saved to ./bboxs/11/28_spm.json\n",
      "29 ./images/11/page_0029.png\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"here i've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor.\",\n",
      "            \"bbox\": [50, 60, 765, 400]\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and you can see that it's really not very good.\",\n",
      "            \"bbox\": [50, 60, 765, 400]\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row.\",\n",
      "            \"bbox\": [50, 60, 765, 400]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Script for page 29 saved to ./bboxs/11/29_spm.json\n",
      "30 ./images/11/page_0030.png\n",
      "{\n",
      "  \"bboxes\": [\n",
      "    {\n",
      "      \"script\": \"and you could imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.\",\n",
      "      \"bbox\": [120, 128, 1300, 640]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Script for page 30 saved to ./bboxs/11/30_spm.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "IMAGE = './images'\n",
    "SCRIPT = './scripts'\n",
    "SPM = './spms'\n",
    "BBOX = './bboxs'\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "   \n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        return json.load(script_file)\n",
    "\n",
    "# Path to your image and script\n",
    "project_id = 11\n",
    "image_directory = os.path.join(IMAGE, f\"{str(project_id)}\")\n",
    "script_path = os.path.join(SPM, \"test_matched_paragraphs.json\")\n",
    "bbox_path = os.path.join(BBOX, f\"{str(project_id)}\")\n",
    "os.makedirs(bbox_path, exist_ok=True)\n",
    "\n",
    "image_paths = sorted([os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith('.png')])\n",
    "\n",
    "# Read the script\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Function to make API request for each page\n",
    "def make_api_request(page_number, script_segment, encoded_image):    \n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                # \"Given the following lecture notes image and the corresponding lecture script, \"\n",
    "                # \"please provide bounding box information for each relevant script sentence. \"\n",
    "                \"Tell me specifically where each sentence in the script describes in the image. \"\n",
    "                \"If there is a corresponding part on the image, please tell me the bounding box information of that area. \"\n",
    "                \"The output should be in JSON format with the structure: \"\n",
    "                \"{\\\"bboxes\\\": [{\\\"script\\\": \\\"string\\\", \\\"bbox\\\": [x, y, w, h]}]} \"\n",
    "                \"Ensure that the value for script in the JSON response should be a sentence from the provided script, and the value must never be text extracted from the image. \"\n",
    "                f\"script: {script_segment} \"\n",
    "                # \"The bounding box is given in the form [x,y,w,h]. x and y represent the center position of the bounding box, \"\n",
    "                # \"while w and h represent the width and height of the bounding box, respectively. \"\n",
    "                # \"The coordinates are based on the top-left corner of the image being (0,0), with the x direction being vertical and the y direction being horizontal. \"\n",
    "                # \"If a sentence in the script is highly relevant to a specific part of the image, \"\n",
    "                # \"use the sentence as the key and provide the bounding box information of the specific part of the image as the value. \"\n",
    "                # \"Only find bounding boxes for the sentences that are highly relevant to specific parts of the image. \"\n",
    "                # \"The original format of the script, including uppercase and lowercase letters, punctuation marks such as periods and commas, must be preserved without any alterations.\"\n",
    "            )\n",
    "        },\n",
    "        # {\"type\": \"text\", \"text\": script_segment},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": encoded_image}}\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Encode images and make API requests\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    page_number = str(i + 1)\n",
    "    if page_number in script_content:\n",
    "        print(page_number, image_path)\n",
    "        script_segment = script_content[page_number]\n",
    "        encoded_image = f\"data:image/png;base64,{encode_image(image_path)}\"\n",
    "        response_data = make_api_request(page_number, script_segment, encoded_image)\n",
    "        \n",
    "        # Process the response data\n",
    "        if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "            script_text = response_data['choices'][0]['message']['content']\n",
    "            print(script_text)\n",
    "            # Convert the script text to JSON format\n",
    "            try:\n",
    "                script_data = json.loads(script_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON for page {page_number}: {e}\")\n",
    "                script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "        else:\n",
    "            print(f\"Error: 'choices' key not found in the response for page {page_number}\")\n",
    "            print(response_data)\n",
    "            script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "        # Save the script data as a JSON file\n",
    "        script_json_path = os.path.join(bbox_path, f\"{page_number}_spm.json\")\n",
    "        with open(script_json_path, \"w\") as json_file:\n",
    "            json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "        print(f\"Script for page {page_number} saved to {script_json_path}\")\n",
    "    else:\n",
    "        print(f\"No script content found for page {page_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"okay, so welcome to lecture two of cs231n.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                1824,\n",
      "                229\n",
      "            ],\n",
      "            \"start\": 0.0,\n",
      "            \"end\": 3.119999885559082\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": []\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"the next administrative issue is about assignment one.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                600,\n",
      "                100\n",
      "            ],\n",
      "            \"start\": 104.13999938964844,\n",
      "            \"end\": 107.9800033569336\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"assignment one will be up later today, probably sometime this afternoon, but i promise, before i go to sleep tonight, it'll be up.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                600,\n",
      "                100\n",
      "            ],\n",
      "            \"start\": 109.0199966430664,\n",
      "            \"end\": 110.19999694824219\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, in this assignment you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                100,\n",
      "                600,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 137.52000427246094,\n",
      "            \"end\": 143.05999755859375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"you'll also implement several different linear classifiers, including the svm and softmax, as well as a simple two-layer neural network.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                100,\n",
      "                600,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 143.4600067138672,\n",
      "            \"end\": 150.63999938964844\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and we'll cover all this content over the next couple of lectures.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                100,\n",
      "                600,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 150.97999572753906,\n",
      "            \"end\": 153.6199951171875\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"all of our assignments are using python and numpy.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                92,\n",
      "                960,\n",
      "                422\n",
      "            ],\n",
      "            \"start\": 156.5,\n",
      "            \"end\": 158.86000061035156\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"if you aren't familiar with python or numpy, then we have written a tutorial that you can find on the course website to try and get you up to speed.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                92,\n",
      "                960,\n",
      "                422\n",
      "            ],\n",
      "            \"start\": 159.4600067138672,\n",
      "            \"end\": 166.89999389648438\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"for those of you who don't have a lot of experience with matlab or numpy or other types of vectorized tensor computation, i recommend that you start looking at this assignment pretty early and also, read carefully through the tutorial.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                92,\n",
      "                960,\n",
      "                422\n",
      "            ],\n",
      "            \"start\": 190.55999755859375,\n",
      "            \"end\": 202.36000061035156\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"we're happy to announce that we're officially supported through google cloud for this class.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                160,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 207.82000732421875,\n",
      "            \"end\": 212.67999267578125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, google cloud is somewhat similar to amazon aws.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                40,\n",
      "                200,\n",
      "                80\n",
      "            ],\n",
      "            \"start\": 213.44000244140625,\n",
      "            \"end\": 216.4199981689453\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"these virtual machines can have gpus.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                80,\n",
      "                200,\n",
      "                120\n",
      "            ],\n",
      "            \"start\": 219.82000732421875,\n",
      "            \"end\": 221.6999969482422\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but our intention is that you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                120,\n",
      "                300,\n",
      "                160\n",
      "            ],\n",
      "            \"start\": 228.33999633789062,\n",
      "            \"end\": 237.02000427246094\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and because google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use google cloud credits for free for the class.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                160,\n",
      "                320,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 237.67999267578125,\n",
      "            \"end\": 247.63999938964844\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"we'll post more details about that, probably, on piazza later today.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                200,\n",
      "                280,\n",
      "                240\n",
      "            ],\n",
      "            \"start\": 257.29998779296875,\n",
      "            \"end\": 260.55999755859375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"i just wanted to mention, because i know there had been a couple of questions about, can i use my laptop? do i have to run on corn? do i have to, whatever?\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                240,\n",
      "                350,\n",
      "                280\n",
      "            ],\n",
      "            \"start\": 261.239990234375,\n",
      "            \"end\": 268.5799865722656\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and the answer is that, you'll be able to run on google cloud and we'll provide you some coupons for that.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                280,\n",
      "                320,\n",
      "                320\n",
      "            ],\n",
      "            \"start\": 268.7799987792969,\n",
      "            \"end\": 270.1199951171875\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example,\",\n",
      "            \"bbox\": [\n",
      "                61,\n",
      "                160,\n",
      "                418,\n",
      "                518\n",
      "            ],\n",
      "            \"start\": 300.8599853515625,\n",
      "            \"end\": 309.1400146484375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, these might be, like, a dog or a cat or a truck or a plane, and there's some fixed set of category labels,\",\n",
      "            \"bbox\": [\n",
      "                664,\n",
      "                169,\n",
      "                236,\n",
      "                67\n",
      "            ],\n",
      "            \"start\": 315.20001220703125,\n",
      "            \"end\": 321.7799987792969\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"actually, what does a computer see when it looks at this image,\",\n",
      "            \"bbox\": [\n",
      "                435,\n",
      "                87,\n",
      "                410,\n",
      "                62\n",
      "            ],\n",
      "            \"start\": 342.8999938964844,\n",
      "            \"end\": 345.7799987792969\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"it definitely doesn't get this holistic idea of a cat that you see when you look at it.\",\n",
      "            \"bbox\": [\n",
      "                9,\n",
      "                57,\n",
      "                462,\n",
      "                533\n",
      "            ],\n",
      "            \"start\": 346.0799865722656,\n",
      "            \"end\": 350.1600036621094\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and the computer really is representing the image as this gigantic grid of numbers.\",\n",
      "            \"bbox\": [\n",
      "                910,\n",
      "                57,\n",
      "                421,\n",
      "                189\n",
      "            ],\n",
      "            \"start\": 350.4599914550781,\n",
      "            \"end\": 354.0199890136719\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, the image might be something like 800 by 600 pixels.\",\n",
      "            \"bbox\": [\n",
      "                730,\n",
      "                369,\n",
      "                215,\n",
      "                29\n",
      "            ],\n",
      "            \"start\": 356.4800109863281,\n",
      "            \"end\": 359.8800048828125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and each pixel is represented by three numbers, giving the red, green, and blue values for that pixel.\",\n",
      "            \"bbox\": [\n",
      "                920,\n",
      "                555,\n",
      "                417,\n",
      "                24\n",
      "            ],\n",
      "            \"start\": 360.1199951171875,\n",
      "            \"end\": 361.1600036621094\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"to the computer, this is just a gigantic grid of numbers.\",\n",
      "            \"bbox\": [\n",
      "                910,\n",
      "                57,\n",
      "                421,\n",
      "                189\n",
      "            ],\n",
      "            \"start\": 366.55999755859375,\n",
      "            \"end\": 368.7200012207031\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and it's very difficult to distill the cat-ness out of this, like, giant array of thousands,\",\n",
      "            \"bbox\": [\n",
      "                9,\n",
      "                57,\n",
      "                462,\n",
      "                533\n",
      "            ],\n",
      "            \"start\": 368.8399963378906,\n",
      "            \"end\": 375.1000061035156\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\",\n",
      "            \"bbox\": [\n",
      "                9,\n",
      "                57,\n",
      "                462,\n",
      "                533\n",
      "            ],\n",
      "            \"start\": 397.2200012207031,\n",
      "            \"end\": 404.44000244140625\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, for example, if we took this same cat,\",\n",
      "            \"bbox\": [\n",
      "                366,\n",
      "                174,\n",
      "                372,\n",
      "                478\n",
      "            ],\n",
      "            \"start\": 404.9200134277344,\n",
      "            \"end\": 406.82000732421875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen,\",\n",
      "            \"bbox\": [\n",
      "                366,\n",
      "                174,\n",
      "                372,\n",
      "                478\n",
      "            ],\n",
      "            \"start\": 406.82000732421875,\n",
      "            \"end\": 411.4599914550781\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but we moved the camera to the other side, then every single grid,\",\n",
      "            \"bbox\": [\n",
      "                42,\n",
      "                108,\n",
      "                122,\n",
      "                82\n",
      "            ],\n",
      "            \"start\": 411.8599853515625,\n",
      "            \"end\": 415.739990234375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"every single pixel, in this giant grid of numbers would be completely different.\",\n",
      "            \"bbox\": [\n",
      "                484,\n",
      "                172,\n",
      "                378,\n",
      "                110\n",
      "            ],\n",
      "            \"start\": 416.0400085449219,\n",
      "            \"end\": 419.6600036621094\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but, somehow, it's still representing the same cat.\",\n",
      "            \"bbox\": [\n",
      "                366,\n",
      "                174,\n",
      "                372,\n",
      "                478\n",
      "            ],\n",
      "            \"start\": 419.9800109863281,\n",
      "            \"end\": 422.1000061035156\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"there can be different lighting conditions going on in the scene.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                250,\n",
      "                330,\n",
      "                440\n",
      "            ],\n",
      "            \"start\": 429.1600036621094,\n",
      "            \"end\": 431.7799987792969\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"whether the cat is appearing in this very dark, moody scene, or like is this very bright, sunlit scene, it's still a cat,\",\n",
      "            \"bbox\": [\n",
      "                350,\n",
      "                250,\n",
      "                980,\n",
      "                440\n",
      "            ],\n",
      "            \"start\": 432.44000244140625,\n",
      "            \"end\": 433.0199890136719\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and our algorithms need to be robust to that.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                250,\n",
      "                1320,\n",
      "                450\n",
      "            ],\n",
      "            \"start\": 422.4599914550781,\n",
      "            \"end\": 440.5799865722656\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"i think cats are, maybe, among the more deformable of animals that you might see out there.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                70,\n",
      "                1365,\n",
      "                512\n",
      "            ],\n",
      "            \"start\": 443.5199890136719,\n",
      "            \"end\": 447.1000061035156\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and cats can really assume a lot of different, varied poses and positions.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                70,\n",
      "                1365,\n",
      "                512\n",
      "            ],\n",
      "            \"start\": 447.3599853515625,\n",
      "            \"end\": 451.5199890136719\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"or in this extreme example, just a tail peeking out from under the couch cushion.\",\n",
      "            \"bbox\": [\n",
      "                1064,\n",
      "                480,\n",
      "                150,\n",
      "                100\n",
      "            ],\n",
      "            \"start\": 435.29998779296875,\n",
      "            \"end\": 466.1600036621094\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"like, just the face,\",\n",
      "            \"bbox\": [\n",
      "                116,\n",
      "                252,\n",
      "                300,\n",
      "                300\n",
      "            ],\n",
      "            \"start\": 461.7200012207031,\n",
      "            \"end\": 462.5199890136719\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"where you might only see part of a cat,\",\n",
      "            \"bbox\": [\n",
      "                530,\n",
      "                300,\n",
      "                300,\n",
      "                300\n",
      "            ],\n",
      "            \"start\": 459.4800109863281,\n",
      "            \"end\": 461.6000061035156\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.\",\n",
      "            \"bbox\": [\n",
      "                40,\n",
      "                150,\n",
      "                550,\n",
      "                405\n",
      "            ],\n",
      "            \"start\": 455.7799987792969,\n",
      "            \"end\": 483.1199951171875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"there can also be problems of background clutter, where maybe the foreground object of the cat, could actually look quite similar in appearance to the background.\",\n",
      "            \"bbox\": [\n",
      "                730,\n",
      "                150,\n",
      "                550,\n",
      "                405\n",
      "            ],\n",
      "            \"start\": 455.7799987792969,\n",
      "            \"end\": 483.1199951171875\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"there's also this problem of intraclass variation, that this one notion of cat-ness, actually spans a lot of different visual appearances.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                1365,\n",
      "                60\n",
      "            ],\n",
      "            \"start\": 492.8399963378906,\n",
      "            \"end\": 501.0\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and cats can come in different shapes and sizes and colors and ages.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                280,\n",
      "                1365,\n",
      "                720\n",
      "            ],\n",
      "            \"start\": 447.3599853515625,\n",
      "            \"end\": 504.70001220703125\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"An image classifier\",\n",
      "            \"bbox\": [\n",
      "                210,\n",
      "                40,\n",
      "                406,\n",
      "                60\n",
      "            ],\n",
      "            \"start\": 559.4199829101562,\n",
      "            \"end\": 560.260009765625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"no obvious way to hard-code the algorithm for recognizing a cat, or other classes.\",\n",
      "            \"bbox\": [\n",
      "                174,\n",
      "                330,\n",
      "                515,\n",
      "                46\n",
      "            ],\n",
      "            \"start\": 572.0399780273438,\n",
      "            \"end\": 847.6799926757812\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"we touched on this a little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears and eyes and mouths and noses.\",\n",
      "            \"bbox\": [\n",
      "                36,\n",
      "                92,\n",
      "                255,\n",
      "                358\n",
      "            ],\n",
      "            \"start\": 620.1400146484375,\n",
      "            \"end\": 628.4400024414062\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and we know that edges, from hubel and wiesel, we know that edges are pretty important when it comes to visual recognition.\",\n",
      "            \"bbox\": [\n",
      "                412,\n",
      "                87,\n",
      "                228,\n",
      "                368\n",
      "            ],\n",
      "            \"start\": 628.5399780273438,\n",
      "            \"end\": 634.0399780273438\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats.\",\n",
      "            \"bbox\": [\n",
      "                36,\n",
      "                92,\n",
      "                255,\n",
      "                358\n",
      "            ],\n",
      "            \"start\": 634.6199951171875,\n",
      "            \"end\": 653.3200073242188\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries, and say that, if we have maybe three lines meeting this way, then it might be a corner, and an ear has one corner here and one corner there and one corner there, and then, kind of, write down this explicit set of rules for recognizing cats.\",\n",
      "            \"bbox\": [\n",
      "                412,\n",
      "                87,\n",
      "                228,\n",
      "                368\n",
      "            ],\n",
      "            \"start\": 634.6199951171875,\n",
      "            \"end\": 653.3200073242188\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but this turns out not to work very well. one, it's super brittle.\",\n",
      "            \"bbox\": [\n",
      "                412,\n",
      "                87,\n",
      "                228,\n",
      "                368\n",
      "            ],\n",
      "            \"start\": 653.7999877929688,\n",
      "            \"end\": 658.5599975585938\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and, two, say, if you want to start over for another object category, and maybe not worry about cats, but talk about trucks or dogs or fishes or something else, then you need to start all over again. so, this is really not a very scalable approach.\",\n",
      "            \"bbox\": [\n",
      "                412,\n",
      "                87,\n",
      "                228,\n",
      "                368\n",
      "            ],\n",
      "            \"start\": 659.0,\n",
      "            \"end\": 672.47998046875\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"and then, separately, another function called, predict, which will input the model and than make predictions for images.\",\n",
      "            \"bbox\": [\n",
      "                35,\n",
      "                311,\n",
      "                354,\n",
      "                57\n",
      "            ],\n",
      "            \"start\": 762.4199829101562,\n",
      "            \"end\": 767.719970703125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but this idea of a data-driven approach is much more general than just deep learning.\",\n",
      "            \"bbox\": [\n",
      "                35,\n",
      "                5,\n",
      "                381,\n",
      "                52\n",
      "            ],\n",
      "            \"start\": 784.239990234375,\n",
      "            \"end\": 788.5\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, during the training step we won't do anything, we'll just memorize all of the training data.\",\n",
      "            \"bbox\": [\n",
      "                400,\n",
      "                200,\n",
      "                250,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 804.7000122070312,\n",
      "            \"end\": 809.2000122070312\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now, during the prediction step, we're going to take some new image and go and try to find the most similar image in the training data to that new image, and now predict the label of that most similar image.\",\n",
      "            \"bbox\": [\n",
      "                400,\n",
      "                340,\n",
      "                260,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 812.0800170898438,\n",
      "            \"end\": 815.8800048828125\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"you might imagine working on this dataset called cifar-10, which is very commonly used in machine learning, as kind of a small test case.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                20,\n",
      "                150,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 834.739990234375,\n",
      "            \"end\": 841.97998046875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and you'll be working with this dataset on your homework.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                20,\n",
      "                150,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 842.280029296875,\n",
      "            \"end\": 844.1400146484375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, the cifar-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                70,\n",
      "                70,\n",
      "                100\n",
      "            ],\n",
      "            \"start\": 844.6199951171875,\n",
      "            \"end\": 852.219970703125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and for each of those 10 categories it provides 50,000 training images, roughly evenly distributed across these 10 categories.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                70,\n",
      "                70,\n",
      "                100\n",
      "            ],\n",
      "            \"start\": 852.9000244140625,\n",
      "            \"end\": 855.1400146484375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and then 10,000 additional testing images that you're supposed to test your algorithm on.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                120,\n",
      "                70,\n",
      "                80\n",
      "            ],\n",
      "            \"start\": 863.4199829101562,\n",
      "            \"end\": 869.5399780273438\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so here's an example of applying this simple nearest neighbor classifier to some of these test images on cifar-10.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                10,\n",
      "                400,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 872.0,\n",
      "            \"end\": 878.219970703125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, on this grid on the right, for the left most column, gives a test image in the cifar-10 dataset.\",\n",
      "            \"bbox\": [\n",
      "                740,\n",
      "                20,\n",
      "                600,\n",
      "                900\n",
      "            ],\n",
      "            \"start\": 878.8200073242188,\n",
      "            \"end\": 886.280029296875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now on the right, we've sorted the training images and show the most similar training images to each of these test examples.\",\n",
      "            \"bbox\": [\n",
      "                760,\n",
      "                30,\n",
      "                580,\n",
      "                880\n",
      "            ],\n",
      "            \"start\": 886.3800048828125,\n",
      "            \"end\": 895.719970703125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and you can see that they look kind of visually similar to the training images, although they are not always correct, right?\",\n",
      "            \"bbox\": [\n",
      "                760,\n",
      "                30,\n",
      "                580,\n",
      "                880\n",
      "            ],\n",
      "            \"start\": 896.4600219726562,\n",
      "            \"end\": 904.0999755859375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, maybe on the second row, we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess.\",\n",
      "            \"bbox\": [\n",
      "                758,\n",
      "                252,\n",
      "                174,\n",
      "                72\n",
      "            ],\n",
      "            \"start\": 904.0999755859375,\n",
      "            \"end\": 913.260009765625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but, this image is a dog and it's nearest neighbor is also a dog, but this next one, i think is actually a deer or a horse or something else.\",\n",
      "            \"bbox\": [\n",
      "                758,\n",
      "                252,\n",
      "                174,\n",
      "                72\n",
      "            ],\n",
      "            \"start\": 913.760009765625,\n",
      "            \"end\": 922.5800170898438\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but, you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot.\",\n",
      "            \"bbox\": [\n",
      "                758,\n",
      "                252,\n",
      "                174,\n",
      "                72\n",
      "            ],\n",
      "            \"start\": 922.8200073242188,\n",
      "            \"end\": 928.7000122070312\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                40,\n",
      "                720,\n",
      "                924\n",
      "            ],\n",
      "            \"start\": 929.260009765625,\n",
      "            \"end\": 935.3599853515625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now, the closest example, we know it's label, because it comes from the training set.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                40,\n",
      "                720,\n",
      "                924\n",
      "            ],\n",
      "            \"start\": 935.6599731445312,\n",
      "            \"end\": 939.780029296875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now, we'll simply say that this testing image is also a dog.\",\n",
      "            \"bbox\": [\n",
      "                758,\n",
      "                252,\n",
      "                174,\n",
      "                72\n",
      "            ],\n",
      "            \"start\": 939.9199829101562,\n",
      "            \"end\": 943.5\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"you can see from these examples that is probably not going to work very well, but it's still kind of a nice example to work through.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                40,\n",
      "                720,\n",
      "                924\n",
      "            ],\n",
      "            \"start\": 896.739990234375,\n",
      "            \"end\": 951.3800048828125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but then, one detail that we need to know is, given a pair of images, how can we actually compare them?\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                40,\n",
      "                720,\n",
      "                924\n",
      "            ],\n",
      "            \"start\": 952.8800048828125,\n",
      "            \"end\": 959.6400146484375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"because, if we're going to take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                40,\n",
      "                720,\n",
      "                924\n",
      "            ],\n",
      "            \"start\": 959.780029296875,\n",
      "            \"end\": 967.8400268554688\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, in the example in the previous slide, we've used what's called the l1 distance, also sometimes called the manhattan distance.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                20,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 968.4400024414062,\n",
      "            \"end\": 972.4600219726562\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, this is a really sort of simple, easy idea for comparing images.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                60,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 177.0,\n",
      "            \"end\": 473.5199890136719\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and that's that we're going to just compare individual pixels in these images.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                100,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 980.5,\n",
      "            \"end\": 985.4199829101562\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, supposing that our test image is maybe just a tiny four by four image of pixel values,\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                140,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 986.0599975585938,\n",
      "            \"end\": 991.780029296875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and get the difference in that pixel between the two images.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                220,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 999.3800048828125,\n",
      "            \"end\": 1001.5599975585938\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and then, sum all these up across all the pixels in the image.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                260,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 1001.6799926757812,\n",
      "            \"end\": 1004.4400024414062\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, this is kind of a stupid way to compare images, but it does some reasonable things sometimes.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                300,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 177.0,\n",
      "            \"end\": 974.0\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but, this gives us a very concrete way to measure the difference between two images.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                340,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 1010.7999877929688,\n",
      "            \"end\": 1014.4600219726562\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and in this case, we have this difference of 456 between these two images.\",\n",
      "            \"bbox\": [\n",
      "                20,\n",
      "                380,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 1015.0,\n",
      "            \"end\": 1018.9199829101562\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, here's some full python code for implementing this nearest neighbor classifier and you can see it's pretty short and pretty concise because we've made use of many of these vectorized operations offered by numpy.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                10,\n",
      "                250,\n",
      "                40\n",
      "            ],\n",
      "            \"start\": 1020.4000244140625,\n",
      "            \"end\": 1033.5\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, here we can see that this training function, that we talked about earlier, is, again, very simple, in the case of nearest neighbor, you just memorize the training data, there's not really much to do here.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                117,\n",
      "                640,\n",
      "                120\n",
      "            ],\n",
      "            \"start\": 1034.3800048828125,\n",
      "            \"end\": 1044.800048828125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now, at test time, we're going to take in our image and then go in and compare using this l1 distance function, our test image to each of these training examples and find the most similar example in the training set.\",\n",
      "            \"bbox\": [\n",
      "                10,\n",
      "                258,\n",
      "                640,\n",
      "                240\n",
      "            ],\n",
      "            \"start\": 1045.5999755859375,\n",
      "            \"end\": 1057.5999755859375\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"and you can see that, we're actually able to do this in just one or two lines of python code by utilizing these vectorized operations in numpy.\",\n",
      "            \"bbox\": [\n",
      "                14,\n",
      "                10,\n",
      "                535,\n",
      "                871\n",
      "            ],\n",
      "            \"start\": 896.4600219726562,\n",
      "            \"end\": 1033.5\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"Nearest Neighbor classifier\",\n",
      "            \"bbox\": [\n",
      "                850,\n",
      "                50,\n",
      "                250,\n",
      "                30\n",
      "            ],\n",
      "            \"start\": 140.74000549316406,\n",
      "            \"end\": 141.63999938964844\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"well, training is probably constant because we don't really need to do anything, we just need to memorize the data.\",\n",
      "            \"bbox\": [\n",
      "                853,\n",
      "                256,\n",
      "                400,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 1083.0,\n",
      "            \"end\": 1090.0400390625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and if you're just copying a pointer, that's going to be constant time no matter how big your dataset is.\",\n",
      "            \"bbox\": [\n",
      "                853,\n",
      "                256,\n",
      "                400,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 1091.0999755859375,\n",
      "            \"end\": 1095.3199462890625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but now, at test time we need to do this comparison stop and compare our test image to each of the n training examples in the dataset.\",\n",
      "            \"bbox\": [\n",
      "                853,\n",
      "                256,\n",
      "                400,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 1095.4200439453125,\n",
      "            \"end\": null\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and this is actually quite slow.\",\n",
      "            \"bbox\": [\n",
      "                853,\n",
      "                256,\n",
      "                400,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 293.5799865722656,\n",
      "            \"end\": 1106.1800537109375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, this is actually somewhat backwards, if you think about it.\",\n",
      "            \"bbox\": [\n",
      "                853,\n",
      "                256,\n",
      "                400,\n",
      "                200\n",
      "            ],\n",
      "            \"start\": 177.0,\n",
      "            \"end\": 227.3800048828125\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"because, in practice, we want our classifiers to be slow at training time and then fast at testing time.\",\n",
      "            \"bbox\": [\n",
      "                800,\n",
      "                30,\n",
      "                500,\n",
      "                60\n",
      "            ],\n",
      "            \"start\": 1111.6800537109375,\n",
      "            \"end\": 1116.3199462890625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"because, you might imagine, that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good.\",\n",
      "            \"bbox\": [\n",
      "                800,\n",
      "                90,\n",
      "                500,\n",
      "                180\n",
      "            ],\n",
      "            \"start\": 1118.3199462890625,\n",
      "            \"end\": 1127.5400390625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but then, when you go and deploy the classifier at test time, you want it to run on your mobile phone or in a browser or some other low power device, and you really want the testing time performance of your classifier to be quite fast.\",\n",
      "            \"bbox\": [\n",
      "                800,\n",
      "                270,\n",
      "                500,\n",
      "                210\n",
      "            ],\n",
      "            \"start\": 1127.8599853515625,\n",
      "            \"end\": 1139.5999755859375\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"Nearest Neighbor classifier\",\n",
      "            \"bbox\": [\n",
      "                900,\n",
      "                100,\n",
      "                300,\n",
      "                50\n",
      "            ],\n",
      "            \"start\": 140.74000549316406,\n",
      "            \"end\": 141.63999938964844\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so, here our training set consists of these points in the two dimensional plane, where the color of the point represents the category, or the class label, of that point.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 1169.1600341796875,\n",
      "            \"end\": 1176.800048828125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, here we see we have five classes and some blue ones up in the corner here, some purple ones in the upper-right hand corner.\",\n",
      "            \"bbox\": [\n",
      "                256,\n",
      "                0,\n",
      "                256,\n",
      "                128\n",
      "            ],\n",
      "            \"start\": 1034.3800048828125,\n",
      "            \"end\": 1183.9200439453125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and now for each pixel in this entire plane, we've gone and computed what is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 1186.6400146484375,\n",
      "            \"end\": 1199.3399658203125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 248.3800048828125,\n",
      "            \"end\": 1172.5999755859375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but this classifier is maybe not so great.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 1207.739990234375,\n",
      "            \"end\": 1211.3399658203125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 1211.5,\n",
      "            \"end\": 1216.93994140625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"for one, this central region actually contains mostly green points, but one little yellow point in the middle.\",\n",
      "            \"bbox\": [\n",
      "                224,\n",
      "                160,\n",
      "                64,\n",
      "                64\n",
      "            ],\n",
      "            \"start\": 1217.56005859375,\n",
      "            \"end\": 1223.9599609375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of this green cluster.\",\n",
      "            \"bbox\": [\n",
      "                224,\n",
      "                160,\n",
      "                64,\n",
      "                64\n",
      "            ],\n",
      "            \"start\": 1224.8599853515625,\n",
      "            \"end\": 1231.199951171875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and that's, maybe, not so great. maybe those points actually should have been green.\",\n",
      "            \"bbox\": [\n",
      "                224,\n",
      "                160,\n",
      "                64,\n",
      "                64\n",
      "            ],\n",
      "            \"start\": 1231.5799560546875,\n",
      "            \"end\": 1236.300048828125\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and then, similarly we also see these, sort of, fingers, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious.\",\n",
      "            \"bbox\": [\n",
      "                64,\n",
      "                128,\n",
      "                128,\n",
      "                64\n",
      "            ],\n",
      "            \"start\": 1237.06005859375,\n",
      "            \"end\": 1247.3599853515625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"so, this kind of motivates a slight generalization of this algorithm called k-nearest neighbors.\",\n",
      "            \"bbox\": [\n",
      "                0,\n",
      "                0,\n",
      "                512,\n",
      "                384\n",
      "            ],\n",
      "            \"start\": 1247.780029296875,\n",
      "            \"end\": 1253.43994140625\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"so here we've shown the exact same set of points using this k=1 nearest neighbor classifier, as well as k=3 and k=5 in the middle and on the right.\",\n",
      "            \"bbox\": [\n",
      "                73,\n",
      "                190,\n",
      "                1175,\n",
      "                448\n",
      "            ],\n",
      "            \"start\": 1163.5799560546875,\n",
      "            \"end\": 1185.9000244140625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and once we move to k=3, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow.\",\n",
      "            \"bbox\": [\n",
      "                465,\n",
      "                256,\n",
      "                225,\n",
      "                192\n",
      "            ],\n",
      "            \"start\": 1292.9200439453125,\n",
      "            \"end\": 1297.3199462890625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"now this entire green portion in the middle is all being classified as green.\",\n",
      "            \"bbox\": [\n",
      "                632,\n",
      "                293,\n",
      "                93,\n",
      "                115\n",
      "            ],\n",
      "            \"start\": 1304.06005859375,\n",
      "            \"end\": 1305.780029296875\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"you can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting.\",\n",
      "            \"bbox\": [\n",
      "                270,\n",
      "                330,\n",
      "                250,\n",
      "                160\n",
      "            ],\n",
      "            \"start\": 1308.699951171875,\n",
      "            \"end\": 1314.9599609375\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and then, once we move to the k=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice.\",\n",
      "            \"bbox\": [\n",
      "                860,\n",
      "                256,\n",
      "                275,\n",
      "                192\n",
      "            ],\n",
      "            \"start\": 1315.5,\n",
      "            \"end\": 1323.300048828125\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": [\n",
      "        {\n",
      "            \"script\": \"here i've colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor.\",\n",
      "            \"bbox\": [\n",
      "                50,\n",
      "                60,\n",
      "                765,\n",
      "                400\n",
      "            ],\n",
      "            \"start\": 1402.43994140625,\n",
      "            \"end\": 1408.260009765625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"and you can see that it's really not very good.\",\n",
      "            \"bbox\": [\n",
      "                50,\n",
      "                60,\n",
      "                765,\n",
      "                400\n",
      "            ],\n",
      "            \"start\": 896.4600219726562,\n",
      "            \"end\": 1127.5400390625\n",
      "        },\n",
      "        {\n",
      "            \"script\": \"but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row.\",\n",
      "            \"bbox\": [\n",
      "                50,\n",
      "                60,\n",
      "                765,\n",
      "                400\n",
      "            ],\n",
      "            \"start\": 1411.02001953125,\n",
      "            \"end\": 1419.8800048828125\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"bboxes\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "SCRIPT = './scripts'\n",
    "SPM = './spms'\n",
    "BBOX = './bboxs'\n",
    "\n",
    "# Load the JSON data from the files\n",
    "with open(os.path.join(SCRIPT, 'test_word_gpt.json'), 'r') as file:\n",
    "    word_times = json.load(file)\n",
    "\n",
    "project_id = 11\n",
    "bbox_path = os.path.join(BBOX, str(project_id), \"13_spm.json\")\n",
    "with open(bbox_path, 'r') as file:\n",
    "    bboxes = json.load(file)\n",
    "\n",
    "# Function to get start and end times of a script based on word times\n",
    "def get_script_times(script_text, word_times):\n",
    "    # Remove punctuation from the script_text and split into words\n",
    "    words = re.findall(r'\\b[\\w\\']+\\b', script_text.lower())\n",
    "    \n",
    "    start_time = None\n",
    "    end_time = None\n",
    "\n",
    "    if len(words) >= 3:\n",
    "        for i in range(len(word_times) - 2):\n",
    "            if (word_times[i]['word'].lower() == words[0] and word_times[i + 1]['word'].lower() == words[1] and word_times[i + 2]['word'].lower() == words[2]):\n",
    "                \n",
    "                # Set start time from the first word\n",
    "                start_time = word_times[i]['start']\n",
    "                \n",
    "                # Find end time from the last word in words\n",
    "                for j in range(i + 2, len(word_times)):\n",
    "                    if word_times[j]['word'].lower() == words[-1]:\n",
    "                        end_time = word_times[j]['end']\n",
    "                        break\n",
    "                break\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "# Update the bbox data with start and end times\n",
    "for page_num in range(1, len(os.listdir(os.path.join(BBOX, str(project_id))))+1):\n",
    "    bbox_path = os.path.join(BBOX, str(project_id), f\"{page_num}_spm.json\")\n",
    "\n",
    "    # Load the bbox data for the current page\n",
    "    with open(bbox_path, 'r') as file:\n",
    "        bboxes = json.load(file)\n",
    "\n",
    "    updated_bboxes = []\n",
    "    for item in bboxes[\"bboxes\"]:\n",
    "        bbox = item[\"bbox\"]\n",
    "        if not bbox or bbox[2]==0 or bbox[3]==0:\n",
    "            continue\n",
    "    \n",
    "        start_time, end_time = get_script_times(item[\"script\"], word_times)\n",
    "        if start_time is not None:\n",
    "            item[\"start\"] = start_time\n",
    "            item[\"end\"] = end_time\n",
    "            updated_bboxes.append(item)\n",
    "\n",
    "    # Save the updated data back to the JSON file\n",
    "    bboxes[\"bboxes\"] = updated_bboxes\n",
    "    with open(bbox_path, 'w') as file:\n",
    "        json.dump(bboxes, file, indent=4)\n",
    "\n",
    "    # Print the updated data (optional)\n",
    "    print(json.dumps(bboxes, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "PDF = './pdfs'\n",
    "TOC = './tocs'\n",
    "project_id = 10\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Assistant for Making Table of Contents \",\n",
    "  instructions=\"You are a helpful assistant designed to output JSON. Use your knowledge base to make a table of contents about attached file.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Making Table of Contents\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "\n",
    "file_paths = [os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\")]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "# PDF = './pdfs'\n",
    "# project_id = 10\n",
    "# message_file = client.files.create(\n",
    "#   file=open(os.path.join(PDF, f\"{project_id}_cs231n_2017_lecture2.pdf\"), \"rb\"), purpose=\"assistants\"\n",
    "# )\n",
    "\n",
    "content = [{\"type\": \"text\", \"text\": (\n",
    "            \"Extract a detailed table of contents with page numbers from the uploaded pdf file. \"\n",
    "            \"Each section should have a unique title, and the subsections should be grouped under these main sections. \"\n",
    "            \"The page numbers should start from 1 and each page number should be in an array. \"\n",
    "            \"Ensure there are at least 5 main sections, and each main section should have at most 5 subsections. \"\n",
    "            \"The output should be in JSON format with the structure: \"\n",
    "            \"{\\\"table_of_contents\\\": [{\\\"title\\\": \\\"string\\\", \\\"subsections\\\": [{\\\"title\\\": \\\"string\\\", \\\"page\\\": [\\\"number\\\"]}]}]} \"\n",
    "            \"Make sure that all pages are included in the table of contents.\"\n",
    "        )}]\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": content,\n",
    "      # # Attach the new file to the message.\n",
    "      # \"attachments\": [\n",
    "      #   { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      # ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted Table of Contents in JSON format based on the uploaded PDF:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"table_of_contents\": [\n",
      "    {\n",
      "      \"title\": \"Administrative\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"Piazza\", \"page\": [2] },\n",
      "        { \"title\": \"Assignment 1\", \"page\": [3] },\n",
      "        { \"title\": \"Python + Numpy\", \"page\": [4] },\n",
      "        { \"title\": \"Google Cloud\", \"page\": [5] }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Image Classification\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"A core task in Computer Vision\", \"page\": [6] },\n",
      "        { \"title\": \"The Problem: Semantic Gap\", \"page\": [7] },\n",
      "        { \"title\": \"Challenges: Viewpoint variation\", \"page\": [8] },\n",
      "        { \"title\": \"Challenges: Illumination\", \"page\": [9] },\n",
      "        { \"title\": \"Challenges: Deformation\", \"page\": [10] },\n",
      "        { \"title\": \"Challenges: Occlusion\", \"page\": [11] },\n",
      "        { \"title\": \"Challenges: Background Clutter\", \"page\": [12] },\n",
      "        { \"title\": \"Challenges: Intraclass variation\", \"page\": [13] }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Classification Methods\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"An image classifier\", \"page\": [14] },\n",
      "        { \"title\": \"Attempts have been made\", \"page\": [15] },\n",
      "        { \"title\": \"Data-Driven Approach\", \"page\": [16] },\n",
      "        { \"title\": \"First classifier: Nearest Neighbor\", \"page\": [17] },\n",
      "        { \"title\": \"Example Dataset: CIFAR10\", \"page\": [18] }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Distance Metrics\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"Distance Metric to compare images\", \"page\": [20] },\n",
      "        { \"title\": \"L1 distance\", \"page\": [21] }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Nearest Neighbor Classifier\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"Memorize training data\", \"page\": [22] },\n",
      "        { \"title\": \"Prediction\", \"page\": [23] },\n",
      "        { \"title\": \"Training and prediction speed\", \"page\": [25, 26] }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"K-Nearest Neighbors\",\n",
      "      \"subsections\": [\n",
      "        { \"title\": \"Majority vote from K closest points\", \"page\": [28] }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON structure incorporates the main sections and their subsections along with the corresponding page numbers.\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "message_content = messages[0].content[0].text\n",
    "\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(citations))\n",
    "print(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 내용 출력 유틸\n",
    "def show_json(obj):\n",
    "  display(json.loads(obj.model_dump_json()))\n",
    "\n",
    "def _get_response(thread_id):\n",
    "    return client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\")\n",
    "\n",
    "# Thread message 출력 유틸\n",
    "def print_message(thread_id):\n",
    "    for res in _get_response(thread_id):\n",
    "        print(f\"[{res.role.upper()}]\\n{res.content[0].text.value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER]\n",
      "Extract a detailed table of contents with page numbers from the uploaded pdf file. Each section should have a unique title, and the subsections should be grouped under these main sections. The page numbers should start from 1 and each page number should be in an array. Ensure there are more than 4 main sections. The output should be in JSON format with the structure: {\"table_of_contents\": [{\"title\": \"string\", \"subsections\": [{\"title\": \"string\", \"page\": [\"number\"]}]}]} and include all pages starting from 1.\n",
      "\n",
      "[ASSISTANT]\n",
      "```json\n",
      "{\n",
      "  \"table_of_contents\": [\n",
      "    {\n",
      "      \"title\": \"Administrative\",\n",
      "      \"subsections\": [\n",
      "        {\"title\": \"Piazza\", \"page\": [2]},\n",
      "        {\"title\": \"Assignment 1\", \"page\": [3]},\n",
      "        {\"title\": \"Python + Numpy\", \"page\": [4]},\n",
      "        {\"title\": \"Google Cloud\", \"page\": [5]}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Introduction to Image Classification\",\n",
      "      \"subsections\": [\n",
      "        {\"title\": \"A core task in Computer Vision\", \"page\": [6]},\n",
      "        {\"title\": \"Challenges\", \"page\": [7, 8, 9, 10, 11, 12]},\n",
      "        {\"title\": \"An Image Classifier\", \"page\": [14]},\n",
      "        {\"title\": \"Attempts have been made\", \"page\": [15]},\n",
      "        {\"title\": \"Data-Driven Approach\", \"page\": [16]},\n",
      "        {\"title\": \"First Classifier: Nearest Neighbor\", \"page\": [17]}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Example Dataset: CIFAR10\",\n",
      "      \"subsections\": [\n",
      "        {\"title\": \"Description\", \"page\": [18, 19]},\n",
      "        {\"title\": \"Distance Metric\", \"page\": [20]}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Nearest Neighbor Classifier\",\n",
      "      \"subsections\": [\n",
      "        {\"title\": \"Training and Prediction Speed\", \"page\": [24, 25]},\n",
      "        {\"title\": \"Handling larger datasets\", \"page\": [26]}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"K-Nearest Neighbor Classifier\",\n",
      "      \"subsections\": [\n",
      "        {\"title\": \"Basics\", \"page\": [28]},\n",
      "        {\"title\": \"Distance Metric\", \"page\": [31, 32]},\n",
      "        {\"title\": \"Demo\", \"page\": [33]}\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_message(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
