{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "project_id = 11\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "\n",
    "# Load the JSON data from the files\n",
    "with open(os.path.join(SCRIPT, 'test_segment_gpt.json'), 'r') as file:\n",
    "    segment_times = json.load(file)\n",
    "\n",
    "script = ''\n",
    "for segment in segment_times:\n",
    "    script += segment['text']\n",
    "\n",
    "script = script.strip()\n",
    "\n",
    "with open(script_path, 'w') as file:\n",
    "    json.dump(script, file)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikim/Desktop/git/VividReview/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw script text: {\n",
      "  \"1\": \"Okay, so welcome to Lecture 2 of CS231N.\",\n",
      "  \"2\": \"And it's sort of easy for emails to get lost in the shuffle if you just send to the course list.\",\n",
      "  \"3\": \"Probably that doesn't affect those of you who are sitting in the room right now, but for those students listening on SCPD.\",\n",
      "  \"4\": \"We're just reshuffling it a little bit to make it, like for example, upgrading to work with Python 3 rather than Python 2.7, and some of these minor cosmetic changes.\",\n",
      "  \"5\": \"NumPy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code.\",\n",
      "  \"6\": \"Because Google has very generously supported this course, we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class.\",\n",
      "  \"7\": \"So the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision.\",\n",
      "  \"8\": \"But this is actually a really, really hard problem for a machine.\",\n",
      "  \"9\": \"So you just, so the image might be something like 800 by 600 pixels.\",\n",
      "  \"10\": \"But somehow, it's still representing the same cat.\",\n",
      "  \"11\": \"Cats can really assume a lot of different varied poses and positions.\",\n",
      "  \"12\": \"And this is something that our algorithms also must be robust to, which is quite difficult, I think.\",\n",
      "  \"13\": \"And this is another thing that we need to handle.\",\n",
      "  \"14\": \"And our algorithm, again, needs to work and handle all these different variations.\",\n",
      "  \"15\": \"So this is some pretty amazing, incredible technology, in my opinion.\",\n",
      "  \"16\": \"You might sit down and try to write a method in Python like this, where you want to take in an image and then do some crazy magic, and then eventually spit out this class label to say cat or dog or whatnot.\",\n",
      "  \"17\": \"But this turns out not to work very well.\",\n",
      "  \"18\": \"And we can actually use tools like Google Image Search or something like that to go out and collect a very large number of examples of these different categories.\",\n",
      "  \"19\": \"And then separately, another function called predict, which will input the model and then make predictions for images.\",\n",
      "  \"20\": \"And I think it's useful to sort of step through this process for a very simple classifier first before we get to these big, complex ones.\",\n",
      "  \"21\": \"Very simple algorithm.\",\n",
      "  \"22\": \"And now the closest example, we know it's label because it comes from the training set, and now we'll simply say that this testing image is also a dog.\",\n",
      "  \"23\": \"And in this case, we have this difference of 456 between these two images.\",\n",
      "  \"24\": \"And you can see it's actually pretty short and pretty concise, because we've made use of many of these vectorized operations offered by NumPy.\",\n",
      "  \"25\": \"Well, training is probably constant because we don't really need to do anything.\",\n",
      "  \"26\": \"So from this perspective, this nearest neighbor algorithm is actually a little bit backwards.\",\n",
      "  \"27\": \"We've gone and computed what is the nearest training, what is the nearest example in the training set.\",\n",
      "  \"28\": \"Maybe those points actually should have been green.\",\n",
      "  \"29\": \"You can imagine slightly more complex ways of doing this.\",\n",
      "  \"30\": \"And you can imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.\"\n",
      "}\n",
      "Script saved to ../spms/11_spm.json\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "PDF = '../pdfs'\n",
    "TOC = '../tocs'\n",
    "IMAGE = '../images'\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"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\"\n",
    "   \n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "# Path to your image and script\n",
    "image_directory = os.path.join(IMAGE, f\"{str(project_id)}\")\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "\n",
    "image_paths = sorted([os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith('.png')])\n",
    "\n",
    "# print(image_paths)\n",
    "encoded_images = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encode_image(image)}\"}} for image in image_paths]\n",
    "\n",
    "# Read the script\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Creating the content for the messages\n",
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"Given the following lecture notes images and the lecture script, \"\n",
    "            \"please distribute the script content accurately to each page of the lecture notes. \"\n",
    "            \"The output should be in the format: {\\\"1\\\": \\\"script\\\", \\\"2\\\": \\\"script\\\", ...}. \"\n",
    "            \"Each key should correspond to the page number in the lecture notes where the script content appears, \"\n",
    "            \"and the value should be the first sentence of the script content for that page. \"\n",
    "            \"The value corresponding to a larger key must be a sentence that appears later in the script.\"\n",
    "            f\"The number of dictionary keys must be equal to {len(encoded_images)}. \"\n",
    "            \"The original format of the script, including uppercase and lowercase letters, punctuation marks such as periods and commas, must be preserved without any alterations. \"\n",
    "            f\"Lecture script: {script_content} \"\n",
    "        )\n",
    "    }\n",
    "] + encoded_images\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "# Get the response\n",
    "response_data = response.json()\n",
    "\n",
    "# Check if 'choices' key exists in the response\n",
    "if 'choices' in response_data and len(response_data['choices']) > 0:\n",
    "    # Parse the table of contents from the response\n",
    "    script_text = response_data['choices'][0]['message']['content']\n",
    "\n",
    "    print(\"Raw script text:\", script_text)\n",
    "    \n",
    "    # Convert the script text to JSON format\n",
    "    try:\n",
    "        script_data = json.loads(script_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "else:\n",
    "    print(\"Error: 'choices' key not found in the response\")\n",
    "    print(response_data)\n",
    "    script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "# Save the script data as a JSON file\n",
    "script_json_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "with open(script_json_path, \"w\") as json_file:\n",
    "    json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Script saved to {script_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase the text\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "def find_best_match(script_content, sentence):\n",
    "    # Initialize the SequenceMatcher with the script content and the sentence\n",
    "    matcher = SequenceMatcher(None, script_content, sentence)\n",
    "    match = matcher.find_longest_match(0, len(script_content), 0, len(sentence))\n",
    "    \n",
    "    if match.size > 0:\n",
    "        return match.a  # Start index of the match in the script content\n",
    "    else:\n",
    "        return -1  # No match found\n",
    "\n",
    "def match_paragraphs_2(script_content, first_sentences):\n",
    "    matched_paragraphs = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "    \n",
    "    script_content_processed = preprocess_text(script_content)\n",
    "\n",
    "    for i, page in enumerate(page_numbers):\n",
    "        current_sentence = preprocess_text(first_sentences[page])\n",
    "        next_sentence = (\n",
    "            preprocess_text(first_sentences[str(int(page) + 1)])\n",
    "            if i < len(page_numbers) - 1\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            start_index = find_best_match(script_content_processed, current_sentence)\n",
    "            if start_index == -1:\n",
    "                print(f\"Match not found for page {page}\")\n",
    "                continue\n",
    "\n",
    "            if i == len(page_numbers) - 1:  # Last page\n",
    "                matched_paragraphs[page] = script_content[start_index:].strip()\n",
    "                print(page, start_index)\n",
    "            else:\n",
    "                end_index = find_best_match(script_content_processed, next_sentence)\n",
    "                if end_index == -1:\n",
    "                    matched_paragraphs[page] = script_content[start_index:].strip()\n",
    "                else:\n",
    "                    matched_paragraphs[page] = script_content[start_index:end_index].strip()\n",
    "                print(page, start_index, end_index)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred at page {page}: {str(e)}\")\n",
    "\n",
    "    return matched_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Okay, so welcome to Lecture 2 of CS231N.', '2': 'One is Piazza, so I saw, when I checked yesterday,', '3': 'Assignment one will be up later today, probably sometime this afternoon,', '4': \"So in this assignment, you'll be implementing your own k-nearest neighbor classifier,\", '5': \"The other thing I wanted to talk about is that we're happy to announce that we got,\", '6': 'So the last lecture we talked a little bit about this task of image classification,', '7': 'And the job of the computer is to look at the picture and assign it one of these fixed category labels.', '8': 'So for example, if we took this same cat, and if the cat happened to sit still and not even twitch,', '9': 'Objects can also deform.', '10': 'I think cats are maybe among the more deformable of animals that you might see out there.', '11': 'There can also be problems of occlusion, where you might only see part of a cat, like just the face,', '12': \"There's also this problem of inter-class variation, that this one notion of catness\", '13': 'And cats can come in different shapes and sizes and colors and ages.', '14': 'And you can see that they look kind of visually similar to the training images,', '15': 'If you think about, like, if it was your first day programming', '16': 'So the first, so this class is primarily about neural networks and convolutional neural networks and deep learning and all that.', '17': 'So probably the simplest classifier you can imagine is something we call nearest neighbor.', '18': 'So to be a little bit more concrete, you might imagine working on this dataset called CIFAR-10,', '19': 'Test image and go and try to find the most similar image in the training data to that new image and', '20': \"So in the example in the previous slide, we've used what's called the L1 distance,\", '21': \"So here's some full Python code for implementing this nearest neighbor classifier.\", '22': \"And now at test time, we're gonna take in our image\", '23': 'And once we move to k equals three, you can see that that spurious yellow point in the middle of the green cluster', '24': \"And you'll see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting.\", '25': \"for each pixel in this entire plane, we've gone and computed what is the nearest training,\", '26': 'So here, our training set consists of these points in the two-dimensional plane,', '27': 'And you can see that this nearest neighbor classifier is just sort of carving up the space', '28': 'But this classifier is maybe not so great, and by looking at this picture, we can start to see', '29': 'This kind of motivates a slight generalization of this algorithm called k-nearest neighbors.', '30': \"Here I've colored in red and green, which images would actually be classified correctly or incorrectly\"}\n",
      "1 0 824\n",
      "2 824 1995\n",
      "3 1995 2553\n",
      "4 2553 3764\n",
      "5 3764 5143\n",
      "6 5143 5768\n",
      "7 5768 7158\n",
      "8 7158 7842\n",
      "9 7842 7867\n",
      "10 7867 8107\n",
      "11 8107 8744\n",
      "12 8744 8882\n",
      "13 8882 15752\n",
      "14 15752 10738\n",
      "15 10738 13826\n",
      "16 13826 14188\n",
      "17 14188 14765\n",
      "18 14765 14501\n",
      "19 14501 17021\n",
      "20 17021 17912\n",
      "21 17912 18354\n",
      "22 18354 22701\n",
      "23 22701 22986\n",
      "24 22986 20806\n",
      "25 20806 20510\n",
      "26 20510 21087\n",
      "27 21087 21230\n",
      "28 21230 21942\n",
      "29 21942 24581\n",
      "30 24581 25062\n",
      "Matched paragraphs saved to ../spms/80_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "    \n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    matched_paragraphs = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "\n",
    "    # # Function to remove punctuation and lowercase the text\n",
    "    # def preprocess_text(text):\n",
    "    #     return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "    script_content_processed = script_content\n",
    "    prev_start_index = 0\n",
    "    for i, page in enumerate(page_numbers):\n",
    "        current_sentence = first_sentences[page]\n",
    "        prev_page = str(int(page) - 1)\n",
    "        next_sentence = (\n",
    "            first_sentences[str(int(page) + 1)]\n",
    "            if i < len(page_numbers) - 1\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            start_index = script_content_processed.find(current_sentence)\n",
    "            if start_index == -1:\n",
    "                start_index = find_best_match(script_content_processed, current_sentence)\n",
    "            if i == len(page_numbers) - 1:  # Last page\n",
    "                end_index = len(script_content)\n",
    "            else:\n",
    "                end_index = script_content_processed.find(next_sentence)\n",
    "                if end_index == -1:\n",
    "                    end_index = find_best_match(script_content_processed, next_sentence)\n",
    "            \n",
    "            # if start_index > end_index:\n",
    "            #     start_index, end_index = end_index, start_index\n",
    "            #     matched_paragraphs[prev_page] = script_content[prev_start_index:start_index].strip()\n",
    "            #     print(prev_page, prev_start_index, start_index)\n",
    "            \n",
    "            print(page, start_index, end_index)\n",
    "\n",
    "            matched_paragraphs[page] = script_content[start_index:end_index].strip()\n",
    "            prev_start_index = start_index\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred at page {page}: {str(e)}\")\n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "\n",
    "project_id = 80\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, f\"10_transcription.json\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "print(first_sentences)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': \"Okay, so welcome to Lecture 2 of CS231N. On Tuesday, we, just recall, we sort of gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. And today, we're really going to dive in for the first time into the details, and we'll start to see in much more depth exactly how some of these learning algorithms actually work in practice. So the first lecture of the class is probably the sort of the largest big picture vision, and the majority of the lectures in this class will be much more detail-oriented, and much more focused on the specific mechanics of these different algorithms. So today, we'll see our first learning algorithm, and that'll be really exciting, I think. But before we get to that, I wanted to talk about a couple administrative issues.\",\n",
       " '2': \"One is Piazza, so I saw, when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza, which means that there are several hundred of you who are not yet there. So we really want Piazza to be the main source of communication between the students and the course staff. So we've gotten a lot of questions to the staff list about project ideas, or questions about midterm attendance, or poster session attendance, and any sort of questions like that should really go to Piazza. You'll probably get answers to your questions faster on Piazza, because all the TAs are knowing to check that. And it's sort of easy for emails to get lost in the shuffle if you just send to the course list. It's also come to my attention that some SCPD students are having a bit of a hard time signing up for Piazza. SCPD students are supposed to receive a .stanford, at stanford.edu email address. So once you get that email address, then you can use the Stanford email to sign into Piazza. Probably that doesn't affect those of you who are sitting in the room right now, but for those students listening on SCPD. The next administrative issue is about assignment one.\",\n",
       " '3': \"Assignment one will be up later today, probably sometime this afternoon, but I promise before I go to sleep tonight, it'll be up. But if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. It'll be pretty much the same content. We're just reshuffling it a little bit to make it, like for example, upgrading to work with Python 3 rather than Python 2.7, and some of these minor cosmetic changes. But the content of the assignment will still be the same as last year.\",\n",
       " '4': \"So in this assignment, you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. You'll also implement several different linear classifiers, including the SVM and Softmax, as well as a simple two-layer neural network. And we'll cover all of this content over the next couple of lectures. So all of our assignments are using Python and NumPy. If you aren't familiar with Python or NumPy, then we have written a tutorial that you can find on the course website to try and get you up to speed. But this is actually pretty important. NumPy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. So this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. And you'll get a lot of practice with this on the first assignment. So for those of you who don't have a lot of experience with MATLAB or NumPy or other types of vectorized tensor computation, I recommend that you start looking at this assignment pretty early and also read carefully through the tutorial.\",\n",
       " '5': \"The other thing I wanted to talk about is that we're happy to announce that we got, we're officially supported through Google Cloud for this class. So Google Cloud is somewhat similar to Amazon AWS. You can go and start virtual machines up in the cloud. These virtual machines can have GPUs. So we have a, well, we're working on the tutorial for exactly how to use Google Cloud and get it to work for the assignments. But our intention is that you'll be able to just download some image and it'll be very seamless for you to work on the assignment on one of these instances on the cloud. And because Google has very generously supported this course, we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class. So you can feel free to use these for the assignments and also for the course projects when you want to start using GPUs and larger machines and whatnot. So we'll post more details about that probably on Piazza later today. But I just wanted to mention, because I know there had been a couple of questions about can I use my laptop? Do I have to run on Corn? Do I have to, whatever. And the answer is that you'll be able to run on Google Cloud and we'll provide you some coupons for that. Yeah, so those are kind of the major administrative issues I wanted to talk about today. And then let's dive into the content.\",\n",
       " '6': \"So the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision. And this is something that we'll really focus on throughout the course of the class is exactly how do we work on this image classification task. So a little bit more concretely, when you're doing image classification, you receive, your system receives some input image, which is this cute cat in this example. And the system is aware of some predetermined set of categories or labels. So these might be like a dog or a cat or a truck or a plane. And there's some fixed set of category labels.\",\n",
       " '7': \"And the job of the computer is to look at the picture and assign it one of these fixed category labels. This seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks. But this is actually a really, really hard problem for a machine. So if you dig in and think about actually what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it. And the computer really is representing the image as this gigantic grid of numbers. So you just, so the image might be something like 800 by 600 pixels. And each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. So to the computer, this is just a gigantic grid of numbers. And it's very difficult to distill the cat-ness out of this giant array of thousands or whatever, very many different numbers. And this, so we refer to this problem as the semantic gap. That this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image. And there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. And this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\",\n",
       " '8': \"So for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never gonna happen, but we moved the camera to the other side, then every single grid, every single pixel in this giant grid of numbers would be completely different. But somehow, it's still representing the same cat. And our algorithms need to be robust to this. But not only viewpoint is one problem, another is illumination. There can be different lighting conditions going on in the scene. Whether the cat is appearing in this very dark, moody scene or in this very bright, sunlit scene, it's still a cat. And our algorithms need to be robust to that.\",\n",
       " '9': 'Objects can also deform.',\n",
       " '10': 'I think cats are maybe among the more deformable of animals that you might see out there. And cats can really assume a lot of different varied poses and positions. And our algorithms should be robust to these different kinds of transforms.',\n",
       " '11': \"There can also be problems of occlusion, where you might only see part of a cat, like just the face, or in this extreme example, just a tail peeking out from under the couch cushion. But in these cases, it's pretty easy for you as a person to realize that this is probably a cat and you still recognize these images as cats. And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems of background clutter, where maybe the foreground object, the cat, could actually look quite similar in appearance to the background. And this is another thing that we need to handle.\",\n",
       " '12': \"There's also this problem of inter-class variation, that this one notion of catness actually spans a lot of different visual appearances.\",\n",
       " '13': \"And cats can come in different shapes and sizes and colors and ages. And our algorithm, again, needs to work and handle all these different variations. So this is actually a really, really challenging problem. And it's sort of easy to forget how easy this is, because so much of your brain is specifically tuned for dealing with these things. But now, if we want our computer programs to deal with all of these problems all simultaneously, and not just for cats, by the way, but for just about any object category you could imagine, this is a fantastically challenging problem. And it's actually somewhat miraculous that this works at all, in my opinion. But actually, not only does it work, but these things work very close to human accuracy in some limited situations. And take maybe only hundreds of milliseconds to do so. So this is some pretty amazing, incredible technology, in my opinion. And over the course of the rest of the class, we'll really see what kinds of advancements have made this possible. So now, if you kind of think about what is the API for writing an image classifier, you might sit down and try to write a method in Python like this, where you want to take in an image and then do some crazy magic, and then eventually spit out this class label to say cat or dog or whatnot. And there's really no obvious way to do this, right? Like, if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or even do something like RSA encryption, you sort of can write down an algorithm and enumerate all the steps that need to happen in order for these things to work. But when we're trying to recognize objects or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense for how you might go about recognizing these objects. So this is, again, quite challenging.\",\n",
       " '15': \"If you think about, like, if it was your first day programming and you had to sit down and write this function, I think most people would be in trouble. That being said, people have definitely made explicit attempts to try to write sort of hand-coded rules for recognizing different animals. So we touched on this a little bit in the last lecture. But maybe one idea for cats is that, you know, we know that cats have ears and eyes and mouths and noses, and we know that edges are, from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries and say that, you know, if we have maybe three lines meeting this way, then it might be a corner and an ear has one corner here and one corner there and one corner there, and then kind of write down this explicit set of rules for recognizing cats. But this turns out not to work very well. One, it's super brittle, and two, say, if you want to start over for another object category and maybe not worry about cats but talk about trucks or dogs or fishes or something else, then you need to start all over again. So this is really not a very scalable approach. We want to come up with some algorithm or some method for these recognition tasks, which scales much more naturally to all the variety of objects in the world. So the insight that sort of makes this all work is this idea of the data-driven approach, is that rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this. And we can actually use tools like Google Image Search or something like that to go out and collect a very large number of examples of these different categories. By the way, this actually takes quite a lot of effort to go out and actually collect these datasets, but luckily, there's a lot of really good, high-quality datasets out there already for you to use. Then once we get this dataset, we train this machine learning classifier that is gonna ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. Then finally, we'll use this trained model and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here, our API has changed a little bit. Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One that's gonna, called train, that's gonna input images and labels and then output a model. And then separately, another function called predict, which will input the model and then make predictions for images. And this is kind of the key insight that allowed all these things to start working really well in the last, over the last 10, 20 years or so.\",\n",
       " '16': \"So the first, so this class is primarily about neural networks and convolutional neural networks and deep learning and all that. But there's, this idea of a data-driven approach is much more general than just deep learning. And I think it's useful to sort of step through this process for a very simple classifier first before we get to these big, complex ones.\",\n",
       " '17': \"So probably the simplest classifier you can imagine is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So during the training step, we won't do anything. We'll just memorize all of the training data. So this is very simple. And now during the prediction step, we're gonna take some new\",\n",
       " '19': 'image and go and try to find the most similar image in the training data to that new image and now predict the label of that most similar image. Very simple algorithm. But it sort of has a lot of these nice properties with respect to data-drivenness and whatnot.',\n",
       " '18': \"So to be a little bit more concrete, you might imagine working on this dataset called CIFAR-10, which is very commonly used in machine learning as kind of a small test case. And you'll be working with this dataset on your homework. So the CIFAR-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. And for each of those 10 categories, it provides 10,000, sorry, it provides 50,000 training images, roughly evenly distributed across these 10 categories, and then 10,000 additional testing images that you're supposed to test your algorithm on. So now if you think about, so here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR-10. So on this grid on the right, for the leftmost column, gives a test image in the CIFAR-10 dataset. And now on the right, we see we've sorted the training images and show the most similar training images to each of these test examples.\",\n",
       " '14': \"And you can see that they look kind of visually similar to the training images, although they are not always correct. So maybe on this second row, we see that the testing, this is kind of hard to see because these images are 32 by 32 pixels. You need to really dive in there and try to make your best guess. But this image is a dog, and its nearest neighbor is also a dog. But this next one, I think is actually a deer or a horse or something else. But you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. So if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set. And now the closest example, we know it's label because it comes from the training set, and now we'll simply say that this testing image is also a dog. You can see kind of from these examples that this is probably not gonna work very well, but it's still kind of a nice example to work through. But then one detail that we need to know is given a pair of images, how can we actually compare them? Because if we're gonna take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\",\n",
       " '20': \"So in the example in the previous slide, we've used what's called the L1 distance, also sometimes called the Manhattan distance. So this is a really sort of simple, easy idea for comparing images. And that's that we're gonna take the, just compare individual pixels in these images. So supposing that our test image is maybe just a tiny four by four image of pixel values, then we're gonna take this upper left-hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. And then sum all these up across all the pixels in the image. So this is kind of a stupid way to compare images, but it does some reasonable things sometimes. But this gives us a very concrete way to measure the difference between two images. And in this case, we have this difference of 456 between these two images.\",\n",
       " '21': \"So here's some full Python code for implementing this nearest neighbor classifier. And you can see it's actually pretty short and pretty concise, because we've made use of many of these vectorized operations offered by NumPy. So here we can see that the training, this train function that we talked about earlier is, again, very simple in the case of nearest neighbor. You just memorize the training data. There's not really much to do here.\",\n",
       " '22': \"And now at test time, we're gonna take in our image and then go in and compare, using this L1 distance function, our test image to each of these training examples. And find the most similar example in the training set. And you can see that we're actually able to do this in just one or two lines of Python code, using, by utilizing these vectorized operations in NumPy. So this is something that you'll get practice with on the first assignment. So now a couple of questions about this simple classifier. First, if we have n examples in our training set, then how fast can we expect training and testing to be? Well, training is probably constant because we don't really need to do anything. We just need to memorize the data. And if you're just copying a pointer, that's gonna be constant time, no matter how big your data set is. But now at test time, we need to do this comparison step and compare our test image to each of the n training examples in the data set. And this is actually quite slow. So this is actually somewhat backwards, if you think about it. Because, you know, in practice, we want our classifiers to be slow at training time and then fast at testing time. Because you might imagine that a classifier might go and be trained in a data center somewhere, and you can afford to spend a lot of computation at training time to make the classifier really good. But then when you go and deploy the classifier at test time, you want it to run on your mobile phone or in the browser or some other low-power device, and you really want the testing time performance of your classifier to be quite fast. So from this perspective, this nearest neighbor algorithm is actually a little bit backwards. And we'll see that once we move to convolutional neural networks and other types of parametric models, they'll be the reverse of this, where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. So then the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? So here we've drawn what we call the decision regions of a nearest neighbor classifier. So here\",\n",
       " '26': 'our training set consists of these points in the two-dimensional plane, where the color of the point represents the category or the class label of that point. So here we see we have five classes, and some blue ones up in the corner here, some purple ones in the upper right-hand corner. And now',\n",
       " '25': \"for each pixel in this entire plane, we've gone and computed what is the nearest training, what is the nearest example in the training set. What is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. So\",\n",
       " '27': 'you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points.',\n",
       " '28': \"But this classifier is maybe not so great, and by looking at this picture, we can start to see some of the problems that might come out with a nearest neighbor classifier. For one, this central region actually contains mostly green points, but one little yellow point in the middle. But because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of the green cluster, and that's maybe not so great. Maybe those points actually should have been green. And then similarly, we also see these sort of fingers of the different, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. So t\",\n",
       " '29': \"his kind of motivates a slight generalization of this algorithm called k-nearest neighbors. So rather than just looking for the single nearest neighbor, instead, we'll do something a little bit fancier and find k of our nearest neighbors according to our distance metric, and then take a vote among each of our neighbors, and then predict the majority vote among our neighbors. You can imagine slightly more complex ways of doing this. Maybe you vote weighted on the distance or something like that, but the simplest thing that tends to work pretty well is taking a majority vote. So here we've shown the exact same set of points using this k equals one nearest neighbor classifier, as well as k equals three and k equals five in the middle and on the right.\",\n",
       " '23': \"And once we move to k equals three, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. Now this entire green portion in the middle is all being classified as green. And you'll also\",\n",
       " '24': \"see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then once we move to the k equals five case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So this is generally something, so generally when you're using nearest neighbor classifiers, you almost always want to use some value of k, which is larger than one, because this tends to smooth out your decision boundaries and lead to better results. So if we, to kind of, oh yeah, question? Yeah, so the question is, what is the deal with these white regions? And the white regions are where there was no majority among the k nearest neighbors. You could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners. But for this simple example, we're just coloring it white to indicate that there was no nearest neighbor in those points. So we already kind of saw, so I like to, whenever we're thinking about computer vision, I think it's really useful to kind of flip back and forth between several different viewpoints. One is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images, because the pixels of the image actually allow us to think of these images as high dimensional vectors. And it's sort of useful to ping pong back and forth between these two different viewpoints. So then, when sort of taking this k nearest neighbor and going back to the images, you can see that it's actually not very good.\",\n",
       " '30': \"Here I've colored in red and green, which images would actually be classified correctly or incorrectly according to their nearest neighbor. And you can see that it's really not very good. But maybe if we used a larger value of k, then this would involve actually voting among maybe the top three or the top five, or maybe even the whole row. And you can imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Okay, so welcome to Lecture 2 of CS231N.', '2': 'One is Piazza, so I saw, when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza, which means that there are several hundred of you who are not yet there.', '3': 'The next administrative issue is about assignment one.', '4': \"So in this assignment, you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture.\", '5': 'But this is actually pretty important.', '6': \"And you'll get a lot of practice with this on the first assignment.\", '7': 'So Google Cloud is somewhat similar to Amazon AWS.', '8': \"And because Google has very generously supported this course, we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class.\", '9': 'Yeah, so those are kind of the major administrative issues I wanted to talk about today.', '10': 'So the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision.', '11': 'This seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks.', '12': 'And this, so we refer to this problem as the semantic gap.', '13': 'So for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never gonna happen, but we moved the camera to the other side, then every single grid, every single pixel in this giant grid of numbers would be completely different.', '14': 'There can also be problems of occlusion, where you might only see part of a cat, like just the face, or in this extreme example, just a tail peeking out from under the couch cushion.', '15': \"There's also this problem of inter-class variation, that this one notion of catness actually spans a lot of different visual appearances.\", '16': \"And it's actually somewhat miraculous that this works at all, in my opinion.\", '17': 'But now if we want our computer programs to deal with all of these problems all simultaneously, and not just for cats, by the way, but for just about any object category you could imagine, this is a fantastically challenging problem.', '18': \"And over the course of the rest of the class, we'll really see what kinds of advancements have made this possible.\", '19': 'So maybe one idea for cats is that, you know, we know that cats have ears and eyes and mouths and noses, and we know that edges are, from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition.', '20': \"So the insight that sort of makes this all work is this idea of the data-driven approach, is that rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this.\", '21': 'Then once we get this dataset, we train this machine learning classifier that is gonna ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories.', '22': 'So here our API has changed a little bit.', '23': 'So probably the simplest classifier you can imagine is something we call nearest neighbor.', '24': 'Very simple algorithm.', '25': \"So now if you think about, so here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR-10.\", '26': \"So in the example in the previous slide, we've used what's called the L1 distance, also sometimes called the Manhattan distance.\", '27': 'And in this case, we have this difference of 456 between these two images.', '28': \"Because in the example in the previous slide, we've used the L1 distance function to predict the class of these images.\", '29': \"So here's some full Python code for implementing this nearest neighbor classifier.\", '30': 'So now a couple of questions about this simple classifier.'}\n",
      "['1', '19', '17', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '18', '20', '21', '22', '23', '24', '25', '26', '28', '27', '29', '30']\n",
      "1 0 10\n",
      "2 10 39\n",
      "3 39 824\n",
      "4 824 1940\n",
      "5 1940 2553\n",
      "6 2553 3098\n",
      "7 3098 3458\n",
      "8 3458 3912\n",
      "9 3912 4352\n",
      "10 4352 5016\n",
      "11 5016 5143\n",
      "12 5143 5872\n",
      "13 5872 6724\n",
      "14 6724 7158\n",
      "15 7158 8107\n",
      "16 8107 8744\n",
      "17 8744 9460\n",
      "18 9460 9778\n",
      "19 9778 12185\n",
      "20 12185 12940\n",
      "21 12940 13329\n",
      "22 13329 14188\n",
      "23 14188 14647\n",
      "24 14647 15375\n",
      "25 15375 17021\n",
      "26 17021 17023\n",
      "27 17023 17837\n",
      "28 17837 17912\n",
      "29 17912 18800\n",
      "30 18800 25062\n",
      "Matched paragraphs saved to ../spms/11_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "def find_best_match(script_content, sentence):\n",
    "    # Initialize the SequenceMatcher with the script content and the sentence\n",
    "    matcher = SequenceMatcher(None, script_content, sentence)\n",
    "    match = matcher.find_longest_match(0, len(script_content), 0, len(sentence))\n",
    "    \n",
    "    if match.size > 0:\n",
    "        return match.a  # Start index of the match in the script content\n",
    "    else:\n",
    "        return -1  # No match found\n",
    "\n",
    "# Function to find start indices\n",
    "def find_start_indices(script_content, first_sentences):\n",
    "    start_indices = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "\n",
    "    for page in page_numbers:\n",
    "        current_sentence = first_sentences[page]\n",
    "        start_index = script_content.find(current_sentence)\n",
    "        if start_index == -1:\n",
    "            start_index = find_best_match(script_content, current_sentence)\n",
    "        start_indices[page] = start_index\n",
    "    \n",
    "    return start_indices\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    # Step 1: Find all start indices\n",
    "    start_indices = find_start_indices(script_content, first_sentences)\n",
    "    \n",
    "    # Step 2: Sort the pages by start index\n",
    "    sorted_pages = sorted(start_indices, key=lambda page: start_indices[page])\n",
    "    print(sorted_pages)\n",
    "    # Step 3: Create the matched paragraphs\n",
    "    matched_paragraphs = {}\n",
    "    for i, page in enumerate(sorted_pages):\n",
    "        start_index = start_indices[page]\n",
    "        if i < len(sorted_pages) - 1:\n",
    "            next_page = sorted_pages[i + 1]\n",
    "            end_index = start_indices[next_page]\n",
    "        else:\n",
    "            end_index = len(script_content)  # Last page\n",
    "        \n",
    "        matched_paragraphs[str(i+1)] = script_content[start_index:end_index].strip()\n",
    "\n",
    "        # print(page, start_index, end_index)\n",
    "        print(str(i+1), start_index, end_index)\n",
    " \n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "project_id = 11\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "print(first_sentences)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '2': 824, '3': 1940, '4': 2553, '5': 3098, '6': 3458, '7': 3912, '8': 4352, '9': 5016, '10': 5143, '11': 5872, '12': 6724, '13': 7158, '14': 8107, '15': 8744, '16': 9460, '17': 9462, '18': 9778, '19': 11090, '20': 12185, '21': 12940, '22': 13329, '23': 14188, '24': 14647, '25': 15375, '26': 17021, '27': 17837, '28': 18444, '29': 21104, '30': 23840}\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30']\n",
      "1 0 824\n",
      "2 824 1940\n",
      "3 1940 2553\n",
      "4 2553 3098\n",
      "5 3098 3458\n",
      "6 3458 3912\n",
      "7 3912 4352\n",
      "8 4352 5016\n",
      "9 5016 5143\n",
      "10 5143 5872\n",
      "11 5872 6724\n",
      "12 6724 7158\n",
      "13 7158 8107\n",
      "14 8107 8744\n",
      "15 8744 9460\n",
      "16 9460 9462\n",
      "17 9462 9778\n",
      "18 9778 11090\n",
      "19 11090 12185\n",
      "20 12185 12940\n",
      "21 12940 13329\n",
      "22 13329 14188\n",
      "23 14188 14647\n",
      "24 14647 15375\n",
      "25 15375 17021\n",
      "26 17021 17837\n",
      "27 17837 18444\n",
      "28 18444 21104\n",
      "29 21104 23840\n",
      "30 23840 25062\n",
      "Matched paragraphs saved to ../spms/11_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "# Function to remove the first word from a sentence\n",
    "def remove_first_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return ' '.join(words[1:])\n",
    "\n",
    "# Function to find the best match using difflib\n",
    "def find_best_match(script_content, sentence, min_index=0):\n",
    "    matcher = SequenceMatcher(None, script_content, sentence)\n",
    "    match = matcher.find_longest_match(min_index, len(script_content), 0, len(sentence))\n",
    "    if match.size > 0:\n",
    "        return match.a  # Return the start index of the match\n",
    "    return -1  # No match found\n",
    "\n",
    "# Function to find start indices with a fallback strategy\n",
    "def find_start_indices(script_content, first_sentences):\n",
    "    start_indices = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "    last_index = 0  # Keep track of the last found index to ensure we find subsequent matches after this\n",
    "\n",
    "    script_content_lower = script_content.lower()\n",
    "\n",
    "    for page in page_numbers:\n",
    "        current_sentence = first_sentences[page]\n",
    "        current_sentence_lower = current_sentence.lower()\n",
    "\n",
    "        # Attempt 1: Direct search after last_index\n",
    "        start_index = script_content_lower.find(current_sentence_lower, last_index)\n",
    "        \n",
    "        # Attempt 2: Search after removing the first word\n",
    "        if start_index == -1:\n",
    "            modified_sentence = remove_first_word(current_sentence_lower)\n",
    "            start_index = script_content_lower.find(modified_sentence, last_index)\n",
    "        \n",
    "        # Attempt 3: Fallback to best match if still not found\n",
    "        if start_index == -1:\n",
    "            start_index = find_best_match(script_content_lower, current_sentence_lower, min_index=last_index)\n",
    "        \n",
    "        \n",
    "        start_indices[page] = start_index\n",
    "        last_index = start_index  # Update last_index to the current start_index\n",
    "    \n",
    "    return start_indices\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    # Step 1: Find all start indices\n",
    "    start_indices = find_start_indices(script_content, first_sentences)\n",
    "    print(start_indices)\n",
    "    # Step 2: Sort the pages by start index\n",
    "    sorted_pages = sorted(start_indices, key=lambda page: start_indices[page])\n",
    "    print(sorted_pages)\n",
    "    # Step 3: Create the matched paragraphs\n",
    "    matched_paragraphs = {}\n",
    "    for i, page in enumerate(sorted_pages):\n",
    "        start_index = start_indices[page]\n",
    "        if i < len(sorted_pages) - 1:\n",
    "            next_page = sorted_pages[i + 1]\n",
    "            end_index = start_indices[next_page]\n",
    "        else:\n",
    "            end_index = len(script_content)  # Last page\n",
    "        \n",
    "        matched_paragraphs[str(i+1)] = script_content[start_index:end_index].strip()\n",
    "\n",
    "        # print(page, start_index, end_index)\n",
    "        print(str(i+1), start_index, end_index)\n",
    " \n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "project_id = 11\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file with sorted keys\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': \"Okay, so welcome to Lecture 2 of CS231N. On Tuesday, we, just recall, we sort of gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. And today, we're really going to dive in for the first time into the details, and we'll start to see in much more depth exactly how some of these learning algorithms actually work in practice. So the first lecture of the class is probably the sort of the largest big picture vision, and the majority of the lectures in this class will be much more detail-oriented, and much more focused on the specific mechanics of these different algorithms. So today, we'll see our first learning algorithm, and that'll be really exciting, I think. But before we get to that, I wanted to talk about a couple administrative issues.\",\n",
       " '2': \"One is Piazza, so I saw, when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza, which means that there are several hundred of you who are not yet there. So we really want Piazza to be the main source of communication between the students and the course staff. So we've gotten a lot of questions to the staff list about project ideas, or questions about midterm attendance, or poster session attendance, and any sort of questions like that should really go to Piazza. You'll probably get answers to your questions faster on Piazza, because all the TAs are knowing to check that. And it's sort of easy for emails to get lost in the shuffle if you just send to the course list. It's also come to my attention that some SCPD students are having a bit of a hard time signing up for Piazza. SCPD students are supposed to receive a .stanford, at stanford.edu email address. So once you get that email address, then you can use the Stanford email to sign into Piazza. Probably that doesn't affect those of you who are sitting in the room right now, but for those students listening on SCPD.\",\n",
       " '3': \"The next administrative issue is about assignment one. Assignment one will be up later today, probably sometime this afternoon, but I promise before I go to sleep tonight, it'll be up. But if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. It'll be pretty much the same content. We're just reshuffling it a little bit to make it, like for example, upgrading to work with Python 3 rather than Python 2.7, and some of these minor cosmetic changes. But the content of the assignment will still be the same as last year.\",\n",
       " '4': \"So in this assignment, you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. You'll also implement several different linear classifiers, including the SVM and Softmax, as well as a simple two-layer neural network. And we'll cover all of this content over the next couple of lectures. So all of our assignments are using Python and NumPy. If you aren't familiar with Python or NumPy, then we have written a tutorial that you can find on the course website to try and get you up to speed.\",\n",
       " '5': 'But this is actually pretty important. NumPy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. So this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations.',\n",
       " '6': \"And you'll get a lot of practice with this on the first assignment. So for those of you who don't have a lot of experience with MATLAB or NumPy or other types of vectorized tensor computation, I recommend that you start looking at this assignment pretty early and also read carefully through the tutorial. The other thing I wanted to talk about is that we're happy to announce that we got, we're officially supported through Google Cloud for this class.\",\n",
       " '7': \"So Google Cloud is somewhat similar to Amazon AWS. You can go and start virtual machines up in the cloud. These virtual machines can have GPUs. So we have a, well, we're working on the tutorial for exactly how to use Google Cloud and get it to work for the assignments. But our intention is that you'll be able to just download some image and it'll be very seamless for you to work on the assignment on one of these instances on the cloud.\",\n",
       " '8': \"And because Google has very generously supported this course, we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class. So you can feel free to use these for the assignments and also for the course projects when you want to start using GPUs and larger machines and whatnot. So we'll post more details about that probably on Piazza later today. But I just wanted to mention, because I know there had been a couple of questions about can I use my laptop? Do I have to run on Corn? Do I have to, whatever. And the answer is that you'll be able to run on Google Cloud and we'll provide you some coupons for that.\",\n",
       " '9': \"Yeah, so those are kind of the major administrative issues I wanted to talk about today. And then let's dive into the content.\",\n",
       " '10': \"So the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision. And this is something that we'll really focus on throughout the course of the class is exactly how do we work on this image classification task. So a little bit more concretely, when you're doing image classification, you receive, your system receives some input image, which is this cute cat in this example. And the system is aware of some predetermined set of categories or labels. So these might be like a dog or a cat or a truck or a plane. And there's some fixed set of category labels. And the job of the computer is to look at the picture and assign it one of these fixed category labels.\",\n",
       " '11': \"This seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks. But this is actually a really, really hard problem for a machine. So if you dig in and think about actually what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it. And the computer really is representing the image as this gigantic grid of numbers. So you just, so the image might be something like 800 by 600 pixels. And each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. So to the computer, this is just a gigantic grid of numbers. And it's very difficult to distill the cat-ness out of this giant array of thousands or whatever, very many different numbers.\",\n",
       " '12': \"And this, so we refer to this problem as the semantic gap. That this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image. And there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. And this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely.\",\n",
       " '13': \"So for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never gonna happen, but we moved the camera to the other side, then every single grid, every single pixel in this giant grid of numbers would be completely different. But somehow, it's still representing the same cat. And our algorithms need to be robust to this. But not only viewpoint is one problem, another is illumination. There can be different lighting conditions going on in the scene. Whether the cat is appearing in this very dark, moody scene or in this very bright, sunlit scene, it's still a cat. And our algorithms need to be robust to that. Objects can also deform. I think cats are maybe among the more deformable of animals that you might see out there. And cats can really assume a lot of different varied poses and positions. And our algorithms should be robust to these different kinds of transforms.\",\n",
       " '14': \"There can also be problems of occlusion, where you might only see part of a cat, like just the face, or in this extreme example, just a tail peeking out from under the couch cushion. But in these cases, it's pretty easy for you as a person to realize that this is probably a cat and you still recognize these images as cats. And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems of background clutter, where maybe the foreground object, the cat, could actually look quite similar in appearance to the background. And this is another thing that we need to handle.\",\n",
       " '15': \"There's also this problem of inter-class variation, that this one notion of catness actually spans a lot of different visual appearances. And cats can come in different shapes and sizes and colors and ages. And our algorithm, again, needs to work and handle all these different variations. So this is actually a really, really challenging problem. And it's sort of easy to forget how easy this is, because so much of your brain is specifically tuned for dealing with these things. But now, if we want our computer programs to deal with all of these problems all simultaneously, and not just for cats, by the way, but for just about any object category you could imagine, this is a fantastically challenging problem.\",\n",
       " '16': 'An',\n",
       " '17': \"d it's actually somewhat miraculous that this works at all, in my opinion. But actually, not only does it work, but these things work very close to human accuracy in some limited situations. And take maybe only hundreds of milliseconds to do so. So this is some pretty amazing, incredible technology, in my opinion.\",\n",
       " '18': \"And over the course of the rest of the class, we'll really see what kinds of advancements have made this possible. So now, if you kind of think about what is the API for writing an image classifier, you might sit down and try to write a method in Python like this, where you want to take in an image and then do some crazy magic, and then eventually spit out this class label to say cat or dog or whatnot. And there's really no obvious way to do this, right? Like, if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or even do something like RSA encryption, you sort of can write down an algorithm and enumerate all the steps that need to happen in order for these things to work. But when we're trying to recognize objects or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense for how you might go about recognizing these objects. So this is, again, quite challenging. If you think about, like, if it was your first day programming and you had to sit down and write this function, I think most people would be in trouble. That being said, people have definitely made explicit attempts to try to write sort of hand-coded rules for recognizing different animals. So we touched on this a little bit in the last lecture. But\",\n",
       " '19': \"maybe one idea for cats is that, you know, we know that cats have ears and eyes and mouths and noses, and we know that edges are, from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries and say that, you know, if we have maybe three lines meeting this way, then it might be a corner and an ear has one corner here and one corner there and one corner there, and then kind of write down this explicit set of rules for recognizing cats. But this turns out not to work very well. One, it's super brittle, and two, say, if you want to start over for another object category and maybe not worry about cats but talk about trucks or dogs or fishes or something else, then you need to start all over again. So this is really not a very scalable approach. We want to come up with some algorithm or some method for these recognition tasks, which scales much more naturally to all the variety of objects in the world.\",\n",
       " '20': \"So the insight that sort of makes this all work is this idea of the data-driven approach, is that rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this. And we can actually use tools like Google Image Search or something like that to go out and collect a very large number of examples of these different categories. By the way, this actually takes quite a lot of effort to go out and actually collect these datasets, but luckily, there's a lot of really good, high-quality datasets out there already for you to use.\",\n",
       " '21': \"Then once we get this dataset, we train this machine learning classifier that is gonna ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. Then finally, we'll use this trained model and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here,\",\n",
       " '22': \"our API has changed a little bit. Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One that's gonna, called train, that's gonna input images and labels and then output a model. And then separately, another function called predict, which will input the model and then make predictions for images. And this is kind of the key insight that allowed all these things to start working really well in the last, over the last 10, 20 years or so. So the first, so this class is primarily about neural networks and convolutional neural networks and deep learning and all that. But there's, this idea of a data-driven approach is much more general than just deep learning. And I think it's useful to sort of step through this process for a very simple classifier first before we get to these big, complex ones.\",\n",
       " '23': \"So probably the simplest classifier you can imagine is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So during the training step, we won't do anything. We'll just memorize all of the training data. So this is very simple. And now during the prediction step, we're gonna take some new image and go and try to find the most similar image in the training data to that new image and now predict the label of that most similar image.\",\n",
       " '24': \"Very simple algorithm. But it sort of has a lot of these nice properties with respect to data-drivenness and whatnot. So to be a little bit more concrete, you might imagine working on this dataset called CIFAR-10, which is very commonly used in machine learning as kind of a small test case. And you'll be working with this dataset on your homework. So the CIFAR-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. And for each of those 10 categories, it provides 10,000, sorry, it provides 50,000 training images, roughly evenly distributed across these 10 categories, and then 10,000 additional testing images that you're supposed to test your algorithm on.\",\n",
       " '25': \"So now if you think about, so here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR-10. So on this grid on the right, for the leftmost column, gives a test image in the CIFAR-10 dataset. And now on the right, we see we've sorted the training images and show the most similar training images to each of these test examples. And you can see that they look kind of visually similar to the training images, although they are not always correct. So maybe on this second row, we see that the testing, this is kind of hard to see because these images are 32 by 32 pixels. You need to really dive in there and try to make your best guess. But this image is a dog, and its nearest neighbor is also a dog. But this next one, I think is actually a deer or a horse or something else. But you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. So if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set. And now the closest example, we know it's label because it comes from the training set, and now we'll simply say that this testing image is also a dog. You can see kind of from these examples that this is probably not gonna work very well, but it's still kind of a nice example to work through. But then one detail that we need to know is given a pair of images, how can we actually compare them? Because if we're gonna take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like.\",\n",
       " '26': \"So in the example in the previous slide, we've used what's called the L1 distance, also sometimes called the Manhattan distance. So this is a really sort of simple, easy idea for comparing images. And that's that we're gonna take the, just compare individual pixels in these images. So supposing that our test image is maybe just a tiny four by four image of pixel values, then we're gonna take this upper left-hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. And then sum all these up across all the pixels in the image. So this is kind of a stupid way to compare images, but it does some reasonable things sometimes. But this gives us a very concrete way to measure the difference between two images.\",\n",
       " '27': \"And in this case, we have this difference of 456 between these two images. So here's some full Python code for implementing this nearest neighbor classifier. And you can see it's actually pretty short and pretty concise, because we've made use of many of these vectorized operations offered by NumPy. So here we can see that the training, this train function that we talked about earlier is, again, very simple in the case of nearest neighbor. You just memorize the training data. There's not really much to do here. And now at test time, we're gonna take in our image and then go in and compare, using this\",\n",
       " '28': \"L1 distance function, our test image to each of these training examples. And find the most similar example in the training set. And you can see that we're actually able to do this in just one or two lines of Python code, using, by utilizing these vectorized operations in NumPy. So this is something that you'll get practice with on the first assignment. So now a couple of questions about this simple classifier. First, if we have n examples in our training set, then how fast can we expect training and testing to be? Well, training is probably constant because we don't really need to do anything. We just need to memorize the data. And if you're just copying a pointer, that's gonna be constant time, no matter how big your data set is. But now at test time, we need to do this comparison step and compare our test image to each of the n training examples in the data set. And this is actually quite slow. So this is actually somewhat backwards, if you think about it. Because, you know, in practice, we want our classifiers to be slow at training time and then fast at testing time. Because you might imagine that a classifier might go and be trained in a data center somewhere, and you can afford to spend a lot of computation at training time to make the classifier really good. But then when you go and deploy the classifier at test time, you want it to run on your mobile phone or in the browser or some other low-power device, and you really want the testing time performance of your classifier to be quite fast. So from this perspective, this nearest neighbor algorithm is actually a little bit backwards. And we'll see that once we move to convolutional neural networks and other types of parametric models, they'll be the reverse of this, where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. So then the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? So here we've drawn what we call the decision regions of a nearest neighbor classifier. So here our training set consists of these points in the two-dimensional plane, where the color of the point represents the category or the class label of that point. So here we see we have five classes, and some blue ones up in the corner here, some purple ones in the upper right-hand corner. And now for each pixel in this entire plane, we've gone and computed what is the nearest training, what is the nearest example in the training set. What is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. So you can see that\",\n",
       " '29': \"this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points. But this classifier is maybe not so great, and by looking at this picture, we can start to see some of the problems that might come out with a nearest neighbor classifier. For one, this central region actually contains mostly green points, but one little yellow point in the middle. But because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of the green cluster, and that's maybe not so great. Maybe those points actually should have been green. And then similarly, we also see these sort of fingers of the different, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. So this kind of motivates a slight generalization of this algorithm called k-nearest neighbors. So rather than just looking for the single nearest neighbor, instead, we'll do something a little bit fancier and find k of our nearest neighbors according to our distance metric, and then take a vote among each of our neighbors, and then predict the majority vote among our neighbors. You can imagine slightly more complex ways of doing this. Maybe you vote weighted on the distance or something like that, but the simplest thing that tends to work pretty well is taking a majority vote. So here we've shown the exact same set of points using this k equals one nearest neighbor classifier, as well as k equals three and k equals five in the middle and on the right. And once we move to k equals three, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. Now this entire green portion in the middle is all being classified as green. And you'll also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then once we move to the k equals five case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So this is generally something, so generally when you're using nearest neighbor classifiers, you almost always want to use some value of k, which is larger than one, because this tends to smooth out your decision boundaries and lead to better results. So if we, to kind of, oh yeah, question? Yeah, so the question is, what is the deal with these white regions? And the white regions are where there was no majority among the k nearest neighbors. You could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners. But for\",\n",
       " '30': \"this simple example, we're just coloring it white to indicate that there was no nearest neighbor in those points. So we already kind of saw, so I like to, whenever we're thinking about computer vision, I think it's really useful to kind of flip back and forth between several different viewpoints. One is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images, because the pixels of the image actually allow us to think of these images as high dimensional vectors. And it's sort of useful to ping pong back and forth between these two different viewpoints. So then, when sort of taking this k nearest neighbor and going back to the images, you can see that it's actually not very good. Here I've colored in red and green, which images would actually be classified correctly or incorrectly according to their nearest neighbor. And you can see that it's really not very good. But maybe if we used a larger value of k, then this would involve actually voting among maybe the top three or the top five, or maybe even the whole row. And you can imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.\"}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay so welcome to lecture 2 of cs231n on tuesday we just recall we sort of gave you the big picture view of what is computer vision what is the history and a little bit of the overview of the class and today were really going to dive in for the first time into the details and well start to see in much more depth exactly how some of these learning algorithms actually work in practice so the first lecture of the class is probably the sort of the largest big picture vision and the majority of the lectures in this class will be much more detailoriented and much more focused on the specific mechanics of these different algorithms so today well see our first learning algorithm and thatll be really exciting i think but before we get to that i wanted to talk about a couple administrative issues one is piazza so i saw when i checked yesterday it seemed like we had maybe 500 students signed up on piazza which means that there are several hundred of you who are not yet there so we really want piazza to be the main source of communication between the students and the course staff so weve gotten a lot of questions to the staff list about project ideas or questions about midterm attendance or poster session attendance and any sort of questions like that should really go to piazza youll probably get answers to your questions faster on piazza because all the tas are knowing to check that and its sort of easy for emails to get lost in the shuffle if you just send to the course list its also come to my attention that some scpd students are having a bit of a hard time signing up for piazza scpd students are supposed to receive a stanford at stanfordedu email address so once you get that email address then you can use the stanford email to sign into piazza probably that doesnt affect those of you who are sitting in the room right now but for those students listening on scpd the next administrative issue is about assignment one assignment one will be up later today probably sometime this afternoon but i promise before i go to sleep tonight itll be up but if youre getting a little bit antsy and really want to start working on it right now then you can look at last years version of assignment one itll be pretty much the same content were just reshuffling it a little bit to make it like for example upgrading to work with python 3 rather than python 27 and some of these minor cosmetic changes but the content of the assignment will still be the same as last year so in this assignment youll be implementing your own knearest neighbor classifier which were going to talk about in this lecture youll also implement several different linear classifiers including the svm and softmax as well as a simple twolayer neural network and well cover all of this content over the next couple of lectures so all of our assignments are using python and numpy if you arent familiar with python or numpy then we have written a tutorial that you can find on the course website to try and get you up to speed but this is actually pretty important numpy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code so this is super important for pretty much all aspects of numerical computing and machine learning and everything like that is efficiently implementing these vectorized operations and youll get a lot of practice with this on the first assignment so for those of you who dont have a lot of experience with matlab or numpy or other types of vectorized tensor computation i recommend that you start looking at this assignment pretty early and also read carefully through the tutorial the other thing i wanted to talk about is that were happy to announce that we got were officially supported through google cloud for this class so google cloud is somewhat similar to amazon aws you can go and start virtual machines up in the cloud these virtual machines can have gpus so we have a well were working on the tutorial for exactly how to use google cloud and get it to work for the assignments but our intention is that youll be able to just download some image and itll be very seamless for you to work on the assignment on one of these instances on the cloud and because google has very generously supported this course well be able to distribute to each of you coupons that let you use google cloud credits for free for the class so you can feel free to use these for the assignments and also for the course projects when you want to start using gpus and larger machines and whatnot so well post more details about that probably on piazza later today but i just wanted to mention because i know there had been a couple of questions about can i use my laptop do i have to run on corn do i have to whatever and the answer is that youll be able to run on google cloud and well provide you some coupons for that yeah so those are kind of the major administrative issues i wanted to talk about today and then lets dive into the content so the last lecture we talked a little bit about this task of image classification which is really a core task in computer vision and this is something that well really focus on throughout the course of the class is exactly how do we work on this image classification task so a little bit more concretely when youre doing image classification you receive your system receives some input image which is this cute cat in this example and the system is aware of some predetermined set of categories or labels so these might be like a dog or a cat or a truck or a plane and theres some fixed set of category labels and the job of the computer is to look at the picture and assign it one of these fixed category labels this seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks but this is actually a really really hard problem for a machine so if you dig in and think about actually what does a computer see when it looks at this image it definitely doesnt get this holistic idea of a cat that you see when you look at it and the computer really is representing the image as this gigantic grid of numbers so you just so the image might be something like 800 by 600 pixels and each pixel is represented by three numbers giving the red green and blue values for that pixel so to the computer this is just a gigantic grid of numbers and its very difficult to distill the catness out of this giant array of thousands or whatever very many different numbers and this so we refer to this problem as the semantic gap that this idea of a cat or this label of a cat is a semantic label that were assigning to this image and theres this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing and this is a really hard problem because you can change the picture in very small subtle ways that will cause this pixel grid to change entirely so for example if we took this same cat and if the cat happened to sit still and not even twitch not move a muscle which is never gonna happen but we moved the camera to the other side then every single grid every single pixel in this giant grid of numbers would be completely different but somehow its still representing the same cat and our algorithms need to be robust to this but not only viewpoint is one problem another is illumination there can be different lighting conditions going on in the scene whether the cat is appearing in this very dark moody scene or in this very bright sunlit scene its still a cat and our algorithms need to be robust to that objects can also deform i think cats are maybe among the more deformable of animals that you might see out there and cats can really assume a lot of different varied poses and positions and our algorithms should be robust to these different kinds of transforms there can also be problems of occlusion where you might only see part of a cat like just the face or in this extreme example just a tail peeking out from under the couch cushion but in these cases its pretty easy for you as a person to realize that this is probably a cat and you still recognize these images as cats and this is something that our algorithms also must be robust to which is quite difficult i think there can also be problems of background clutter where maybe the foreground object the cat could actually look quite similar in appearance to the background and this is another thing that we need to handle theres also this problem of interclass variation that this one notion of catness actually spans a lot of different visual appearances and cats can come in different shapes and sizes and colors and ages and our algorithm again needs to work and handle all these different variations so this is actually a really really challenging problem and its sort of easy to forget how easy this is because so much of your brain is specifically tuned for dealing with these things but now if we want our computer programs to deal with all of these problems all simultaneously and not just for cats by the way but for just about any object category you could imagine this is a fantastically challenging problem and its actually somewhat miraculous that this works at all in my opinion but actually not only does it work but these things work very close to human accuracy in some limited situations and take maybe only hundreds of milliseconds to do so so this is some pretty amazing incredible technology in my opinion and over the course of the rest of the class well really see what kinds of advancements have made this possible so now if you kind of think about what is the api for writing an image classifier you might sit down and try to write a method in python like this where you want to take in an image and then do some crazy magic and then eventually spit out this class label to say cat or dog or whatnot and theres really no obvious way to do this right like if youre taking an algorithms class and your task is to sort numbers or compute a convex hull or even do something like rsa encryption you sort of can write down an algorithm and enumerate all the steps that need to happen in order for these things to work but when were trying to recognize objects or recognize cats or images theres no really clear explicit algorithm that makes intuitive sense for how you might go about recognizing these objects so this is again quite challenging if you think about like if it was your first day programming and you had to sit down and write this function i think most people would be in trouble that being said people have definitely made explicit attempts to try to write sort of handcoded rules for recognizing different animals so we touched on this a little bit in the last lecture but maybe one idea for cats is that you know we know that cats have ears and eyes and mouths and noses and we know that edges are from hubel and wiesel we know that edges are pretty important when it comes to visual recognition so one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries and say that you know if we have maybe three lines meeting this way then it might be a corner and an ear has one corner here and one corner there and one corner there and then kind of write down this explicit set of rules for recognizing cats but this turns out not to work very well one its super brittle and two say if you want to start over for another object category and maybe not worry about cats but talk about trucks or dogs or fishes or something else then you need to start all over again so this is really not a very scalable approach we want to come up with some algorithm or some method for these recognition tasks which scales much more naturally to all the variety of objects in the world so the insight that sort of makes this all work is this idea of the datadriven approach is that rather than sitting down and writing these handspecified rules to try to craft exactly what is a cat or a fish or what have you instead well go out onto the internet and collect a large dataset of many many cats and many many airplanes and many many deer and different things like this and we can actually use tools like google image search or something like that to go out and collect a very large number of examples of these different categories by the way this actually takes quite a lot of effort to go out and actually collect these datasets but luckily theres a lot of really good highquality datasets out there already for you to use then once we get this dataset we train this machine learning classifier that is gonna ingest all of the data summarize it in some way and then spit out a model that summarizes the knowledge of how to recognize these different object categories then finally well use this trained model and apply it on new images that will then be able to recognize cats and dogs and whatnot so here our api has changed a little bit rather than a single function that just inputs an image and recognizes a cat we have these two functions one thats gonna called train thats gonna input images and labels and then output a model and then separately another function called predict which will input the model and then make predictions for images and this is kind of the key insight that allowed all these things to start working really well in the last over the last 10 20 years or so so the first so this class is primarily about neural networks and convolutional neural networks and deep learning and all that but theres this idea of a datadriven approach is much more general than just deep learning and i think its useful to sort of step through this process for a very simple classifier first before we get to these big complex ones so probably the simplest classifier you can imagine is something we call nearest neighbor the algorithm is pretty dumb honestly so during the training step we wont do anything well just memorize all of the training data so this is very simple and now during the prediction step were gonna take some new image and go and try to find the most similar image in the training data to that new image and now predict the label of that most similar image very simple algorithm but it sort of has a lot of these nice properties with respect to datadrivenness and whatnot so to be a little bit more concrete you might imagine working on this dataset called cifar10 which is very commonly used in machine learning as kind of a small test case and youll be working with this dataset on your homework so the cifar10 dataset gives you 10 different classes airplanes and automobiles and birds and cats and different things like that and for each of those 10 categories it provides 10000 sorry it provides 50000 training images roughly evenly distributed across these 10 categories and then 10000 additional testing images that youre supposed to test your algorithm on so now if you think about so heres an example of applying this simple nearest neighbor classifier to some of these test images on cifar10 so on this grid on the right for the leftmost column gives a test image in the cifar10 dataset and now on the right we see weve sorted the training images and show the most similar training images to each of these test examples and you can see that they look kind of visually similar to the training images although they are not always correct so maybe on this second row we see that the testing this is kind of hard to see because these images are 32 by 32 pixels you need to really dive in there and try to make your best guess but this image is a dog and its nearest neighbor is also a dog but this next one i think is actually a deer or a horse or something else but you can see that it looks quite visually similar because theres kind of a white blob in the middle and whatnot so if were applying the nearest neighbor algorithm to this image well find the closest example in the training set and now the closest example we know its label because it comes from the training set and now well simply say that this testing image is also a dog you can see kind of from these examples that this is probably not gonna work very well but its still kind of a nice example to work through but then one detail that we need to know is given a pair of images how can we actually compare them because if were gonna take our test image and compare it to all the training images we actually have many different choices for exactly what that comparison function should look like so in the example in the previous slide weve used whats called the l1 distance also sometimes called the manhattan distance so this is a really sort of simple easy idea for comparing images and thats that were gonna take the just compare individual pixels in these images so supposing that our test image is maybe just a tiny four by four image of pixel values then were gonna take this upper lefthand pixel of the test image subtract off the value in the training image take the absolute value and get the difference in that pixel between the two images and then sum all these up across all the pixels in the image so this is kind of a stupid way to compare images but it does some reasonable things sometimes but this gives us a very concrete way to measure the difference between two images and in this case we have this difference of 456 between these two images so heres some full python code for implementing this nearest neighbor classifier and you can see its actually pretty short and pretty concise because weve made use of many of these vectorized operations offered by numpy so here we can see that the training this train function that we talked about earlier is again very simple in the case of nearest neighbor you just memorize the training data theres not really much to do here and now at test time were gonna take in our image and then go in and compare using this l1 distance function our test image to each of these training examples and find the most similar example in the training set and you can see that were actually able to do this in just one or two lines of python code using by utilizing these vectorized operations in numpy so this is something that youll get practice with on the first assignment so now a couple of questions about this simple classifier first if we have n examples in our training set then how fast can we expect training and testing to be well training is probably constant because we dont really need to do anything we just need to memorize the data and if youre just copying a pointer thats gonna be constant time no matter how big your data set is but now at test time we need to do this comparison step and compare our test image to each of the n training examples in the data set and this is actually quite slow so this is actually somewhat backwards if you think about it because you know in practice we want our classifiers to be slow at training time and then fast at testing time because you might imagine that a classifier might go and be trained in a data center somewhere and you can afford to spend a lot of computation at training time to make the classifier really good but then when you go and deploy the classifier at test time you want it to run on your mobile phone or in the browser or some other lowpower device and you really want the testing time performance of your classifier to be quite fast so from this perspective this nearest neighbor algorithm is actually a little bit backwards and well see that once we move to convolutional neural networks and other types of parametric models theyll be the reverse of this where youll spend a lot of compute at training time but then theyll be quite fast at testing time so then the question is what exactly does this nearest neighbor algorithm look like when you apply it in practice so here weve drawn what we call the decision regions of a nearest neighbor classifier so here our training set consists of these points in the twodimensional plane where the color of the point represents the category or the class label of that point so here we see we have five classes and some blue ones up in the corner here some purple ones in the upper righthand corner and now for each pixel in this entire plane weve gone and computed what is the nearest training what is the nearest example in the training set what is the nearest example in these training data and then colored the point of the background corresponding to what is the class label so you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points but this classifier is maybe not so great and by looking at this picture we can start to see some of the problems that might come out with a nearest neighbor classifier for one this central region actually contains mostly green points but one little yellow point in the middle but because were just looking at the nearest neighbor this causes a little yellow island to appear in this middle of the green cluster and thats maybe not so great maybe those points actually should have been green and then similarly we also see these sort of fingers of the different like the green region pushing into the blue region again due to the presence of one point which may have been noisy or spurious so this kind of motivates a slight generalization of this algorithm called knearest neighbors so rather than just looking for the single nearest neighbor instead well do something a little bit fancier and find k of our nearest neighbors according to our distance metric and then take a vote among each of our neighbors and then predict the majority vote among our neighbors you can imagine slightly more complex ways of doing this maybe you vote weighted on the distance or something like that but the simplest thing that tends to work pretty well is taking a majority vote so here weve shown the exact same set of points using this k equals one nearest neighbor classifier as well as k equals three and k equals five in the middle and on the right and once we move to k equals three you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow now this entire green portion in the middle is all being classified as green and youll also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting and then once we move to the k equals five case then these decision boundaries between the blue and red regions have become quite smooth and quite nice so this is generally something so generally when youre using nearest neighbor classifiers you almost always want to use some value of k which is larger than one because this tends to smooth out your decision boundaries and lead to better results so if we to kind of oh yeah question yeah so the question is what is the deal with these white regions and the white regions are where there was no majority among the k nearest neighbors you could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners but for this simple example were just coloring it white to indicate that there was no nearest neighbor in those points so we already kind of saw so i like to whenever were thinking about computer vision i think its really useful to kind of flip back and forth between several different viewpoints one is this idea of high dimensional points in the plane and then the other is actually looking at concrete images because the pixels of the image actually allow us to think of these images as high dimensional vectors and its sort of useful to ping pong back and forth between these two different viewpoints so then when sort of taking this k nearest neighbor and going back to the images you can see that its actually not very good here ive colored in red and green which images would actually be classified correctly or incorrectly according to their nearest neighbor and you can see that its really not very good but maybe if we used a larger value of k then this would involve actually voting among maybe the top three or the top five or maybe even the whole row and you can imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way\n",
      "okay so welcome to lecture 2 of cs231n\n",
      "one is piazza so i saw when i checked yesterday it seemed like we had maybe 500 students signed up on piazza which means that there are several hundred of you who are not yet there\n",
      "the next administrative issue is about assignment one\n",
      "so in this assignment youll be implementing your own knearest neighbor classifier which were going to talk about in this lecture\n",
      "but this is actually pretty important\n",
      "and youll get a lot of practice with this on the first assignment\n",
      "so google cloud is somewhat similar to amazon aws\n",
      "and because google has very generously supported this course well be able to distribute to each of you coupons that let you use google cloud credits for free for the class\n",
      "yeah so those are kind of the major administrative issues i wanted to talk about today\n",
      "so the last lecture we talked a little bit about this task of image classification which is really a core task in computer vision\n",
      "this seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks\n",
      "and this so we refer to this problem as the semantic gap\n",
      "so for example if we took this same cat and if the cat happened to sit still and not even twitch not move a muscle which is never gonna happen but we moved the camera to the other side then every single grid every single pixel in this giant grid of numbers would be completely different\n",
      "there can also be problems of occlusion where you might only see part of a cat like just the face or in this extreme example just a tail peeking out from under the couch cushion\n",
      "theres also this problem of interclass variation that this one notion of catness actually spans a lot of different visual appearances\n",
      "and its actually somewhat miraculous that this works at all in my opinion\n",
      "but now if we want our computer programs to deal with all of these problems all simultaneously and not just for cats by the way but for just about any object category you could imagine this is a fantastically challenging problem\n",
      "and over the course of the rest of the class well really see what kinds of advancements have made this possible\n",
      "so maybe one idea for cats is that you know we know that cats have ears and eyes and mouths and noses and we know that edges are from hubel and wiesel we know that edges are pretty important when it comes to visual recognition\n",
      "so the insight that sort of makes this all work is this idea of the datadriven approach is that rather than sitting down and writing these handspecified rules to try to craft exactly what is a cat or a fish or what have you instead well go out onto the internet and collect a large dataset of many many cats and many many airplanes and many many deer and different things like this\n",
      "then once we get this dataset we train this machine learning classifier that is gonna ingest all of the data summarize it in some way and then spit out a model that summarizes the knowledge of how to recognize these different object categories\n",
      "so here our api has changed a little bit\n",
      "so probably the simplest classifier you can imagine is something we call nearest neighbor\n",
      "very simple algorithm\n",
      "so now if you think about so heres an example of applying this simple nearest neighbor classifier to some of these test images on cifar10\n",
      "so in the example in the previous slide weve used whats called the l1 distance also sometimes called the manhattan distance\n",
      "and in this case we have this difference of 456 between these two images\n",
      "because in the example in the previous slide weve used the l1 distance function to predict the class of these images\n",
      "so heres some full python code for implementing this nearest neighbor classifier\n",
      "so now a couple of questions about this simple classifier\n",
      "1 0 799\n",
      "2 799 1888\n",
      "3 1888 2482\n",
      "4 2482 3010\n",
      "5 3010 3366\n",
      "6 3366 3811\n",
      "7 3811 4241\n",
      "8 4241 4891\n",
      "9 4891 5014\n",
      "10 5014 5728\n",
      "11 5728 6561\n",
      "12 6561 6985\n",
      "13 6985 7909\n",
      "14 7909 8530\n",
      "15 8530 9227\n",
      "16 9227 9229\n",
      "17 9229 9535\n",
      "18 9535 10816\n",
      "19 10816 11889\n",
      "20 11889 12626\n",
      "21 12626 13000\n",
      "22 13000 13843\n",
      "23 13843 14290\n",
      "24 14290 14996\n",
      "25 14996 16601\n",
      "26 16601 17395\n",
      "27 17395 17984\n",
      "28 17984 20580\n",
      "29 20580 23261\n",
      "30 23261 25062\n",
      "Matched paragraphs saved to ../spms/11_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "# Function to remove punctuation and lowercase the text\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Function to remove the first word from a sentence\n",
    "def remove_first_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return ' '.join(words[1:])\n",
    "\n",
    "# Function to find the best match using difflib\n",
    "def find_best_match(script_content, sentence, min_index=0):\n",
    "    matcher = SequenceMatcher(None, script_content, sentence)\n",
    "    match = matcher.find_longest_match(min_index, len(script_content), 0, len(sentence))\n",
    "    if match.size > 0:\n",
    "        return match.a  # Return the start index of the match\n",
    "    return -1  # No match found\n",
    "\n",
    "# Function to find start indices with a fallback strategy\n",
    "def find_start_indices(script_content, first_sentences):\n",
    "    start_indices = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "    last_index = 0  # Keep track of the last found index to ensure we find subsequent matches after this\n",
    "\n",
    "    script_content_processed = preprocess_text(script_content)\n",
    "\n",
    "    print(script_content_processed)\n",
    "\n",
    "    for page in page_numbers:\n",
    "        current_sentence = first_sentences[page]\n",
    "        current_sentence_processed = preprocess_text(current_sentence)\n",
    "\n",
    "        print(current_sentence_processed)\n",
    "        \n",
    "        # Attempt 1: Direct search after last_index\n",
    "        start_index = script_content_processed.find(current_sentence_processed, last_index)\n",
    "        \n",
    "        # Attempt 2: Search after removing the first word\n",
    "        if start_index == -1:\n",
    "            modified_sentence = remove_first_word(current_sentence_processed)\n",
    "            start_index = script_content_processed.find(modified_sentence, last_index)\n",
    "        \n",
    "        # Attempt 3: Fallback to best match if still not found\n",
    "        if start_index == -1:\n",
    "            start_index = find_best_match(script_content_processed, current_sentence_processed, min_index=last_index)\n",
    "        \n",
    "        # # Ensure start_index is greater than last_index\n",
    "        # if start_index <= last_index:\n",
    "        #     start_index = last_index + 1\n",
    "        \n",
    "        start_indices[page] = start_index\n",
    "        last_index = start_index  # Update last_index to the current start_index\n",
    "    \n",
    "    return start_indices\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    # Step 1: Find all start indices\n",
    "    start_indices = find_start_indices(script_content, first_sentences)\n",
    "    \n",
    "    # Step 2: Sort the pages by start index\n",
    "    sorted_pages = sorted(start_indices, key=lambda page: start_indices[page])\n",
    "    \n",
    "    # Step 3: Create the matched paragraphs\n",
    "    matched_paragraphs = {}\n",
    "    for i, page in enumerate(sorted_pages):\n",
    "        start_index = start_indices[page]\n",
    "        if i < len(sorted_pages) - 1:\n",
    "            next_page = sorted_pages[i + 1]\n",
    "            end_index = start_indices[next_page]\n",
    "        else:\n",
    "            end_index = len(script_content)  # Last page\n",
    "        \n",
    "        matched_paragraphs[str(i+1)] = script_content[start_index:end_index].strip()\n",
    "\n",
    "        print(str(i+1), start_index, end_index)\n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "project_id = 11\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file with sorted keys\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1, '2': 824, '3': 1940, '4': 2553, '5': 3098, '6': 3458, '7': 3912, '8': 4352, '9': 5016, '10': 5146, '11': 5872, '12': 6724, '13': 7158, '14': 8107, '15': 8744, '16': 9460, '17': 9700, '18': 20638, '19': 21163, '20': 21704, '21': 22258, '22': 24580, '23': 24580, '24': 24580, '25': 24580, '26': 24580, '27': 24580, '28': 24580, '29': 24580, '30': 24580}\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30']\n",
      "1 1 824\n",
      "2 824 1940\n",
      "3 1940 2553\n",
      "4 2553 3098\n",
      "5 3098 3458\n",
      "6 3458 3912\n",
      "7 3912 4352\n",
      "8 4352 5016\n",
      "9 5016 5146\n",
      "10 5146 5872\n",
      "11 5872 6724\n",
      "12 6724 7158\n",
      "13 7158 8107\n",
      "14 8107 8744\n",
      "15 8744 9460\n",
      "16 9460 9700\n",
      "17 9700 20638\n",
      "18 20638 21163\n",
      "19 21163 21704\n",
      "20 21704 22258\n",
      "21 22258 24580\n",
      "22 24580 24580\n",
      "23 24580 24580\n",
      "24 24580 24580\n",
      "25 24580 24580\n",
      "26 24580 24580\n",
      "27 24580 24580\n",
      "28 24580 24580\n",
      "29 24580 24580\n",
      "30 24580 25062\n",
      "Matched paragraphs saved to ../spms/11_matched_paragraphs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "SCRIPT = '../scripts'\n",
    "SPM = '../spms'\n",
    "\n",
    "# Load the first sentences JSON file\n",
    "def load_first_sentences(json_path):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "# Read script file\n",
    "def read_script(script_path):\n",
    "    with open(script_path, \"r\") as script_file:\n",
    "        script_content = script_file.read()\n",
    "        return json.loads(script_content)\n",
    "\n",
    "# Function to remove punctuation and lowercase the text\n",
    "def preprocess_text_and_map(text):\n",
    "    original_to_processed_map = []\n",
    "    processed_text = []\n",
    "    for i, char in enumerate(text):\n",
    "        if char.isalnum() or char.isspace():\n",
    "            processed_text.append(char.lower())\n",
    "            original_to_processed_map.append(i)\n",
    "    return ''.join(processed_text), original_to_processed_map\n",
    "\n",
    "# Function to remove the first word from a sentence\n",
    "def remove_first_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return ' '.join(words[1:])\n",
    "\n",
    "# Function to find the best match using difflib\n",
    "def find_best_match(script_content, sentence, min_index=0):\n",
    "    matcher = SequenceMatcher(None, script_content, sentence)\n",
    "    match = matcher.find_longest_match(min_index, len(script_content), 0, len(sentence))\n",
    "    if match.size > 0:\n",
    "        return match.a  # Return the start index of the match\n",
    "    return -1  # No match found\n",
    "\n",
    "# Function to find start indices with a fallback strategy\n",
    "def find_start_indices(script_content, first_sentences):\n",
    "    start_indices = {}\n",
    "    page_numbers = sorted(first_sentences.keys(), key=int)\n",
    "    last_index = 0  # Keep track of the last found index to ensure we find subsequent matches after this\n",
    "\n",
    "    # Preprocess script content and create a mapping to the original text\n",
    "    script_content_processed, original_to_processed_map = preprocess_text_and_map(script_content)\n",
    "\n",
    "    for page in page_numbers:\n",
    "        current_sentence = first_sentences[page]\n",
    "        current_sentence_processed, _ = preprocess_text_and_map(current_sentence)\n",
    "        \n",
    "        # Attempt 1: Direct search after last_index\n",
    "        start_index_processed = script_content_processed.find(current_sentence_processed, last_index)\n",
    "        \n",
    "        # Attempt 2: Search after removing the first word\n",
    "        if start_index_processed == -1:\n",
    "            modified_sentence = remove_first_word(current_sentence_processed)\n",
    "            start_index_processed = script_content_processed.find(modified_sentence, last_index)\n",
    "        \n",
    "        # Attempt 3: Fallback to best match if still not found\n",
    "        if start_index_processed == -1:\n",
    "            start_index_processed = find_best_match(script_content_processed, current_sentence_processed, min_index=last_index)\n",
    "        \n",
    "        # Ensure start_index is greater than last_index\n",
    "        if start_index_processed <= last_index:\n",
    "            start_index_processed = last_index + 1\n",
    "        \n",
    "        # Convert the processed text index back to the original text index\n",
    "        if start_index_processed < len(original_to_processed_map):\n",
    "            start_index = original_to_processed_map[start_index_processed]\n",
    "        else:\n",
    "            start_index = last_index \n",
    "\n",
    "        start_indices[page] = start_index\n",
    "        last_index = start_index  # Update last_index to the current start_index\n",
    "    \n",
    "    return start_indices\n",
    "\n",
    "# Match the paragraphs to the first sentences\n",
    "def match_paragraphs(script_content, first_sentences):\n",
    "    # Step 1: Find all start indices\n",
    "    start_indices = find_start_indices(script_content, first_sentences)\n",
    "    print(start_indices)\n",
    "    \n",
    "    # Step 2: Sort the pages by start index\n",
    "    sorted_pages = sorted(start_indices, key=lambda page: start_indices[page])\n",
    "    print(sorted_pages)\n",
    "    \n",
    "    # Step 3: Create the matched paragraphs\n",
    "    matched_paragraphs = {}\n",
    "    for i, page in enumerate(sorted_pages):\n",
    "        start_index = start_indices[page]\n",
    "        if i < len(sorted_pages) - 1:\n",
    "            next_page = sorted_pages[i + 1]\n",
    "            end_index = start_indices[next_page]\n",
    "        else:\n",
    "            end_index = len(script_content)  # Last page\n",
    "        \n",
    "        matched_paragraphs[str(i+1)] = script_content[start_index:end_index].strip()\n",
    "\n",
    "        print(str(i+1), start_index, end_index)\n",
    "\n",
    "    return matched_paragraphs\n",
    "\n",
    "project_id = 11\n",
    "# Read the script content\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "script_content = read_script(script_path)\n",
    "\n",
    "# Load the first sentences\n",
    "first_sentences_path = os.path.join(SPM, f\"{project_id}_spm.json\")\n",
    "first_sentences = load_first_sentences(first_sentences_path)\n",
    "\n",
    "# Match the paragraphs\n",
    "matched_paragraphs = match_paragraphs(script_content, first_sentences)\n",
    "\n",
    "# Save the matched paragraphs to a JSON file with sorted keys\n",
    "matched_paragraphs_json_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_json_path, \"w\") as json_file:\n",
    "    json.dump(matched_paragraphs, json_file, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Matched paragraphs saved to {matched_paragraphs_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"start\": 0.0,\n",
      "        \"end\": 40.0,\n",
      "        \"script\": \"Okay, so welcome to Lecture 2 of CS231N. On Tuesday, we, just recall, we sort of gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. And today, we're really going to dive in for the first time into the details, and we'll start to see in much more depth exactly how some of these learning algorithms actually work in practice. So the first lecture of the class is probably the sort of the largest big picture vision, and the majority of the lectures in this class will be much more detail-oriented, and much more focused on the specific mechanics of these different algorithms. So today, we'll see our first learning algorithm, and that'll be really exciting, I think. But before we get to that, I wanted to talk about a couple administrative issues.\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"start\": 40.47999954223633,\n",
      "        \"end\": 95.62000274658203,\n",
      "        \"script\": \"One is Piazza, so I saw, when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza, which means that there are several hundred of you who are not yet there. So we really want Piazza to be the main source of communication between the students and the course staff. So we've gotten a lot of questions to the staff list about project ideas, or questions about midterm attendance, or poster session attendance, and any sort of questions like that should really go to Piazza. You'll probably get answers to your questions faster on Piazza, because all the TAs are knowing to check that. And it's sort of easy for emails to get lost in the shuffle if you just send to the course list. It's also come to my attention that some SCPD students are having a bit of a hard time signing up for Piazza. SCPD students are supposed to receive a .stanford, at stanford.edu email address. So once you get that email address, then you can use the Stanford email to sign into Piazza.\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"start\": 95.62000274658203,\n",
      "        \"end\": 124.55999755859375,\n",
      "        \"script\": \"Probably that doesn't affect those of you who are sitting in the room right now, but for those students listening on SCPD. The next administrative issue is about assignment one. Assignment one will be up later today, probably sometime this afternoon, but I promise before I go to sleep tonight, it'll be up. But if you're getting a little bit antsy and really want to start working on it right now, then you can look at last year's version of assignment one. It'll be pretty much the same content.\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"start\": 124.55999755859375,\n",
      "        \"end\": 167.94000244140625,\n",
      "        \"script\": \"We're just reshuffling it a little bit to make it, like for example, upgrading to work with Python 3 rather than Python 2.7, and some of these minor cosmetic changes. But the content of the assignment will still be the same as last year. So in this assignment, you'll be implementing your own k-nearest neighbor classifier, which we're going to talk about in this lecture. You'll also implement several different linear classifiers, including the SVM and Softmax, as well as a simple two-layer neural network. And we'll cover all of this content over the next couple of lectures. So all of our assignments are using Python and NumPy. If you aren't familiar with Python or NumPy, then we have written a tutorial that you can find on the course website to try and get you up to speed. But this is actually pretty important.\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"start\": 167.94000244140625,\n",
      "        \"end\": 238.55999755859375,\n",
      "        \"script\": \"NumPy lets you write these very efficient vectorized operations that let you do quite a lot of computation in just a couple lines of code. So this is super important for pretty much all aspects of numerical computing and machine learning and everything like that, is efficiently implementing these vectorized operations. And you'll get a lot of practice with this on the first assignment. So for those of you who don't have a lot of experience with MATLAB or NumPy or other types of vectorized tensor computation, I recommend that you start looking at this assignment pretty early and also read carefully through the tutorial. The other thing I wanted to talk about is that we're happy to announce that we got, we're officially supported through Google Cloud for this class. So Google Cloud is somewhat similar to Amazon AWS. You can go and start virtual machines up in the cloud. These virtual machines can have GPUs. So we have a, well, we're working on the tutorial for exactly how to use Google Cloud and get it to work for the assignments. But our intention is that you'll be able to just download some image and it'll be very seamless for you to work on the assignment on one of these instances on the cloud. And\"\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"start\": 238.55999755859375,\n",
      "        \"end\": 286.9599914550781,\n",
      "        \"script\": \"because Google has very generously supported this course, we'll be able to distribute to each of you coupons that let you use Google Cloud credits for free for the class. So you can feel free to use these for the assignments and also for the course projects when you want to start using GPUs and larger machines and whatnot. So we'll post more details about that probably on Piazza later today. But I just wanted to mention, because I know there had been a couple of questions about can I use my laptop? Do I have to run on Corn? Do I have to, whatever. And the answer is that you'll be able to run on Google Cloud and we'll provide you some coupons for that. Yeah, so those are kind of the major administrative issues I wanted to talk about today. And then let's dive into the content.\"\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"start\": 286.9599914550781,\n",
      "        \"end\": 337.8399963378906,\n",
      "        \"script\": \"So the last lecture we talked a little bit about this task of image classification, which is really a core task in computer vision. And this is something that we'll really focus on throughout the course of the class is exactly how do we work on this image classification task. So a little bit more concretely, when you're doing image classification, you receive, your system receives some input image, which is this cute cat in this example. And the system is aware of some predetermined set of categories or labels. So these might be like a dog or a cat or a truck or a plane. And there's some fixed set of category labels. And the job of the computer is to look at the picture and assign it one of these fixed category labels. This seems like a really easy problem because so much of your own visual system and your brain is hardwired for doing these sort of visual recognition tasks.\"\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"start\": 337.8399963378906,\n",
      "        \"end\": 355.1400146484375,\n",
      "        \"script\": \"But this is actually a really, really hard problem for a machine. So if you dig in and think about actually what does a computer see when it looks at this image, it definitely doesn't get this holistic idea of a cat that you see when you look at it. And the computer really is representing the image as this gigantic grid of numbers.\"\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"start\": 355.1400146484375,\n",
      "        \"end\": 419.6600036621094,\n",
      "        \"script\": \"So you just, so the image might be something like 800 by 600 pixels. And each pixel is represented by three numbers, giving the red, green, and blue values for that pixel. So to the computer, this is just a gigantic grid of numbers. And it's very difficult to distill the cat-ness out of this giant array of thousands or whatever, very many different numbers. And this, so we refer to this problem as the semantic gap. That this idea of a cat, or this label of a cat, is a semantic label that we're assigning to this image. And there's this huge gap between the semantic idea of a cat and these pixel values that the computer is actually seeing. And this is a really hard problem because you can change the picture in very small, subtle ways that will cause this pixel grid to change entirely. So for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never gonna happen, but we moved the camera to the other side, then every single grid, every single pixel in this giant grid of numbers would be completely different.\"\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"start\": 419.9800109863281,\n",
      "        \"end\": 448.20001220703125,\n",
      "        \"script\": \"But somehow, it's still representing the same cat. And our algorithms need to be robust to this. But not only viewpoint is one problem, another is illumination. There can be different lighting conditions going on in the scene. Whether the cat is appearing in this very dark, moody scene or in this very bright, sunlit scene, it's still a cat. And our algorithms need to be robust to that. Objects can also deform. I think cats are maybe among the more deformable of animals that you might see out there. And\"\n",
      "    },\n",
      "    \"11\": {\n",
      "        \"start\": 448.20001220703125,\n",
      "        \"end\": 474.1600036621094,\n",
      "        \"script\": \"cats can really assume a lot of different varied poses and positions. And our algorithms should be robust to these different kinds of transforms. There can also be problems of occlusion, where you might only see part of a cat, like just the face, or in this extreme example, just a tail peeking out from under the couch cushion. But in these cases, it's pretty easy for you as a person to realize that this is probably a cat and you still recognize these images as cats.\"\n",
      "    },\n",
      "    \"12\": {\n",
      "        \"start\": 474.6199951171875,\n",
      "        \"end\": 489.44000244140625,\n",
      "        \"script\": \"And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems of background clutter, where maybe the foreground object, the cat, could actually look quite similar in appearance to the background.\"\n",
      "    },\n",
      "    \"13\": {\n",
      "        \"start\": 489.7799987792969,\n",
      "        \"end\": 504.70001220703125,\n",
      "        \"script\": \"And this is another thing that we need to handle. There's also this problem of inter-class variation, that this one notion of catness actually spans a lot of different visual appearances. And cats can come in different shapes and sizes and colors and ages.\"\n",
      "    },\n",
      "    \"14\": {\n",
      "        \"start\": 504.70001220703125,\n",
      "        \"end\": 544.8800048828125,\n",
      "        \"script\": \"And our algorithm, again, needs to work and handle all these different variations. So this is actually a really, really challenging problem. And it's sort of easy to forget how easy this is, because so much of your brain is specifically tuned for dealing with these things. But now, if we want our computer programs to deal with all of these problems all simultaneously, and not just for cats, by the way, but for just about any object category you could imagine, this is a fantastically challenging problem. And it's actually somewhat miraculous that this works at all, in my opinion. But actually, not only does it work, but these things work very close to human accuracy in some limited situations. And take maybe only hundreds of milliseconds to do so.\"\n",
      "    },\n",
      "    \"15\": {\n",
      "        \"start\": 544.8800048828125,\n",
      "        \"end\": 559.739990234375,\n",
      "        \"script\": \"So this is some pretty amazing, incredible technology, in my opinion. And over the course of the rest of the class, we'll really see what kinds of advancements have made this possible. So now, if you kind of think about what is the API for writing an image classifier,\"\n",
      "    },\n",
      "    \"16\": {\n",
      "        \"start\": 559.739990234375,\n",
      "        \"end\": 651.3400268554688,\n",
      "        \"script\": \"you might sit down and try to write a method in Python like this, where you want to take in an image and then do some crazy magic, and then eventually spit out this class label to say cat or dog or whatnot. And there's really no obvious way to do this, right? Like, if you're taking an algorithms class and your task is to sort numbers or compute a convex hull or even do something like RSA encryption, you sort of can write down an algorithm and enumerate all the steps that need to happen in order for these things to work. But when we're trying to recognize objects or recognize cats or images, there's no really clear, explicit algorithm that makes intuitive sense for how you might go about recognizing these objects. So this is, again, quite challenging. If you think about, like, if it was your first day programming and you had to sit down and write this function, I think most people would be in trouble. That being said, people have definitely made explicit attempts to try to write sort of hand-coded rules for recognizing different animals. So we touched on this a little bit in the last lecture. But maybe one idea for cats is that, you know, we know that cats have ears and eyes and mouths and noses, and we know that edges are, from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to categorize all the different corners and boundaries and say that, you know, if we have maybe three lines meeting this way, then it might be a corner and an ear has one corner here and one corner there and one corner there, and then kind of write down this explicit set of rules for recognizing cats.\"\n",
      "    },\n",
      "    \"17\": {\n",
      "        \"start\": 651.3400268554688,\n",
      "        \"end\": 705.7000122070312,\n",
      "        \"script\": \"But this turns out not to work very well. One, it's super brittle, and two, say, if you want to start over for another object category and maybe not worry about cats but talk about trucks or dogs or fishes or something else, then you need to start all over again. So this is really not a very scalable approach. We want to come up with some algorithm or some method for these recognition tasks, which scales much more naturally to all the variety of objects in the world. So the insight that sort of makes this all work is this idea of the data-driven approach, is that rather than sitting down and writing these hand-specified rules to try to craft exactly what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large dataset of many, many cats and many, many airplanes and many, many deer and different things like this.\"\n",
      "    },\n",
      "    \"18\": {\n",
      "        \"start\": 705.7000122070312,\n",
      "        \"end\": 759.219970703125,\n",
      "        \"script\": \"And we can actually use tools like Google Image Search or something like that to go out and collect a very large number of examples of these different categories. By the way, this actually takes quite a lot of effort to go out and actually collect these datasets, but luckily, there's a lot of really good, high-quality datasets out there already for you to use. Then once we get this dataset, we train this machine learning classifier that is gonna ingest all of the data, summarize it in some way, and then spit out a model that summarizes the knowledge of how to recognize these different object categories. Then finally, we'll use this trained model and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here, our API has changed a little bit. Rather than a single function that just inputs an image and recognizes a cat, we have these two functions. One that's gonna, called train, that's gonna input images and labels and then output a model.\"\n",
      "    },\n",
      "    \"19\": {\n",
      "        \"start\": 759.219970703125,\n",
      "        \"end\": 788.5,\n",
      "        \"script\": \"And then separately, another function called predict, which will input the model and then make predictions for images. And this is kind of the key insight that allowed all these things to start working really well in the last, over the last 10, 20 years or so. So the first, so this class is primarily about neural networks and convolutional neural networks and deep learning and all that. But there's, this idea of a data-driven approach is much more general than just deep learning.\"\n",
      "    },\n",
      "    \"20\": {\n",
      "        \"start\": 788.8400268554688,\n",
      "        \"end\": 825.0399780273438,\n",
      "        \"script\": \"And I think it's useful to sort of step through this process for a very simple classifier first before we get to these big, complex ones. So probably the simplest classifier you can imagine is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So during the training step, we won't do anything. We'll just memorize all of the training data. So this is very simple. And now during the prediction step, we're gonna take some new image and go and try to find the most similar image in the training data to that new image and now predict the label of that most similar image.\"\n",
      "    },\n",
      "    \"21\": {\n",
      "        \"start\": 825.0399780273438,\n",
      "        \"end\": 934.7000122070312,\n",
      "        \"script\": \"Very simple algorithm. But it sort of has a lot of these nice properties with respect to data-drivenness and whatnot. So to be a little bit more concrete, you might imagine working on this dataset called CIFAR-10, which is very commonly used in machine learning as kind of a small test case. And you'll be working with this dataset on your homework. So the CIFAR-10 dataset gives you 10 different classes, airplanes and automobiles and birds and cats and different things like that. And for each of those 10 categories, it provides 10,000, sorry, it provides 50,000 training images, roughly evenly distributed across these 10 categories, and then 10,000 additional testing images that you're supposed to test your algorithm on. So now if you think about, so here's an example of applying this simple nearest neighbor classifier to some of these test images on CIFAR-10. So on this grid on the right, for the leftmost column, gives a test image in the CIFAR-10 dataset. And now on the right, we see we've sorted the training images and show the most similar training images to each of these test examples. And you can see that they look kind of visually similar to the training images, although they are not always correct. So maybe on this second row, we see that the testing, this is kind of hard to see because these images are 32 by 32 pixels. You need to really dive in there and try to make your best guess. But this image is a dog, and its nearest neighbor is also a dog. But this next one, I think is actually a deer or a horse or something else. But you can see that it looks quite visually similar, because there's kind of a white blob in the middle and whatnot. So if we're applying the nearest neighbor algorithm to this image, we'll find the closest example in the training set.\"\n",
      "    },\n",
      "    \"22\": {\n",
      "        \"start\": 934.7000122070312,\n",
      "        \"end\": 1012.3599853515625,\n",
      "        \"script\": \"And now the closest example, we know it's label because it comes from the training set, and now we'll simply say that this testing image is also a dog. You can see kind of from these examples that this is probably not gonna work very well, but it's still kind of a nice example to work through. But then one detail that we need to know is given a pair of images, how can we actually compare them? Because if we're gonna take our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison function should look like. So in the example in the previous slide, we've used what's called the L1 distance, also sometimes called the Manhattan distance. So this is a really sort of simple, easy idea for comparing images. And that's that we're gonna take the, just compare individual pixels in these images. So supposing that our test image is maybe just a tiny four by four image of pixel values, then we're gonna take this upper left-hand pixel of the test image, subtract off the value in the training image, take the absolute value, and get the difference in that pixel between the two images. And then sum all these up across all the pixels in the image. So this is kind of a stupid way to compare images, but it does some reasonable things sometimes. But this gives us a very concrete way to measure the difference between two images.\"\n",
      "    },\n",
      "    \"23\": {\n",
      "        \"start\": 1012.3599853515625,\n",
      "        \"end\": 1023.0999755859375,\n",
      "        \"script\": \"And in this case, we have this difference of 456 between these two images. So here's some full Python code for implementing this nearest neighbor classifier.\"\n",
      "    },\n",
      "    \"24\": {\n",
      "        \"start\": 1023.0999755859375,\n",
      "        \"end\": 1079.260009765625,\n",
      "        \"script\": \"And you can see it's actually pretty short and pretty concise, because we've made use of many of these vectorized operations offered by NumPy. So here we can see that the training, this train function that we talked about earlier is, again, very simple in the case of nearest neighbor. You just memorize the training data. There's not really much to do here. And now at test time, we're gonna take in our image and then go in and compare, using this L1 distance function, our test image to each of these training examples. And find the most similar example in the training set. And you can see that we're actually able to do this in just one or two lines of Python code, using, by utilizing these vectorized operations in NumPy. So this is something that you'll get practice with on the first assignment. So now a couple of questions about this simple classifier. First, if we have n examples in our training set, then how fast can we expect training and testing to be?\"\n",
      "    },\n",
      "    \"25\": {\n",
      "        \"start\": 1079.260009765625,\n",
      "        \"end\": 1137.8800048828125,\n",
      "        \"script\": \"Well, training is probably constant because we don't really need to do anything. We just need to memorize the data. And if you're just copying a pointer, that's gonna be constant time, no matter how big your data set is. But now at test time, we need to do this comparison step and compare our test image to each of the n training examples in the data set. And this is actually quite slow. So this is actually somewhat backwards, if you think about it. Because, you know, in practice, we want our classifiers to be slow at training time and then fast at testing time. Because you might imagine that a classifier might go and be trained in a data center somewhere, and you can afford to spend a lot of computation at training time to make the classifier really good. But then when you go and deploy the classifier at test time, you want it to run on your mobile phone or in the browser or some other low-power device, and you really want the testing time performance of your classifier to be quite fast.\"\n",
      "    },\n",
      "    \"26\": {\n",
      "        \"start\": 1137.8800048828125,\n",
      "        \"end\": 1187.06005859375,\n",
      "        \"script\": \"So from this perspective, this nearest neighbor algorithm is actually a little bit backwards. And we'll see that once we move to convolutional neural networks and other types of parametric models, they'll be the reverse of this, where you'll spend a lot of compute at training time, but then they'll be quite fast at testing time. So then the question is, what exactly does this nearest neighbor algorithm look like when you apply it in practice? So here we've drawn what we call the decision regions of a nearest neighbor classifier. So here our training set consists of these points in the two-dimensional plane, where the color of the point represents the category or the class label of that point. So here we see we have five classes, and some blue ones up in the corner here, some purple ones in the upper right-hand corner. And now for each pixel in this entire plane,\"\n",
      "    },\n",
      "    \"27\": {\n",
      "        \"start\": 1187.06005859375,\n",
      "        \"end\": 1234.739990234375,\n",
      "        \"script\": \"we've gone and computed what is the nearest training, what is the nearest example in the training set. What is the nearest example in these training data, and then colored the point of the background corresponding to what is the class label. So you can see that this nearest neighbor classifier is just sort of carving up the space and coloring the space according to the nearby points. But this classifier is maybe not so great, and by looking at this picture, we can start to see some of the problems that might come out with a nearest neighbor classifier. For one, this central region actually contains mostly green points, but one little yellow point in the middle. But because we're just looking at the nearest neighbor, this causes a little yellow island to appear in this middle of the green cluster, and that's maybe not so great.\"\n",
      "    },\n",
      "    \"28\": {\n",
      "        \"start\": 1234.739990234375,\n",
      "        \"end\": 1270.6600341796875,\n",
      "        \"script\": \"Maybe those points actually should have been green. And then similarly, we also see these sort of fingers of the different, like the green region pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. So this kind of motivates a slight generalization of this algorithm called k-nearest neighbors. So rather than just looking for the single nearest neighbor, instead, we'll do something a little bit fancier and find k of our nearest neighbors according to our distance metric, and then take a vote among each of our neighbors, and then predict the majority vote among our neighbors.\"\n",
      "    },\n",
      "    \"29\": {\n",
      "        \"start\": 1271.0799560546875,\n",
      "        \"end\": 1417.800048828125,\n",
      "        \"script\": \"You can imagine slightly more complex ways of doing this. Maybe you vote weighted on the distance or something like that, but the simplest thing that tends to work pretty well is taking a majority vote. So here we've shown the exact same set of points using this k equals one nearest neighbor classifier, as well as k equals three and k equals five in the middle and on the right. And once we move to k equals three, you can see that that spurious yellow point in the middle of the green cluster is no longer causing the points near that region to be classified as yellow. Now this entire green portion in the middle is all being classified as green. And you'll also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then once we move to the k equals five case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So this is generally something, so generally when you're using nearest neighbor classifiers, you almost always want to use some value of k, which is larger than one, because this tends to smooth out your decision boundaries and lead to better results. So if we, to kind of, oh yeah, question? Yeah, so the question is, what is the deal with these white regions? And the white regions are where there was no majority among the k nearest neighbors. You could imagine maybe doing something slightly fancier and maybe taking a guess or randomly selecting among the majority winners. But for this simple example, we're just coloring it white to indicate that there was no nearest neighbor in those points. So we already kind of saw, so I like to, whenever we're thinking about computer vision, I think it's really useful to kind of flip back and forth between several different viewpoints. One is this idea of high dimensional points in the plane, and then the other is actually looking at concrete images, because the pixels of the image actually allow us to think of these images as high dimensional vectors. And it's sort of useful to ping pong back and forth between these two different viewpoints. So then, when sort of taking this k nearest neighbor and going back to the images, you can see that it's actually not very good. Here I've colored in red and green, which images would actually be classified correctly or incorrectly according to their nearest neighbor. And you can see that it's really not very good. But maybe if we used a larger value of k, then this would involve actually voting among maybe the top three or the top five, or maybe even the whole row.\"\n",
      "    },\n",
      "    \"30\": {\n",
      "        \"start\": 1417.800048828125,\n",
      "        \"end\": 1426.8599853515625,\n",
      "        \"script\": \"And you can imagine that that would end up being a lot more robust to some of this noise that we see when retrieving neighbors in this way.\"\n",
      "    }\n",
      "}\n",
      "4535\n",
      "4544\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data from the files\n",
    "project_id = 11\n",
    "with open(os.path.join(SCRIPT, 'test_word_gpt.json'), 'r') as file:\n",
    "    word_times = json.load(file)\n",
    "\n",
    "with open(os.path.join(SCRIPT, 'test_segment_gpt.json'), 'r') as file:\n",
    "    segment_times = json.load(file)\n",
    "\n",
    "with open(os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\"), 'r') as file:\n",
    "    paragraphs = json.load(file)\n",
    "\n",
    "def get_script_times(script_text, word_timestamp):\n",
    "    # Remove punctuation from the script_text and split into words\n",
    "\n",
    "    words = re.findall(r\"\\b[\\w\\']+\\b\", script_text.lower())\n",
    "\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "\n",
    "    if len(words) >= 5:\n",
    "        for i in range(len(word_timestamp) - 4):\n",
    "            if (\n",
    "                word_timestamp[i][\"word\"].lower() == words[0]\n",
    "                and word_timestamp[i + 1][\"word\"].lower() == words[1]\n",
    "                and word_timestamp[i + 2][\"word\"].lower() == words[2]\n",
    "                and word_timestamp[i + 3][\"word\"].lower() == words[3]\n",
    "                and word_timestamp[i + 4][\"word\"].lower() == words[4]\n",
    "            ):\n",
    "                # Set start time from the first word\n",
    "                start_time = word_timestamp[i][\"start\"]\n",
    "\n",
    "                # Find end time from the last word in words\n",
    "                for j in range(i + 4, len(word_timestamp)):\n",
    "                    if word_timestamp[j][\"word\"].lower() == words[-1]:\n",
    "                        end_time = word_timestamp[j][\"end\"]\n",
    "                        break\n",
    "                break\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "def get_script_times_by_segment(script_text, segment_timestamp, start_time):\n",
    "    script_text_without_punctuation = re.sub(r'[^\\w\\s]', '', script_text.lower().strip())\n",
    "    print(script_text_without_punctuation)\n",
    "\n",
    "    # start_time = None\n",
    "\n",
    "    for segment in segment_timestamp:\n",
    "        segment_text = re.sub(r'[^\\w\\s]', '',segment[\"text\"].lower().strip())\n",
    "        print(segment_text)\n",
    "        if segment_text in script_text_without_punctuation:\n",
    "            if segment[\"start\"] > start_time:\n",
    "                start_time = segment[\"start\"] \n",
    "            break\n",
    "\n",
    "    return start_time\n",
    "\n",
    "# Create the structured output\n",
    "output = {}\n",
    "offset = 0\n",
    "start_time = 0\n",
    "for para_id, paragraph_text in paragraphs.items():\n",
    "    # start_time, end_time = get_script_times(paragraph_text, word_times)\n",
    "    words = paragraph_text.split()\n",
    "    start_time = word_times[offset][\"start\"]\n",
    "    end_time = word_times[offset + len(words) - 1][\"end\"] if int(para_id) < len(paragraphs) else word_times[-1][\"end\"]\n",
    "    # if int(para_id) > 1:\n",
    "    #     start_time = get_script_times_by_segment(paragraph_text, segment_times, start_time)\n",
    "    \n",
    "    # output[para_id] = {}\n",
    "    # output[para_id][\"start\"] = start_time\n",
    "    # if int(para_id) > 1:\n",
    "    #     if int(para_id) == len(paragraphs):\n",
    "    #         output[para_id][\"end\"] = segment_times[-1][\"end\"]\n",
    "    #     output[str(int(para_id)-1)][\"end\"] = start_time \n",
    "    # output[para_id][\"script\"] = paragraph_text\n",
    "    output[para_id] = {\n",
    "        \"start\": start_time,\n",
    "        \"end\": end_time,\n",
    "        \"script\": paragraph_text\n",
    "    }\n",
    "    \n",
    "    offset += len(words) \n",
    "\n",
    "# Save the output to a JSON file\n",
    "with open(os.path.join(SPM, \"test_page_info.json\"), 'w') as file:\n",
    "    json.dump(output, file, indent=4)\n",
    "\n",
    "# Print the output (optional)\n",
    "print(json.dumps(output, indent=4))\n",
    "\n",
    "print(offset)\n",
    "print(len(word_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n",
      "so\n",
      "welcome\n",
      "to\n",
      "lecture\n",
      "2\n",
      "of\n",
      "cs231n\n",
      "on\n",
      "tuesday\n",
      "we\n",
      "just\n",
      "recall\n",
      "we\n",
      "sort\n",
      "of\n",
      "gave\n",
      "you\n",
      "the\n",
      "big\n",
      "picture\n",
      "view\n",
      "of\n",
      "what\n",
      "is\n",
      "computer\n",
      "vision\n",
      "what\n",
      "is\n",
      "the\n",
      "history\n",
      "and\n",
      "a\n",
      "little\n",
      "bit\n",
      "of\n",
      "the\n",
      "overview\n",
      "of\n",
      "the\n",
      "class\n",
      "and\n",
      "today\n",
      "we're\n",
      "really\n",
      "going\n",
      "to\n",
      "dive\n",
      "in\n",
      "for\n",
      "the\n",
      "first\n",
      "time\n",
      "into\n",
      "the\n",
      "details\n",
      "and\n",
      "we'll\n",
      "start\n",
      "to\n",
      "see\n",
      "in\n",
      "much\n",
      "more\n",
      "depth\n",
      "exactly\n",
      "how\n",
      "some\n",
      "of\n",
      "these\n",
      "learning\n",
      "algorithms\n",
      "actually\n",
      "work\n",
      "in\n",
      "practice\n",
      "so\n",
      "the\n",
      "first\n",
      "lecture\n",
      "of\n",
      "the\n",
      "class\n",
      "is\n",
      "probably\n",
      "the\n",
      "sort\n",
      "of\n",
      "the\n",
      "largest\n",
      "big\n",
      "picture\n",
      "vision\n",
      "and\n",
      "the\n",
      "majority\n",
      "of\n",
      "the\n",
      "lectures\n",
      "in\n",
      "this\n",
      "class\n",
      "will\n",
      "be\n",
      "much\n",
      "more\n",
      "detail\n",
      "oriented\n",
      "and\n",
      "much\n",
      "more\n",
      "focused\n",
      "on\n",
      "the\n",
      "specific\n",
      "mechanics\n",
      "of\n",
      "these\n",
      "different\n",
      "algorithms\n",
      "so\n",
      "today\n",
      "we'll\n",
      "see\n",
      "our\n",
      "first\n",
      "learning\n",
      "algorithm\n",
      "and\n",
      "that'll\n",
      "be\n",
      "really\n",
      "exciting\n",
      "i\n",
      "think\n",
      "but\n",
      "before\n",
      "we\n",
      "get\n",
      "to\n",
      "that\n",
      "i\n",
      "wanted\n",
      "to\n",
      "talk\n",
      "about\n",
      "a\n",
      "couple\n",
      "administrative\n",
      "issues\n"
     ]
    }
   ],
   "source": [
    "words = \"Okay, so welcome to Lecture 2 of CS231N. On Tuesday, we, just recall, we sort of gave you the big picture view of what is computer vision, what is the history, and a little bit of the overview of the class. And today, we're really going to dive in for the first time into the details, and we'll start to see in much more depth exactly how some of these learning algorithms actually work in practice. So the first lecture of the class is probably the sort of the largest big picture vision, and the majority of the lectures in this class will be much more detail-oriented, and much more focused on the specific mechanics of these different algorithms. So today, we'll see our first learning algorithm, and that'll be really exciting, I think. But before we get to that, I wanted to talk about a couple administrative issues.\"\n",
    "\n",
    "words = re.findall(r\"\\b[\\w\\']+\\b\", words.lower())\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SPM, f\"10_page_info.json\"), 'r') as file:\n",
    "    page_info = json.load(file)\n",
    "\n",
    "with open(os.path.join(SPM, f\"test_page_info.json\"), 'r') as file:\n",
    "    update_info = json.load(file)\n",
    "\n",
    "for page_id, page_data in page_info[\"pages\"].items():\n",
    "    page_data[\"start\"] = update_info[page_id][\"start\"]\n",
    "    page_data[\"end\"] = update_info[page_id][\"end\"]\n",
    "    page_data[\"script\"] = update_info[page_id][\"script\"]\n",
    "\n",
    "with open(os.path.join(SPM, f\"10_page_info.json\"), 'w') as file:\n",
    "    json.dump(page_info, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 11\n",
    "image_directory = os.path.join(IMAGE, f\"{str(project_id)}\")\n",
    "script_path = os.path.join(SCRIPT, f\"{project_id}_transcription.json\")\n",
    "\n",
    "image_paths = sorted([os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith('.png')])\n",
    "\n",
    "# print(image_paths)\n",
    "encoded_images = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encode_image(image)}\"}} for image in image_paths]\n",
    "\n",
    "matched_paragraphs_path = os.path.join(SPM, f\"{project_id}_matched_paragraphs.json\")\n",
    "with open(matched_paragraphs_path, 'r') as file:\n",
    "    matched_paragraphs = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4o\"\n",
    "gpt_api_key = \"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\"\n",
    "\n",
    "def filter_script_data(script_data):\n",
    "    allowed_keys = {\"keyword\", \"formal\"}\n",
    "    return {key: value for key, value in script_data.items() if key in allowed_keys}\n",
    "\n",
    "def bbox_api_request(script_segment, encoded_image):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {gpt_api_key}\",\n",
    "    }\n",
    "\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                # \"Given the following lecture notes image and the corresponding lecture script, \"\n",
    "                # \"please provide bounding box information for each relevant script sentence. \"\n",
    "                \"Tell me specifically where each sentence in the script describes in the image. \"\n",
    "                \"If there is a corresponding part on the image, please tell me the bounding box information of that area. \"\n",
    "                \"The output should be in JSON format with the structure: \"\n",
    "                '{\"bboxes\": [{\"script\": \"string\", \"bbox\": [x, y, w, h]}]} '\n",
    "                \"Ensure that the value for script in the JSON response should be a sentence from the provided script, and the value must never be text extracted from the image. \"\n",
    "                f\"script: {script_segment} \"\n",
    "                # \"The bounding box is given in the form [x,y,w,h]. x and y represent the center position of the bounding box, \"\n",
    "                # \"while w and h represent the width and height of the bounding box, respectively. \"\n",
    "                # \"The coordinates are based on the top-left corner of the image being (0,0), with the x direction being vertical and the y direction being horizontal. \"\n",
    "                # \"If a sentence in the script is highly relevant to a specific part of the image, \"\n",
    "                # \"use the sentence as the key and provide the bounding box information of the specific part of the image as the value. \"\n",
    "                # \"Only find bounding boxes for the sentences that are highly relevant to specific parts of the image. \"\n",
    "                # \"The original format of the script, including uppercase and lowercase letters, punctuation marks such as periods and commas, must be preserved without any alterations.\"\n",
    "            ),\n",
    "        },\n",
    "        # {\"type\": \"text\", \"text\": script_segment},\n",
    "        encoded_image\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_MODEL,\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def keyword_api_request(script_segment):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {gpt_api_key}\",\n",
    "    }\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": (\n",
    "                \"Given the lecture script, identify at least one important keywords. \"\n",
    "                \"Next, transform the script into a more formal tone, breaking it down into a bullet point structure where appropriate. \"\n",
    "                \"The output should be in JSON format with the following structure: \"\n",
    "                '{\"keyword\": [\"string\", \"string\", ...], \"formal\": \"string\"} '\n",
    "                f\"lecture script: {script_segment} \"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "BBOX = '../bboxs'\n",
    "KEYWORD = '../keywords'\n",
    "project_id = 11\n",
    "bbox_dir = os.path.join(BBOX, str(project_id))\n",
    "keyword_dir = os.path.join(KEYWORD, str(project_id))\n",
    "\n",
    "for i, encoded_image in enumerate(encoded_images):\n",
    "    page_number = str(i + 1)\n",
    "    script_segment = matched_paragraphs[page_number]\n",
    "    response_data_bbox = bbox_api_request(script_segment, encoded_image)\n",
    "\n",
    "    # Process the response data for bbox\n",
    "    if \"choices\" in response_data_bbox and len(response_data_bbox[\"choices\"]) > 0:\n",
    "        script_text = response_data_bbox[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Convert the script text to JSON format\n",
    "        try:\n",
    "            script_data = json.loads(script_text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for page {page_number}: {e}\")\n",
    "            script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error: 'choices' key not found in the response for page {page_number}\"\n",
    "        )\n",
    "        script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "    # Save the script data as a JSON file\n",
    "    bbox_path = os.path.join(bbox_dir, f\"{page_number}_spm.json\")\n",
    "    with open(bbox_path, \"w\") as json_file:\n",
    "        json.dump(script_data, json_file, indent=4)\n",
    "\n",
    "    response_data_keyword = keyword_api_request(script_segment)\n",
    "\n",
    "    # Process the response data for keyword\n",
    "    if (\n",
    "        \"choices\" in response_data_keyword\n",
    "        and len(response_data_keyword[\"choices\"]) > 0\n",
    "    ):\n",
    "        script_text = response_data_keyword[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Convert the script text to JSON format\n",
    "        try:\n",
    "            script_data = json.loads(script_text)\n",
    "            script_data = filter_script_data(script_data)\n",
    "            script_data[\"original\"] = script_segment\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for page {page_number}: {e}\")\n",
    "            script_data = {\"error\": \"Failed to decode JSON\"}\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error: 'choices' key not found in the response for page {page_number}\"\n",
    "        )\n",
    "        script_data = {\"error\": \"Failed to retrieve scripts\"}\n",
    "\n",
    "    keyword_path = os.path.join(keyword_dir, f\"{page_number}_spm.json\")\n",
    "    with open(keyword_path, \"w\") as json_file:\n",
    "        json.dump(script_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT = '../scripts'   \n",
    "def timestamp_for_bbox(project_id, word_timestamp):\n",
    "    for page_num in range(1, len(os.listdir(os.path.join(BBOX, str(project_id)))) + 1):\n",
    "        bbox_path = os.path.join(BBOX, str(project_id), f\"{page_num}_spm.json\")\n",
    "\n",
    "        # Load the bbox data for the current page\n",
    "        with open(bbox_path, \"r\") as file:\n",
    "            bboxes = json.load(file)\n",
    "\n",
    "        updated_bboxes = []\n",
    "        for item in bboxes[\"bboxes\"]:\n",
    "            bbox = item[\"bbox\"]\n",
    "            \n",
    "            if bbox and isinstance(bbox[0], list):\n",
    "                bbox = bbox[0]    \n",
    "            if not bbox or bbox[2] == 0 or bbox[3] == 0:\n",
    "                continue\n",
    "\n",
    "            start_time, end_time = get_script_times(item[\"script\"], word_timestamp)\n",
    "            if start_time is not None:\n",
    "                item[\"start\"] = start_time\n",
    "                item[\"end\"] = end_time\n",
    "                updated_bboxes.append(item)\n",
    "\n",
    "        # Save the updated data back to the JSON file\n",
    "        bboxes[\"bboxes\"] = updated_bboxes\n",
    "        with open(bbox_path, \"w\") as file:\n",
    "            json.dump(bboxes, file, indent=4)\n",
    "\n",
    "\n",
    "with open(os.path.join(SCRIPT, f\"{project_id}_timestamp.json\"), \"r\") as file:\n",
    "    word_timestamp = json.load(file)\n",
    "    \n",
    "\n",
    "timestamp_for_bbox(project_id, word_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../images/10/raw/page_0001.png', '../images/10/raw/page_0002.png', '../images/10/raw/page_0003.png', '../images/10/raw/page_0004.png', '../images/10/raw/page_0005.png', '../images/10/raw/page_0006.png', '../images/10/raw/page_0007.png', '../images/10/raw/page_0008.png', '../images/10/raw/page_0009.png', '../images/10/raw/page_0010.png', '../images/10/raw/page_0011.png', '../images/10/raw/page_0012.png', '../images/10/raw/page_0013.png', '../images/10/raw/page_0014.png', '../images/10/raw/page_0015.png', '../images/10/raw/page_0016.png', '../images/10/raw/page_0017.png', '../images/10/raw/page_0018.png', '../images/10/raw/page_0019.png', '../images/10/raw/page_0020.png', '../images/10/raw/page_0021.png', '../images/10/raw/page_0022.png', '../images/10/raw/page_0023.png', '../images/10/raw/page_0024.png', '../images/10/raw/page_0025.png', '../images/10/raw/page_0026.png', '../images/10/raw/page_0027.png', '../images/10/raw/page_0028.png', '../images/10/raw/page_0029.png', '../images/10/raw/page_0030.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "IMAGE = '../images'\n",
    "project_id = 10\n",
    "image_directory = os.path.join(IMAGE, str(project_id), 'raw')\n",
    "\n",
    "image_paths = sorted(\n",
    "        [\n",
    "            os.path.join(image_directory, f)\n",
    "            for f in os.listdir(image_directory)\n",
    "            if f.lower().endswith(\".png\")\n",
    "        ]\n",
    "    )\n",
    "print(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_pages(toc_data, total_pages):\n",
    "    \"\"\"\n",
    "    누락된 페이지를 확인하고, 적절한 위치에 포함시키는 함수.\n",
    "\n",
    "    :param toc_data: 생성된 TOC 데이터 (JSON 형식)\n",
    "    :param total_pages: PDF 파일의 전체 페이지 수\n",
    "    :return: 누락된 페이지가 포함된 TOC 데이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 모든 페이지 번호를 추출\n",
    "    included_pages = set()\n",
    "    for section in toc_data[\"table_of_contents\"]:\n",
    "        for subsection in section[\"subsections\"]:\n",
    "            included_pages.update(subsection[\"page\"])\n",
    "\n",
    "    # 누락된 페이지를 찾기\n",
    "    missing_pages = sorted(set(range(1, total_pages + 1)) - included_pages)\n",
    "\n",
    "    # 첫 페이지와 마지막 페이지 처리\n",
    "    if 1 in missing_pages:\n",
    "        toc_data[\"table_of_contents\"][0][\"subsections\"][0][\"page\"].insert(0, 1)\n",
    "        missing_pages.remove(1)\n",
    "\n",
    "    if total_pages in missing_pages:\n",
    "        toc_data[\"table_of_contents\"][-1][\"subsections\"][-1][\"page\"].append(total_pages)\n",
    "        missing_pages.remove(total_pages)\n",
    "\n",
    "    # 나머지 누락된 페이지를 적절한 위치에 포함시키기\n",
    "    for missing_page in missing_pages:\n",
    "        placed = False\n",
    "\n",
    "        # 각 섹션 및 하위 섹션 간에서 누락된 페이지를 삽입할 위치 찾기\n",
    "        for section in toc_data[\"table_of_contents\"]:\n",
    "            subsections = section[\"subsections\"]\n",
    "\n",
    "            for j in range(len(subsections)):\n",
    "                current_pages = subsections[j][\"page\"]\n",
    "\n",
    "                if j < len(subsections) - 1:\n",
    "                    next_pages = subsections[j + 1][\"page\"]\n",
    "\n",
    "                    # 현재 subsection과 다음 subsection 사이에 누락된 페이지가 있는 경우\n",
    "                    if current_pages[-1] < missing_page < next_pages[0]:\n",
    "                        if len(current_pages) <= len(next_pages):\n",
    "                            current_pages.append(missing_page)\n",
    "                            current_pages.sort()\n",
    "                        else:\n",
    "                            next_pages.insert(0, missing_page)\n",
    "                        placed = True\n",
    "                        break\n",
    "\n",
    "            if placed:\n",
    "                break\n",
    "\n",
    "        # main section 간에 누락된 페이지가 있는지 확인\n",
    "        if not placed:\n",
    "            for i in range(len(toc_data[\"table_of_contents\"]) - 1):\n",
    "                current_section = toc_data[\"table_of_contents\"][i]\n",
    "                next_section = toc_data[\"table_of_contents\"][i + 1]\n",
    "\n",
    "                current_pages = current_section[\"subsections\"][-1][\"page\"]\n",
    "                next_pages = next_section[\"subsections\"][0][\"page\"]\n",
    "\n",
    "                if current_pages[-1] < missing_page < next_pages[0]:\n",
    "                    if len(current_pages) <= len(next_pages):\n",
    "                        current_pages.append(missing_page)\n",
    "                        current_pages.sort()\n",
    "                    else:\n",
    "                        next_pages.insert(0, missing_page)\n",
    "                    placed = True\n",
    "                    break\n",
    "\n",
    "    return toc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOC = '../tocs'\n",
    "\n",
    "with open(os.path.join(TOC, f\"113_toc.json\"), \"r\") as file:\n",
    "    toc_data = json.load(file)\n",
    "\n",
    "total_pages = 30\n",
    "updated_toc_data = fill_missing_pages(toc_data, total_pages)\n",
    "\n",
    "with open(os.path.join(TOC, f\"113_toc_updated.json\"), \"w\") as file:\n",
    "    json.dump(updated_toc_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
